{"articles":[{"id":1,"title":"A General Framework for Constrained Bayesian Optimization using Information-based Search","url":"https://www.researchgate.net/publication/285458515_A_General_Framework_for_Constrained_Bayesian_Optimization_using_Information-based_Search","abstraction":"We present an information-theoretic framework for solving global black-box optimization problems that also have black-box constraints. Of particular interest to us is to efficiently solve problems with decoupled constraints, in which subsets of the objective and constraint functions may be evaluated independently. For example, when the objective is evaluated on a CPU and the constraints are evaluated independently on a GPU. These problems require an acquisition function that can be separated into the contributions of the individual function evaluations. We develop one such acquisition function and call it Predictive Entropy Search with Constraints (PESC). PESC is an approximation to the expected information gain criterion and it compares favorably to alternative approaches based on improvement in several synthetic and real-world problems. In addition to this, we consider problems with a mix of functions that are fast and slow to evaluate. These problems require balancing the amount of time spent in the meta-computation of PESC and in the actual evaluation of the target objective. We take a bounded rationality approach and develop a partial update for PESC which trades off accuracy against speed. We then propose a method for adaptively switching between the partial and full updates for PESC. This allows us to interpolate between versions of PESC that are efficient in terms of function evaluations and those that are efficient in terms of wall-clock time. Overall, we demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization."},{"id":2,"title":"Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions","url":"https://www.researchgate.net/publication/284579255_Parallel_Predictive_Entropy_Search_for_Batch_Global_Optimization_of_Expensive_Objective_Functions","abstraction":"We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics."},{"id":3,"title":"Sandwiching the marginal likelihood using bidirectional Monte Carlo","url":"https://www.researchgate.net/publication/283658712_Sandwiching_the_marginal_likelihood_using_bidirectional_Monte_Carlo","abstraction":"Computing the marginal likelihood (ML) of a model requires marginalizing out all of the parameters and latent variables, a difficult high-dimensional summation or integration problem. To make matters worse, it is often hard to measure the accuracy of one's ML estimates. We present bidirectional Monte Carlo, a technique for obtaining accurate log-ML estimates on data simulated from a model. This method obtains stochastic lower bounds on the log-ML using annealed importance sampling or sequential Monte Carlo, and obtains stochastic upper bounds by running these same algorithms in reverse starting from an exact posterior sample. The true value can be sandwiched between these two stochastic bounds with high probability. Using the ground truth log-ML estimates obtained from our method, we quantitatively evaluate a wide variety of existing ML estimators on several latent variable models: clustering, a low rank approximation, and a binary attributes model. These experiments yield insights into how to accurately estimate marginal likelihoods."},{"id":4,"title":"Dirichlet Fragmentation Processes","url":"https://www.researchgate.net/publication/281895707_Dirichlet_Fragmentation_Processes","abstraction":"Tree structures are ubiquitous in data across many domains, and many datasets are naturally modelled by unobserved tree structures. In this paper, first we review the theory of random fragmentation processes [Bertoin, 2006], and a number of existing methods for modelling trees, including the popular nested Chinese restaurant process (nCRP). Then we define a general class of probability distributions over trees: the Dirichlet fragmentation process (DFP) through a novel combination of the theory of Dirichlet processes and random fragmentation processes. This DFP presents a stick-breaking construction, and relates to the nCRP in the same way the Dirichlet process relates to the Chinese restaurant process. Furthermore, we develop a novel hierarchical mixture model with the DFP, and empirically compare the new model to similar models in machine learning. Experiments show the DFP mixture model to be convincingly better than existing state-of-the-art approaches for hierarchical clustering and density modelling."},{"id":5,"title":"Subsampling-Based Approximate Monte Carlo for Discrete Distributions","url":"https://www.researchgate.net/publication/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Drawing a sample from a discrete distribution is one of the building\n <br> components for Monte Carlo methods. Like other sampling algorithms, discrete\n <br> sampling also suffers from high computational burden in large-scale inference\n <br> problems. We study the problem of sampling a discrete random variable with a\n <br> high degree of dependency that is typical in large-scale Bayesian inference and\n <br> graphical models, and propose an efficient approximate solution with a\n <br> subsampling approach. We make a novel connection between the discrete sampling\n <br> and Multi-Armed Bandits problems with a finite reward population and provide\n <br> three algorithms with theoretical guarantees. Empirical evaluations show the\n <br> robustness and efficiency of the approximate algorithms in both synthetic and\n <br> real-world large-scale problems.\n</div> \n<p></p>"},{"id":6,"title":"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process","url":"https://www.researchgate.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process","abstraction":"Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we find that different posterior dependencies are important in BPFA relative to LDA. Particularly, approximations able to model intra-local variable dependence perform best."},{"id":7,"title":"MCMC for Variationally Sparse Gaussian Processes","url":"https://www.researchgate.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","abstraction":"Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper will be available shortly."},{"id":8,"title":"A Bayesian approach to constrained single- and multi-objective optimization","url":"https://www.researchgate.net/publication/282570552_A_Bayesian_approach_to_constrained_single-_and_multi-objective_optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article addresses the problem of derivative-free (single- or multi-objective) optimization subject to multiple inequality constraints. Both the objective and constraint functions are assumed to be smooth, non-linear and expensive to evaluate. As a consequence, the number of evaluations that can be used to carry out the optimization is very limited, as in complex industrial design optimization problems. The method we propose to overcome this difficulty has its roots in the Bayesian and the multiobjective optimization literatures. More specifically, an extended domination rule is used to handle the constraints and a corresponding Bayesian expected hyper-volume improvement sampling criterion is proposed. This new criterion extends existing Bayesian sampling criteria to the multi-objective constrained case, and makes it possible to start the algorithm without an initial feasible point. The calculation and optimization of the criterion are performed using Sequential Monte Carlo techniques. In particular, an algorithm similar to the subset simulation method, which is well known in the field of structural reliability, is used to estimate the expected hyper-volume improvement criterion. The method, which we call BMOO (for Bayesian Multi-Objective Optimization), is compared to state-of-the-art algorithms for single-objective and multi-objective constrained optimization problems.\n</div> \n<p></p>"},{"id":9,"title":"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144","url":"https://www.researchgate.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian\n <br> statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP\n <br> mixtures has enabled the application of nonparametric Bayesian methods to a variety of\n <br> practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it\n <br> is important to explore alternatives. One class of alternatives is provided by variational\n <br> methods, a class of deterministic algorithms that convert inference problems into\n <br> optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far,\n <br> variational methods have mainly been explored in the parametric setting, in particular\n <br> within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001;\n <br> Blei et al. 2003). In this paper, we present a variational inference algorithm for DP\n <br> mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms\n <br> for DP mixtures of Gaussians and present an application to a large-scale image analysis \n <br> problem.\n</div> \n<p></p>"},{"id":10,"title":"Social Signal Processing: Survey of an Emerging Domain","url":"https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence – the ability to recognize human social signals and social behaviours like turn taking, politeness, and disagreement – in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for social signal processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially aware computing.\n</div> \n<p></p>"},{"id":11,"title":"The Infinite Hidden Markov Random Field Model","url":"https://www.researchgate.net/publication/44572779_The_Infinite_Hidden_Markov_Random_Field_Model","abstraction":"Hidden Markov random field (HMRF) models are widely used for image segmentation, as they appear naturally in problems where a spatially constrained clustering scheme is asked for. A major limitation of HMRF models concerns the automatic selection of the proper number of their states, i.e., the number of region clusters derived by the image segmentation procedure. Existing methods, including likelihood- or entropy-based criteria, and reversible Markov chain Monte Carlo methods, usually tend to yield noisy model size estimates while imposing heavy computational requirements. Recently, Dirichlet process (DP, infinite) mixture models have emerged in the cornerstone of nonparametric Bayesian statistics as promising candidates for clustering applications where the number of clusters is unknown a priori; infinite mixture models based on the original DP or spatially constrained variants of it have been applied in unsupervised image segmentation applications showing promising results. Under this motivation, to resolve the aforementioned issues of HMRF models, in this paper, we introduce a nonparametric Bayesian formulation for the HMRF model, the infinite HMRF model, formulated on the basis of a joint Dirichlet process mixture (DPM) and Markov random field (MRF) construction. We derive an efficient variational Bayesian inference algorithm for the proposed model, and we experimentally demonstrate its advantages over competing methodologies."},{"id":12,"title":"Painful data: The UNBC-McMaster shoulder pain expression archive database","url":"https://www.researchgate.net/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A major factor hindering the deployment of a fully functional automatic facial expression detection system is the lack of representative data. A solution to this is to narrow the context of the target application, so enough data is available to build robust models so high performance can be gained. Automatic pain detection from a patient's face represents one such application. To facilitate this work, researchers at McMaster University and University of Northern British Columbia captured video of participant's faces (who were suffering from shoulder pain) while they were performing a series of active and passive range-of-motion tests to their affected and unaffected limbs on two separate occasions. Each frame of this data was AU coded by certified FACS coders, and self-report and observer measures at the sequence level were taken as well. This database is called the UNBC-McMaster Shoulder Pain Expression Archive Database. To promote and facilitate research into pain and augment current datasets, we have publicly made available a portion of this database which includes: (1) 200 video sequences containing spontaneous facial expressions, (2) 48,398 FACS coded frames, (3) associated pain frame-by-frame scores and sequence-level self-report and observer measures, and (4) 66-point AAM landmarks. This paper documents this data distribution in addition to describing baseline results of our AAM/SVM system. This data will be available for distribution in March 2011.\n</div> \n<p></p>"},{"id":13,"title":"On the Goldstein-Levitin-Polyak gradient projection method","url":"https://www.researchgate.net/publication/224680516_On_the_Goldstein-Levitin-Polyak_gradient_projection_method","abstraction":"This paper considers some aspects of a gradient projection method proposed by Goldstein [1], Levitin and Polyak [3] and more recently, in a less general context, by Mc-Cormick [10]. We propose and analyze some convergent stepsize rules to be used in conjunction with the method. These rules are similar in spirit with the efficient Armijo rule for the method of steepest descent and under mild assumptions they have the desirable property that they identify the set of active inequality constraints in a finite number of iterations. As a result the method may be converted towards the end of the process to a conjugate direction, Quasi-Newton or Newton's method and achieve the attendant superlinear convergence rate. As an example we propose a quadratically convergent combination of the method with Newton's method. Such combined methods appear to be very efficient for large scale problems with many simple constraints such as those often appearing in optimal control."},{"id":14,"title":"OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit","url":"https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Various open-source toolkits exist for speech recognition and speech processing. These toolkits have brought a great benefit to the research community, i.e. speeding up research. Yet, no such freely available toolkit exists for automatic affect recognition from speech. We herein introduce a novel open-source affect and emotion recognition engine, which integrates all necessary components in one highly efficient software package. The components include audio recording and audio file reading, state-of-the-art paralinguistic feature extraction and plugable classification modules. In this paper we introduce the engine and extensive baseline results. Pre-trained models for four affect recognition tasks are included in the openEAR distribution. The engine is tailored for multi-threaded, incremental on-line processing of live input in real-time, however it can also be used for batch processing of databases.\n</div> \n<p></p>"},{"id":15,"title":"Beam sampling for the infinite hidden Markov model","url":"https://www.researchgate.net/publication/221345210_Beam_sampling_for_the_infinite_hidden_Markov_model","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The innite hidden Markov model is a non- parametric extension of the widely used hid- den Markov model. Our paper introduces a new inference algorithm for the innite Hidden Markov model called beam sam- pling. Beam sampling combines slice sam- pling, which limits the number of states con- sidered at each time step to a nite number, with dynamic programming, which samples whole state trajectories eciently. Our algo- rithm typically outperforms the Gibbs sam- pler and is more robust. We present appli- cations of iHMM inference using the beam sampler on changepoint detection and text prediction problems.\n</div> \n<p></p>"},{"id":16,"title":"Nonparametric Bayesian Image Segmentation","url":"https://www.researchgate.net/publication/225797350_Nonparametric_Bayesian_Image_Segmentation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Image segmentation algorithms partition the set of pixels of an image into a specific number of different, spatially homogeneous\n <br> groups. We propose a nonparametric Bayesian model for histogram clustering which automatically determines the number of segments\n <br> when spatial smoothness constraints on the class assignments are enforced by a Markov Random Field. A Dirichlet process prior\n <br> controls the level of resolution which corresponds to the number of clusters in data with a unique cluster structure. The resulting posterior is efficiently\n <br> sampled by a variant of a conjugate-case sampling algorithm for Dirichlet process mixture models. Experimental results are\n <br> provided for real-world gray value images, synthetic aperture radar images and magnetic resonance imaging data.\n</div> \n<p></p>"},{"id":17,"title":"An HDP-HMM for systems with state persistence","url":"https://www.researchgate.net/publication/221344933_An_HDP-HMM_for_systems_with_state_persistence","abstraction":"The hierarchical Dirichlet process hidden Markov model (HDP-HMM) is a flexible, nonparametric model which allows state spaces of unknown size to be learned from data. We demonstrate some limitations of the original HDP-HMM formulation (Teh et al., 2006), and propose a sticky exten- sion which allows more robust learning of smoothly varying dynamics. Using DP mix- tures, this formulation also allows learning of more complex, multimodal emission dis- tributions. We further develop a sampling algorithm that employs a truncated approx- imation of the DP to jointly resample the full state sequence, greatly improving mixing rates. Via extensive experiments with syn- thetic data and the NIST speaker diarization database, we demonstrate the advantages of our sticky extension, and the utility of the HDP-HMM in real-world applications."},{"id":18,"title":"Infinite Hidden Conditional Random Fields for Human Behavior Analysis","url":"https://www.researchgate.net/publication/260353896_Infinite_Hidden_Conditional_Random_Fields_for_Human_Behavior_Analysis","abstraction":"Hidden conditional random fields (HCRFs) are discriminative latent variable models that have been shown to successfully learn the hidden structure of a given classification problem (provided an appropriate validation of the number of hidden states). In this brief, we present the infinite HCRF (iHCRF), which is a nonparametric model based on hierarchical Dirichlet processes and is capable of automatically learning the optimal number of hidden states for a classification task. We show how we learn the model hyperparameters with an effective Markov-chain Monte Carlo sampling technique, and we explain the process that underlines our iHCRF model with the Restaurant Franchise Rating Agencies analogy. We show that the iHCRF is able to converge to a correct number of represented hidden states, and outperforms the best finite HCRFs-chosen via cross-validation-for the difficult tasks of recognizing instances of agreement, disagreement, and pain. Moreover, the iHCRF manages to achieve this performance in significantly less total training, validation, and testing time."},{"id":19,"title":"Bayesian Cluster Analysis","url":"https://www.researchgate.net/publication/31002185_Bayesian_Cluster_Analysis","abstraction":"SUMMARY A parametric model for partitioning individuals into mutually exclusive groups is given. A Bayesian analysis is applied and a loss structure imposed. A model-dependent definition of a similarity matrix is proposed and estimates based on this matrix are justified in a decision-theoretic framework. Some existing cluster analysis techniques are derived as special limiting cases. The results of the procedure applied to two data sets are compared with other analyses."},{"id":20,"title":"An Improved Criterion for Clustering Based on the Posterior Similarity Matrix","url":"https://www.researchgate.net/publication/241251898_An_Improved_Criterion_for_Clustering_Based_on_the_Posterior_Similarity_Matrix","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we address the problem of choosing a single clustering estimate ^c based on an MCMC sample of clusterings c(1);c(2) :::;c(M) from the posterior distribution of a Bayesian cluster model. Methods to derive ^c based on the posterior similarity matrix, a matrix with entries P(ci = cjjy), the posterior probabilities that the observations i and j are in the same cluster, are reviewed and discussed. Minimization of a com- monly used loss function for this purpose by Binder (1978) is shown to be equivalent to maximizing the Rand index between estimated and true clustering. We propose a new criterion for choosing an estimated cluster- ing, the posterior expected adjusted Rand index with the true clustering, which outperforms Binder's loss, MAP and an ad hoc criterion in a sim- ulation study. An application to Fisher's Iris data is also provided. Keywords: Adjusted Rand index; Bayesian inference; Cluster analysis; Markov chain Monte Carlo; Loss functions.\n</div> \n<p></p>"},{"id":21,"title":"Bayesian cluster analysis: Point estimation and credible balls","url":"https://www.researchgate.net/publication/276296321_Bayesian_cluster_analysis_Point_estimation_and_credible_balls","abstraction":"Clustering is widely studied in statistics and machine learning, with applications in a variety of fields. As opposed to classical algorithms which return a single clustering solution, Bayesian nonparametric models provide a posterior over the entire space of partitions, allowing one to assess statistical properties, such as uncertainty on the number of clusters. However, an important problem is how to summarize the posterior; the huge dimension of partition space and difficulties in visualizing it add to this problem. In a Bayesian analysis, the posterior of a real-valued parameter of interest is often summarized by reporting a point estimate such as the posterior mean along with 95% credible intervals to characterize uncertainty. In this paper, we extend these ideas to develop appropriate point estimates and credible sets to summarize the posterior of clustering structure based on decision and information theoretic techniques."},{"id":22,"title":"Scaling the iHMM: Parallelization versus Hadoop","url":"https://www.researchgate.net/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper compares parallel and distributed implementations of an iterative, Gibbs sampling, machine learning algorithm. Distributed implementations run under Hadoop on facility computing clouds. The probabilistic model under study is the infinite HMM, in which parameters are learnt using an instance blocked Gibbs sampling, with a step consisting of a dynamic program. We apply this model to learn part-of-speech tags from newswire text in an unsupervised fashion. However our focus here is on runtime performance, as opposed to NLP-relevant scores, embodied by iteration duration, ease of development, deployment and debugging.\n</div> \n<p></p>"},{"id":23,"title":"Bayesian Learning via Stochastic Gradient Langevin Dynamics","url":"https://www.researchgate.net/publication/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics","abstraction":"In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a “sampling threshold ” and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients. 1."},{"id":24,"title":"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget","url":"https://www.researchgate.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Can we make Bayesian posterior MCMC sampling more efficient when faced with\n <br> very large datasets? We argue that computing the likelihood for N datapoints\n <br> twice in order to reach a single binary decision is computationally\n <br> inefficient. We introduce an approximate Metropolis-Hastings rule based on a\n <br> sequential hypothesis test which allows us to accept or reject samples with\n <br> high confidence using only a fraction of the data required for the exact MH\n <br> rule. While this introduces an asymptotic bias, we show that this bias can be\n <br> controlled and is more than offset by a decrease in variance due to our ability\n <br> to draw more samples per unit of time. We show that the same idea can also be\n <br> applied to Gibbs sampling in densely connected graphs.\n</div> \n<p></p>"},{"id":25,"title":"Concentration inequalities for sampling without replacement","url":"https://www.researchgate.net/publication/256606492_Concentration_inequalities_for_sampling_without_replacement","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Concentration inequalities quantify the deviation of a random variable from a\n <br> fixed value. In spite of numerous applications, such as opinion surveys or\n <br> ecological counting procedures, few concentration results are known for the\n <br> setting of sampling without replacement from a finite population. Until now,\n <br> the best general concentration inequality has been a Hoeffding inequality due\n <br> to Serfling (1974). In this paper, we first improve on the fundamental result\n <br> of Serfling (1974), and further extend it to obtain a Bernstein concentration\n <br> bound for sampling without replacement. We then derive an empirical version of\n <br> our bound that does not require the variance to be known to the user.\n</div> \n<p></p>"},{"id":26,"title":"Monte Carlo MCMC: efficient inference by approximate sampling","url":"https://www.researchgate.net/publication/262350854_Monte_Carlo_MCMC_efficient_inference_by_approximate_sampling","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Conditional random fields and other graphical models have achieved state of the art results in a variety of tasks such as coreference, relation extraction, data integration, and parsing. Increasingly, practitioners are using models with more complex structure---higher tree-width, larger fan-out, more features, and more data---rendering even approximate inference methods such as MCMC inefficient. In this paper we propose an alternative MCMC sampling scheme in which transition probabilities are approximated by sampling from the set of relevant factors. We demonstrate that our method converges more quickly than a traditional MCMC sampler for both marginal and MAP inference. In an author coreference task with over 5 million mentions, we achieve a 13 times speedup over regular MCMC inference.\n</div> \n<p></p>"},{"id":27,"title":"The Tradeoffs of Large Scale Learning.","url":"https://www.researchgate.net/publication/221618614_The_Tradeoffs_of_Large_Scale_Learning","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-sc ale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.\n</div> \n<p></p>"},{"id":28,"title":"Slice sampling mixture models","url":"https://www.researchgate.net/publication/220286498_Slice_sampling_mixture_models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a more efficient version of the slice sampler for Dirichlet process mixture models described by Walker (Commun.\n <br> Stat., Simul. Comput. 36:45–54, 2007). This new sampler allows for the fitting of infinite mixture models with a wide-range of prior specifications. To illustrate\n <br> this flexibility we consider priors defined through infinite sequences of independent positive random variables. Two applications\n <br> are considered: density estimation using mixture models and hazard function estimation. In each case we show how the slice\n <br> efficient sampler can be applied to make inference in the models. In the mixture case, two submodels are studied in detail.\n <br> The first one assumes that the positive random variables are Gamma distributed and the second assumes that they are inverse-Gaussian\n <br> distributed. Both priors have two hyperparameters and we consider their effect on the prior distribution of the number of\n <br> occupied clusters in a sample. Extensive computational comparisons with alternative “conditional” simulation techniques for\n <br> mixture models using the standard Dirichlet process prior and our new priors are made. The properties of the new priors are\n <br> illustrated on a density estimation problem.\n</div> \n<p></p>"},{"id":29,"title":"Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation","url":"https://www.researchgate.net/publication/2360567_Hoeffding_Races_Accelerating_Model_Selection_Search_for_Classification_and_Function_Approximation","abstraction":"Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high. Techniques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models reduce the applicability of these search methods. Hoeffding Races is a technique for finding a good model for the data by quickly discarding bad models, and concentrating the computational effort at differentiating between the better ones. This paper focuses on the special case of leave-one-out cross validation applied to memorybased learning algorithms, but we also argue that it is applicable to any class of model selection problems. 1 Introduction Model selection addresses \"high level\" decisions about how best to tune learning algorithm architectures for particular tasks. Such decisions include which..."},{"id":30,"title":"lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits","url":"https://www.researchgate.net/publication/259478458_lil%27_UCB_An_Optimal_Exploration_Algorithm_for_Multi-Armed_Bandits","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The paper proposes a novel upper confidence bound (UCB) procedure for\n <br> identifying the arm with the largest mean in a multi-armed bandit game in the\n <br> fixed confidence setting using a small number of total samples. The procedure\n <br> cannot be improved in the sense that the number of samples required to identify\n <br> the best arm is within a constant factor of a lower bound based on the law of\n <br> the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence\n <br> bounds to explicitly account for the infinite time horizon of the algorithm. In\n <br> addition, by using a novel stopping time for the algorithm we avoid a union\n <br> bound over the arms that has been observed in other UCB-type algorithms. We\n <br> prove that the algorithm is optimal up to constants and also show through\n <br> simulations that it provides superior performance with respect to the\n <br> state-of-the-art.\n</div> \n<p></p>"},{"id":31,"title":"Probability Inequalities for the Sum in Sampling without Replacement","url":"https://www.researchgate.net/publication/38357750_Probability_Inequalities_for_the_Sum_in_Sampling_without_Replacement","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Upper bounds are established for the probability that, in sampling without replacement from a finite population, the sample sum exceeds its expected value by a specified amount. These are obtained as corollaries of two main results. Firstly, a useful upper bound is derived for the moment generating function of the sum, leading to an exponential probability inequality and related moment inequalities. Secondly, maximal inequalities are obtained, extending Kolmogorov's inequality and the Hajek-Renyi inequality. Compared to sampling with replacement, the results incorporate sharpenings reflecting the influence of the sampling fraction, $n/N$, where $n$ denotes the sample size and $N$ the population size. We go somewhat beyond previous work by Hoeffding (1963) and Sen (1970). As in the latter reference, martingale techniques are exploited. Applications to simple linear rank statistics are noted, dealing with the two-sample Wilcoxon statistic as an example. Finally, the question of sharpness of the exponential bounds is considered.\n</div> \n<p></p>"},{"id":32,"title":"Auto-Encoding Variational Bayes","url":"https://www.researchgate.net/publication/259400035_Auto-Encoding_Variational_Bayes","abstraction":"Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and in case of large datasets? We introduce a novel learning and approximate inference method that works efficiently, under some mild conditions, even in the on-line and intractable case. The method involves optimization of a stochastic objective function that can be straightforwardly optimized w.r.t. all parameters, using standard gradient-based optimization methods. The method does not require the typically expensive sampling loops per datapoint required for Monte Carlo EM, and all parameter updates correspond to optimization of the variational lower bound of the marginal likelihood, unlike the wake-sleep algorithm. These theoretical advantages are reflected in experimental results."},{"id":33,"title":"Infinite Sparse Factor Analysis and Infinite Independent Components Analysis","url":"https://www.researchgate.net/publication/220848162_Infinite_Sparse_Factor_Analysis_and_Infinite_Independent_Components_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A nonparametric Bayesian extension of Independent Components Analysis (ICA) is proposed where observed data Y is modelled as a linear superposition, G, of a potentially infinite number of hidden sources, X. Whether a given source is active for a specific data point is specified by an infinite binary matrix, Z. The resulting sparse representation allows increased data reduction compared to standard ICA. We define a prior on Z using the Indian Buffet Process (IBP). We describe four variants of the model, with Gaussian or Laplacian priors on X and the one or two-parameter IBPs. We demonstrate Bayesian inference under these models using a Markov Chain Monte Carlo (MCMC) algorithm on synthetic and gene expression data and compare to standard ICA algorithms.\n</div> \n<p></p>"},{"id":34,"title":"Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression","url":"https://www.researchgate.net/publication/228083473_Fixed-Form_Variational_Posterior_Approximation_through_Stochastic_Linear_Regression","abstraction":"We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice."},{"id":35,"title":"Variational Inference for Bayesian Mixtures of Factor Analysers","url":"https://www.researchgate.net/publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present an algorithm that infers the model structure of a mixture of factor analysers using an ecient and deterministic variational approximation to full Bayesian integration over model parameters. This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i.e. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to over tting. Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples. By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exa...\n</div> \n<p></p>"},{"id":36,"title":"Sparse Stochastic Inference for Latent Dirichlet allocation","url":"https://www.researchgate.net/publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation","abstraction":"We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models."},{"id":37,"title":"Nonparametric Bayes Estimators Based on Beta Processes in Models for Life History Data","url":"https://www.researchgate.net/publication/38359431_Nonparametric_Bayes_Estimators_Based_on_Beta_Processes_in_Models_for_Life_History_Data","abstraction":"Several authors have constructed nonparametric Bayes estimators for a cumulative distribution function based on (possibly right-censored) data. The prior distributions have, for example, been Dirichlet processes or, more generally, processes neutral to the right. The present article studies the related problem of finding Bayes estimators for cumulative hazard rates and related quantities, w.r.t. prior distributions that correspond to cumulative hazard rate processes with nonnegative independent increments. A particular class of prior processes, termed beta processes, is introduced and is shown to constitute a conjugate class. To arrive at these, a nonparametric time-discrete framework for survival data, which has some independent interest, is studied first. An important bonus of the approach based on cumulative hazards is that more complicated models for life history data than the simple life table situation can be treated, for example, time-inhomogeneous Markov chains. We find posterior distributions and derive Bayes estimators in such models and also present a semiparametric Bayesian analysis of the Cox regression model. The Bayes estimators are easy to interpret and easy to compute. In the limiting case of a vague prior the Bayes solution for a cumulative hazard is the Nelson-Aalen estimator and the Bayes solution for a survival probability is the Kaplan-Meier estimator."},{"id":38,"title":"Infinite Latent Feature Models and the Indian Buffet Process","url":"https://www.researchgate.net/publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process","abstraction":"We define a probability distribution over equivalence classes of binary ma- trices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We derive the distribution by taking the limit of a distribution over N × K binary matrices as K ! 1, a strategy inspired by the derivation of the Chinese restaurant process (Aldous, 1985; Pitman, 2002) as the limit of a Dirichlet-multinomial model. This strategy preserves the exchangeability of the rows of matrices. We define several simple generative processes that result in the same distri- bution over equivalence classes of binary matrices, one of which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algo- rithm for inference in this model and applying this algorithm to an artificial dataset."},{"id":39,"title":"Natural Gradient Works Efficiently in Learning","url":"https://www.researchgate.net/publication/2433873_Natural_Gradient_Works_Efficiently_in_Learning","abstraction":"When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural graadient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behaviour of natural gradient online learning is analyzed and is proved to be Fischer efficient, implying that it has assymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."},{"id":40,"title":"Estimating Mixture of Dirichlet Process Models","url":"https://www.researchgate.net/publication/2468788_Estimating_Mixture_of_Dirichlet_Process_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Current Gibbs sampling schemes in mixture of Dirichlet process (MDP) models are restricted to using \"conjugate\" base measures which allow analytic evaluation of the transition probabilities when resampling configurations, or alternatively need to rely on approximate numeric evaluations of some transition probabilities. Implementation of Gibbs sampling in more general MDP models is an open and important problem since most applications call for the use of non-conjugate base measures. In this paper we propose a conceptual framework for computational strategies. This framework provides a perspective on current methods, facilitates comparisons between them, and leads to several new methods that expand the scope of MDP models to non-conjugate situations. We discuss one in detail. The basic strategy is based on expanding the parameter vector, and is applicable for MDP models with arbitrary base measure and likelihood. Strategies are also presented for the important class of normal-normal MDP ...\n</div> \n<p></p>"},{"id":41,"title":"Sparse On-Line Gaussian Processes","url":"https://www.researchgate.net/publication/11500673_Sparse_On-Line_Gaussian_Processes","abstraction":"We develop an approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets. The method is based on a combination of a Bayesian on-line algorithm, together with a sequential construction of a relevant subsample of the data that fully specifies the prediction of the GP model. By using an appealing parameterization and projection techniques in a reproducing kernel Hilbert space, recursions for the effective parameters and a sparse gaussian approximation of the posterior process are obtained. This allows for both a propagation of predictions and Bayesian error measures. The significance and robustness of our approach are demonstrated on a variety of experiments."},{"id":42,"title":"Bayesian Filtering and Smoothing","url":"https://www.researchgate.net/publication/259390620_Bayesian_Filtering_and_Smoothing","abstraction":"Filtering and smoothing methods are used to produce an accurate estimate of the state of a time-varying system based on multiple observational inputs (data). Interest in these methods has exploded in recent years, with numerous applications emerging in fields such as navigation, aerospace engineering, telecommunications and medicine. This compact, informal introduction for graduate students and advanced undergraduates presents the current state-of-the-art filtering and smoothing methods in a unified Bayesian framework. Readers learn what non-linear Kalman filters and particle filters are, how they are related, and their relative advantages and disadvantages. They also discover how state-of-the-art Bayesian parameter estimation methods can be combined with state-of-the-art filtering and smoothing algorithms. The book's practical and algorithmic approach assumes only modest mathematical prerequisites. Examples include Matlab computations, and the numerous end-of-chapter exercises include computational assignments. Matlab code is available for download at www.cambridge.org/sarkka, promoting hands-on work with the methods."},{"id":43,"title":"A comparative evaluation of stochastic-based inference methods for Gaussian process models","url":"https://www.researchgate.net/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Gaussian Process (GP) models are extensively used in data analysis given their flexible modeling capabilities and interpretability. The fully Bayesian treatment of GP models is analytically intractable, and therefore it is necessary to resort to either deterministic or stochastic approximations. This paper focuses on stochastic-based inference techniques. After discussing the challenges associated with the fully Bayesian treatment of GP models, a number of inference strategies based on Markov chain Monte Carlo methods are presented and rigorously assessed. In particular, strategies based on efficient parameterizations and efficient proposal mechanisms are extensively compared on simulated and real data on the basis of convergence speed, sampling efficiency, and computational cost.\n</div> \n<p></p>"},{"id":44,"title":"Variational Inference for Gaussian Process Modulated Poisson Processes","url":"https://www.researchgate.net/publication/267759656_Variational_Inference_for_Gaussian_Process_Modulated_Poisson_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present the first fully variational Bayesian inference scheme for\n <br> continuous Gaussian-process-modulated Poisson processes. Such point processes\n <br> are used in a variety of domains, including neuroscience, geo-statistics and\n <br> astronomy, but their use is hindered by the computational cost of existing\n <br> inference schemes. Our scheme: requires no discretisation of the domain; scales\n <br> linearly in the number of observed events; and is many orders of magnitude\n <br> faster than previous sampling based approaches. The resulting algorithm is\n <br> shown to outperform standard methods on synthetic examples, coal mining\n <br> disaster data and in the prediction of Malaria incidences in Kenya.\n</div> \n<p></p>"},{"id":45,"title":"Scalable Variational Gaussian Process Classification","url":"https://www.researchgate.net/publication/268079368_Scalable_Variational_Gaussian_Process_Classification","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Gaussian process classification is a popular method with a number of\n <br> appealing properties. We show how to scale the model within a variational\n <br> inducing point framework, outperforming the state of the art on benchmark\n <br> datasets. Importantly, the variational formulation can be exploited to allow\n <br> classification in problems with millions of data points, as we demonstrate in\n <br> experiments.\n</div> \n<p></p>"},{"id":46,"title":"A Unifying View of Sparse Approximate Gaussian Process Regression","url":"https://www.researchgate.net/publication/41781406_A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.\n</div> \n<p></p>"},{"id":47,"title":"Scaling Limits for the Transient Phase of Local Metropolis-Hastings Algorithms","url":"https://www.researchgate.net/publication/2834582_Scaling_Limits_for_the_Transient_Phase_of_Local_Metropolis-Hastings_Algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper considers high-dimensional Metropolis and Langevin algorithms in their initial transient phase. In stationarity, these algorithms are well-understood and it is now well-known how to scale their proposal distribution variances. For the random walk Metropolis algorithm, convergence during the transient phase is extremely regular - to the extent that the algorithm's sample path actually resembles a deterministic trajectory. In contrast, the Langevin algorithm with variance scaled to be optimal for stationarity, performs rather erratically. We give weak convergence results which explain both of these types of behaviour, and give practical guidance on implementation based on our theory.\n</div> \n<p></p>"},{"id":48,"title":"Log Gaussian Cox Processes","url":"https://www.researchgate.net/publication/227701452_Log_Gaussian_Cox_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Planar Cox processes directed by a log Gaussian intensity process are investigated in the univariate and multivariate cases. The appealing properties of such models are demonstrated theoretically as well as through data examples and simulations. In particular, the first, second and third-order properties are studied and utilized in the statistical analysis of clustered point patterns. Also empirical Bayesian inference for the underlying intensity surface is considered.\n</div> \n<p></p>"},{"id":49,"title":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models","url":"https://www.researchgate.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The recently developed Bayesian Gaussian process latent variable model\n <br> (GPLVM) is a powerful generative model for discovering low dimensional\n <br> embeddings in linear time complexity. However, modern datasets are so large\n <br> that even linear-time models find them difficult to cope with. We introduce a\n <br> novel re-parametrisation of variational inference for the GPLVM and sparse GP\n <br> model that allows for an efficient distributed inference algorithm.\n <br> We present a unifying derivation for both models, analytically deriving the\n <br> optimal variational distribution over the inducing points. We then assess the\n <br> suggested inference on datasets of different sizes, showing that it scales well\n <br> with both data and computational resources. We furthermore demonstrate its\n <br> practicality in real-world settings using datasets with up to 100 thousand\n <br> points, comparing the inference to sequential implementations, assessing the\n <br> distribution of the load among the different nodes, and testing its robustness\n <br> to network failures.\n</div> \n<p></p>"},{"id":50,"title":"Assessing Approximate Inference for Binary Gaussian Process Classification","url":"https://www.researchgate.net/publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification","abstraction":"Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace‘s method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classification model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace‘s method."},{"id":51,"title":"Adaptive Multiple Importance Sampling for Gaussian Processes","url":"https://www.researchgate.net/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","abstraction":"In applications of Gaussian processes where quantification of uncertainty is a strict requirement, it is necessary to accurately characterize the posterior distribution over Gaussian process covariance parameters. Normally, this is done by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on Gaussian process regression where the marginal likelihood is computable but expensive to evaluate, this paper studies algorithms based on importance sampling to carry out expectations under the posterior distribution over covariance parameters. The results indicate that expectations computed using Adaptive Multiple Importance Sampling converge faster per unit of computation than those computed with MCMC algorithms for models with few covariance parameters, and converge as fast as MCMC for models with up to around twenty covariance parameters."},{"id":52,"title":"The pseudo-marginal approach for effcient Monte Carlo computations","url":"https://www.researchgate.net/publication/24168169_The_pseudo-marginal_approach_for_effcient_Monte_Carlo_computations","abstraction":"We introduce a powerful and flexible MCMC algorithm for stochastic simulation. The method builds on a pseudo-marginal method originally introduced in [Genetics 164 (2003) 1139--1160], showing how algorithms which are approximations to an idealized marginal algorithm, can share the same marginal stationary distribution as the idealized method. Theoretical results are given describing the convergence properties of the proposed method, and simple numerical examples are given to illustrate the promising empirical characteristics of the technique. Interesting comparisons with a more obvious, but inexact, Monte Carlo approximation to the marginal algorithm, are also given."},{"id":53,"title":"A tutorial on adaptive MCMC","url":"https://www.researchgate.net/publication/49458431_A_tutorial_on_adaptive_MCMC","abstraction":"We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem."},{"id":54,"title":"Estimation of small failure probabilities in high dimensions by Subset Simulation","url":"https://www.researchgate.net/publication/222546544_Estimation_of_small_failure_probabilities_in_high_dimensions_by_Subset_Simulation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A new simulation approach, called ‘subset simulation’, is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made sufficiently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and efficiently estimated by means of simulation. The conditional probabilities cannot be estimated efficiently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and efficient in computing small probabilities. The efficiency of the method is demonstrated by calculating the first-excursion probabilities for a linear oscillator subjected to white noise excitation and for a five-story nonlinear hysteretic shear building under uncertain seismic excitation.\n</div> \n<p></p>"},{"id":55,"title":"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization","url":"https://www.researchgate.net/publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In the field of evolutionary multi-criterion optimization, the hypervolume indicator is the only single set quality measure that is known to be strictly monotonic with regard to Pareto dominance: whenever a Pareto set approximation entirely dominates another one, then the indicator value of the dominant set will also be better. This property is of high interest and relevance for problems involving a large number of objective functions. However, the high computational effort required for hypervolume calculation has so far prevented the full exploitation of this indicator's potential; current hypervolume-based search algorithms are limited to problems with only a few objectives. This paper addresses this issue and proposes a fast search algorithm that uses Monte Carlo simulation to approximate the exact hypervolume values. The main idea is not that the actual indicator values are important, but rather that the rankings of solutions induced by the hypervolume indicator. In detail, we present HypE, a hypervolume estimation algorithm for multi-objective optimization, by which the accuracy of the estimates and the available computing resources can be traded off; thereby, not only do many-objective problems become feasible with hypervolume-based search, but also the runtime can be flexibly adapted. Moreover, we show how the same principle can be used to statistically compare the outcomes of different multi-objective optimizers with respect to the hypervolume--so far, statistical testing has been restricted to scenarios with few objectives. The experimental results indicate that HypE is highly effective for many-objective problems in comparison to existing multi-objective evolutionary algorithms. HypE is available for download at http://www.tik.ee.ethz.ch/sop/download/supplementary/hype/.\n</div> \n<p></p>"},{"id":56,"title":"Sequential design of computer experiments for the estimation of a probability of failure","url":"https://www.researchgate.net/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper deals with the problem of estimating the volume of the excursion set of a function f:?d\n <br> ?? above a given threshold, under a probability measure on ?d\n <br> that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of f and aim at performing evaluations of f as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure.\n</div> \n<p></p>"},{"id":57,"title":"Sequential Monte Carlo for rare event estimation","url":"https://www.researchgate.net/publication/226031060_Sequential_Monte_Carlo_for_rare_event_estimation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper discusses a novel strategy for simulating rare events and an associated Monte Carlo estimation of tail probabilities.\n <br> Our method uses a system of interacting particles and exploits a Feynman-Kac representation of that system to analyze their\n <br> fluctuations. Our precise analysis of the variance of a standard multilevel splitting algorithm reveals an opportunity for\n <br> improvement. This leads to a novel method that relies on adaptive levels and produces, in the limit of an idealized version\n <br> of the algorithm, estimates with optimal variance. The motivation for this theoretical work comes from problems occurring\n <br> in watermarking and fingerprinting of digital contents, which represents a new field of applications of rare event simulation\n <br> techniques. Some numerical results show performance close to the idealized version of our technique for these practical applications.\n <br> \n <br> KeywordsRare event–Sequential importance sampling–Feynman-Kac formula–Metropolis-Hastings–Fingerprinting–Watermarking\n</div> \n<p></p>"},{"id":58,"title":"Constrained Multi-objective Optimization Using Steady State Genetic Algorithms","url":"https://www.researchgate.net/publication/220742835_Constrained_Multi-objective_Optimization_Using_Steady_State_Genetic_Algorithms","abstraction":"In this paper we propose two novel approaches for solving constrained multi-objective optimization problems using steady state GAs. These methods are intended for solving real-world application problems that have many constraints and very small feasible regions. One method called Objective Exchange Genetic Algorithm for Design Optimization (OEGADO) runs several GAs concurrently with each GA optimizing one objective and exchanging information about its objective with the others. The other method called Objective Switching Genetic Algorithm for Design Optimization (OSGADO) runs each objective sequentially with a common population for all objectives. Empirical results in benchmark and engineering design domains are presented. A comparison between our methods and Non-Dominated Sorting Genetic Algorithm-II (NSGA-II) shows that our methods performed better than NSGA-II for difficult problems and found Pareto-optimal solutions in fewer objective evaluations. The results suggest that our methods are better applicable for solving real-world application problems wherein the objective computation time is large."},{"id":59,"title":"Fast Parallel Kriging-Based Stepwise Uncertainty Reduction With Application to the Identification of an Excursion Set","url":"https://www.researchgate.net/publication/228438518_Fast_Parallel_Kriging-Based_Stepwise_Uncertainty_Reduction_With_Application_to_the_Identification_of_an_Excursion_Set","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A Stepwise Uncertainty Reduction (SUR) strategy aims at constructing a sequence X 1 (f), X 2 (f), . . . of evaluation points of a function f : R d ? R in such a way that the residual uncertainty about a quantity of interest S(f) given the information provided by the evaluation results is small. In Bect, Ginsbourger, Li, Picheny and Vazquez, Statistics and Computing, 2011, several SUR approaches have been shown to be particularly efficient for the problem of estimating the volume of an excursion set of a function f above a threshold. Here, we build upon these results and we present fast implementations of some SUR strategies, which are based on two ideas. The first idea is to take advantage of update formulas for kriging. The second idea is to derive closed-form expressions for some integrals that appear in the SUR criteria. We are able to demonstrate significant speed-ups and we illustrate our algorithms on a nuclear safety application.\n</div> \n<p></p>"},{"id":60,"title":"Latent Dirichlet Allocation","url":"https://www.researchgate.net/publication/221620547_Latent_Dirichlet_Allocation","abstraction":"We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model."},{"id":61,"title":"Automatic Image Annotation and Retrieval using Cross-Media Relevance Models","url":"https://www.researchgate.net/publication/2480258_Automatic_Image_Annotation_and_Retrieval_using_Cross-Media_Relevance_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Libraries have traditionally used manual image annotation for indexing and then later retrieving their image collections. However, manual image annotation is an expensive and labor intensive procedure and hence there has been great interest in coming up with automatic ways to retrieve images based on content. Here, we propose an automatic approach to annotating and retrieving images based on a training set of images. We assume that regions in an image can be described using a small vocabulary of blobs. Blobs are generated from image features using clustering. Given a training set of images with annotations, we show that probabilistic models allow us to predict the probability of generating a word given the blobs in an image. This may be used to automatically annotate and retrieve images given a word as a query. We show that relevance models. allow us to derive these probabilities in a natural way. Experiments show that the annotation performance of this cross-media rele- vance model is almost six times as good (in terms of mean precision) than a model based on word-blob co-occurrence model and twice as good as a state of the art model derived from machine translation. Our approach shows the usefulness of using formal information retrieval models for the task of image annotation and retrieval.\n</div> \n<p></p>"},{"id":62,"title":"Matching Words and Pictures","url":"https://www.researchgate.net/publication/2491892_Matching_Words_and_Pictures","abstraction":"We present a new and very rich approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images."},{"id":63,"title":"Propagation Algorithms for Variational Bayesian Learning","url":"https://www.researchgate.net/publication/2495015_Propagation_Algorithms_for_Variational_Bayesian_Learning","abstraction":"Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set."},{"id":64,"title":"Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models","url":"https://www.researchgate.net/publication/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models","abstraction":"We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods."},{"id":65,"title":"Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions","url":"https://www.researchgate.net/publication/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a novel method for multiple clustering that assumes a\n <br> co-clustering structure (partitions in both rows and columns of the data\n <br> matrix) in each view. The new method is applicable to high-dimensional data. It\n <br> is based on a nonparametric Bayesian approach in which the number of views and\n <br> the number of feature-/subject clusters are inferred in a data-driven manner.\n <br> We simultaneously model different distribution families, such as Gaussian,\n <br> Poisson, and multinomial distributions in each cluster block. This makes our\n <br> method applicable to datasets consisting of both numerical and categorical\n <br> variables, which biomedical data typically do. Clustering solutions are based\n <br> on variational inference with mean field approximation. We apply the proposed\n <br> method to synthetic and real data, and show that our method outperforms other\n <br> multiple clustering methods both in recovering true cluster structures and in\n <br> computation time. Finally, we apply our method to a depression dataset with no\n <br> true cluster structure available, from which useful inferences are drawn about\n <br> possible clustering structures of the data.\n</div> \n<p></p>"},{"id":66,"title":"Robust Frequency-Hopping Spectrum Estimation Based on Sparse Bayesian Method","url":"https://www.researchgate.net/publication/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method","abstraction":"This paper considers the problem of estimating multiple frequency hopping signals with unknown hopping pattern. By segmenting the received signals into overlapped measurements and leveraging the property that frequency content at each time instant is intrinsically parsimonious, a sparsity-inspired high-resolution time-frequency representation (TFR) is developed to achieve robust estimation. Inspired by the sparse Bayesian learning algorithm, the problem is formulated hierarchically to induce sparsity. In addition to the sparsity, the hopping pattern is exploited via temporal-aware clustering by exerting a dependent Dirichlet process prior over the latent parametric space. The estimation accuracy of the parameters can be greatly improved by this particular information-sharing scheme, and sharp boundary of the hopping time estimation is manifested. Moreover, the proposed algorithm is further extended to multi-channel cases, where task-relation is utilized to obtain robust clustering of the latent parameters for better estimation performance. Since the problem is formulated in a full Bayesian framework, laborintensive parameter tuning process can be avoided. Another superiority of the approach is that high-resolution instantaneous frequency estimation can be directly obtained without further refinement of the TFR. Results of numerical experiments show that the proposed algorithm can achieve superior performance particularly in low signal-to-noise ratio scenarios compared with other recently reported ones."},{"id":67,"title":"Deep Learning with Nonparametric Clustering","url":"https://www.researchgate.net/publication/270906298_Deep_Learning_with_Nonparametric_Clustering","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Clustering is an essential problem in machine learning and data mining. One\n <br> vital factor that impacts clustering performance is how to learn or design the\n <br> data representation (or features). Fortunately, recent advances in deep\n <br> learning can learn unsupervised features effectively, and have yielded state of\n <br> the art performance in many classification problems, such as character\n <br> recognition, object recognition and document categorization. However, little\n <br> attention has been paid to the potential of deep learning for unsupervised\n <br> clustering problems. In this paper, we propose a deep belief network with\n <br> nonparametric clustering. As an unsupervised method, our model first leverages\n <br> the advantages of deep learning for feature representation and dimension\n <br> reduction. Then, it performs nonparametric clustering under a maximum margin\n <br> framework -- a discriminative clustering model and can be trained online\n <br> efficiently in the code space. Lastly model parameters are refined in the deep\n <br> belief network. Thus, this model can learn features for clustering and infer\n <br> model complexity in an unified framework. The experimental results show the\n <br> advantage of our approach over competitive baselines.\n</div> \n<p></p>"},{"id":68,"title":"A Latent Manifold Markovian Dynamics Gaussian Process","url":"https://www.researchgate.net/publication/260985227_A_Latent_Manifold_Markovian_Dynamics_Gaussian_Process","abstraction":"In this paper, we propose a Gaussian process model for analysis of nonlinear time series. Formulation of our model is based on the consideration that the observed data are functions of latent variables, with the associated mapping between observations and latent representations modeled through Gaussian process priors. In addition, to capture the temporal dynamics in the modeled data, we assume that subsequent latent representations depend on each other on the basis of a hidden Markov prior imposed over them. Derivation of our model is performed by marginalizing out the model parameters in closed form by using Gaussian process priors for observation mappings, and appropriate stick-breaking priors for the latent variable (Markovian) dynamics. This way, we eventually obtain a nonparametric Bayesian model for dynamical systems that accounts for uncertainty in the modeled data. We provide efficient inference algorithms for our model on the basis of a truncated variational Bayesian approximation. We demonstrate the efficacy of our approach considering a number of applications dealing with real-world data, and compare it to related state-of-the-art approaches."},{"id":69,"title":"Covariance Matrices for Mean Field Variational Bayes","url":"https://www.researchgate.net/publication/267454297_Covariance_Matrices_for_Mean_Field_Variational_Bayes","abstraction":"Mean Field Variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is its (sometimes severe) underestimates of the uncertainty of model variables and lack of information about model variable covariance. We develop a fast, general methodology for exponential families that augments MFVB to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables. MFVB for exponential families defines a fixed-point equation in the means of the approximating posterior, and our approach yields a covariance estimate by perturbing this fixed point. Inspired by linear response theory, we call our method linear response variational Bayes (LRVB). We demonstrate the accuracy of our method on simulated data sets."},{"id":70,"title":"Variational Bayes for Merging Noisy Databases","url":"https://www.researchgate.net/publication/267099477_Variational_Bayes_for_Merging_Noisy_Databases","abstraction":"Bayesian entity resolution merges together multiple, noisy databases and returns the minimal collection of unique individuals represented, together with their true, latent record values. Bayesian methods allow flexible generative models that share power across databases as well as principled quantification of uncertainty for queries of the final, resolved database. However, existing Bayesian methods for entity resolution use Markov monte Carlo method (MCMC) approximations and are too slow to run on modern databases containing millions or billions of records. Instead, we propose applying variational approximations to allow scalable Bayesian inference in these models. We derive a coordinate-ascent approximation for mean-field variational Bayes, qualitatively compare our algorithm to existing methods, note unique challenges for inference that arise from the expected distribution of cluster sizes in entity resolution, and discuss directions for future work in this domain."},{"id":71,"title":"Camera Control For Learning Nonlinear Target Dynamics via Bayesian Non-Parametric Dirichlet-Process Gaussian-Process (DP-GP) Models","url":"https://www.researchgate.net/publication/265383462_Camera_Control_For_Learning_Nonlinear_Target_Dynamics_via_Bayesian_Non-Parametric_Dirichlet-Process_Gaussian-Process_DP-GP_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a camera control approach for learning unknown nonlinear target dynamics by approximating information value functions using particles that represent targets' position distributions. The target dynamics are described by a non-parametric mixture model that can learn a potentially infinite number of motion patterns. Assuming that each motion pattern can be represented as a velocity field, the target behaviors can be described by a non-parametric Dirichlet process-Gaussian process (DP-GP) mixture model. The DP-GP model has been successfully applied for clustering time-invariant spatial phenomena due to its flexibility to adapt to data complexity without overfitting. A new DP-GP information value function is presented that can be used by the sensor to explore and improve the DP-GP mixture model. The optimal camera control is computed to maximize this information value function online via a computationally efficient particle-based search method. The proposed approach is demonstrated through numerical simulations and hardware experiments in the RAVEN testbed at MIT.\n</div> \n<p></p>"},{"id":72,"title":"Bayesian estimation of Dirichlet mixture model with variational inference","url":"https://www.researchgate.net/publication/262266576_Bayesian_estimation_of_Dirichlet_mixture_model_with_variational_inference","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In statistical modeling, parameter estimation is an essential and challengeable task. Estimation of the parameters in the Dirichlet mixture model (DMM) is analytically intractable, due to the integral expressions of the gamma function and its corresponding derivatives. We introduce a Bayesian estimation strategy to estimate the posterior distribution of the parameters in DMM. By assuming the gamma distribution as the prior to each parameter, we approximate both the prior and the posterior distribution of the parameters with a product of several mutually independent gamma distributions. The extended factorized approximation method is applied to introduce a single lower-bound to the variational objective function and an analytically tractable estimation solution is derived. Moreover, there is only one function that is maximized during iterations and, therefore, the convergence of the proposed algorithm is theoretically guaranteed. With synthesized data, the proposed method shows the advantages over the EM-based method and the previously proposed Bayesian estimation method. With two important multimedia signal processing applications, the good performance of the proposed Bayesian estimation method is demonstrated.\n</div> \n<p></p>"},{"id":73,"title":"Large Scale Image Categorization in Sparse Nonparametric Bayesian Representation","url":"https://www.researchgate.net/publication/269630292_Large_Scale_Image_Categorization_in_Sparse_Nonparametric_Bayesian_Representation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n There are two main contributions in this paper: (1) A new hierarchical sparse coding algorithm is proposed, which combines hard and soft assignment coding in a fully unsupervised manner for image categorization. Hard coding assigns data into independent cluster globally to enable local dictionary learning to be learned by soft coding. The new coding algorithm is characterized by better fitting of data, more discriminative global clustering, low computational complexity and convergence speed up. (2) We utilize variational inference optimization on large data to solve regularized and optimization problem in the proposed hard soft sparse coding algorithm. Different from other convex optimization algorithm for sparse coding, the proposed algorithm has no limitation or preference on the data with proper prior estimation. In our experiments, we tested MIT 8 scene categories data-set and achieved a 10% improvement on the best existing algorithm with faster convergence rate.\n</div> \n<p></p>"},{"id":74,"title":"The automatic measurement of facial beauty","url":"https://www.researchgate.net/publication/3927745_The_automatic_measurement_of_facial_beauty","abstraction":"We develop an automatic facial beauty scoring system based on ratios between facial features. After isolating the face, eyes, eyebrows and mouth in a portrait photograph, we represent a face abstractly as an 8-element vector of ratios between these features. We use a variant of the K-nearest neighbor algorithm, in the context of a parameterized metric space optimized using a genetic algorithm, to learn a beauty assignment function from a training set of photographs rated by humans. We assess performance on a test set of photographs, concluding that when facial ratios are accurately extracted in the computer vision phase, the results of the program are highly correlated with median-human ratings of beauty"},{"id":75,"title":"Thin Slices of Expressive Behavior as Predictors of Interpersonal Consequences: A Meta-Analysis","url":"https://www.researchgate.net/publication/229059871_Thin_Slices_of_Expressive_Behavior_as_Predictors_of_Interpersonal_Consequences_A_Meta-Analysis","abstraction":"A meta-analysis was conducted on the accuracy of predictions of various objective outcomes in the areas of clinical and social psychology from short observations of expressive behavior (under 5 min). The overall effect size for the accuracy of predictions for 38 different results was .39. Studies using longer periods of behavioral observation did not yield greater predictive accuracy; predictions based on observations under 0.5 min in length did not differ significantly from predictions based on 4- and 5-min observations. The type of behavioral channel (such as the face, speech, the body, tone of voice) on which the ratings were based was not related to the accuracy of predictions. Accuracy did not vary significantly between behaviors manipulated in a laboratory and more naturally occurring behavior. Last, effect sizes did not differ significantly for predictions in the areas of clinical psychology, social psychology, and the accuracy of detecting deception. (PsycINFO Database Record (c) 2012 APA, all rights reserved)"},{"id":76,"title":"Virtual Interpersonal Touch and Digital Chameleons","url":"https://www.researchgate.net/publication/225766702_Virtual_Interpersonal_Touch_and_Digital_Chameleons","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We studied the characteristics of hand touch with a mechanical device that approximated a handshake, and we then examined\n <br> the effect of handshake mimicry on assessment of a partner. Two participants interacted with a force-feedback joystick that\n <br> recorded each of their hand movements individually. The two participants then greeted one another by feeling the recording\n <br> of the other person’s movements via the force-feedback device. For each dyad, one of the participants actually received his\n <br> or her own virtual handshake back under the guise that it was the other person’s virtual handshake. Results demonstrated three\n <br> significant findings. First, for any given participant, a metric that took into account position, angle, speed, and acceleration\n <br> of the hand movements correlated highly within individuals across two handshakes. Second, across participants, these metrics\n <br> demonstrated specific differences by gender. Finally, there was an interaction between gender and mimicry, such that male\n <br> participants liked people who mimicked their handshakes more than female participants did. We discuss the implications of\n <br> these findings and relate them to theories of social interaction.\n</div> \n<p></p>"},{"id":77,"title":"Using simple speech based features to detect the state of a meeting and the roles of the meeting participants","url":"https://www.researchgate.net/publication/221490832_Using_simple_speech_based_features_to_detect_the_state_of_a_meeting_and_the_roles_of_the_meeting_participants","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We introduce a simple taxonomy of meeting states and participant roles. Our goal is to automatically de- tect the state of a meeting and the role of each meeting participant and to do so concurrent with a meeting. We trained a decision tree classifier that learns to detect these states and roles from simple speech-based features that are easy to compute automatically. This classifier detects meeting states 18% absolute more accurately than a ran- dom classifier , and detects participant roles 10% absolute more accurately than a majority classifier . The results im- ply that simple, easy to compute features can be used for this purpose.\n</div> \n<p></p>"},{"id":78,"title":"The components of conversational facial expressions","url":"https://www.researchgate.net/publication/52012703_The_components_of_conversational_facial_expressions","abstraction":"Conversing with others is one of the most central of human behaviours. In any conversation, humans use facial motion to help modify what is said, to control the flow of a dialog, or to convey complex intentions without saying a word. Here, we employ a custom, image-based, stereo motion-tracking algorithm to track and selectively \"freeze\" portions of an actor or actress's face in video recordings in order to determine the necessary and sufficient facial motions for nine conversational expressions. The results show that most expressions rely primarily on a single facial area to convey meaning, with different expressions using different facial areas. The results also show that the combination of rigid head, eye, eyebrow, and mouth motion is sufficient to produce versions of these expressions that are as easy to recognize as the original recordings. Finally, the results show that the manipulation technique introduced few perceptible artifacts into the altered video sequences. The use of advanced computer graphics techniques provided a means to systematically examine real facial expressions. This provides not only fundamental insights into human perception and cognition, but also yields the basis for a systematic description of what needs to be animated in order to produce realistic, recognizable facial expressions."},{"id":79,"title":"Triggs, B.: Histograms of Oriented Gradients for Human Detection. In: CVPR","url":"https://www.researchgate.net/publication/4156272_Triggs_B_Histograms_of_Oriented_Gradients_for_Human_Detection_In_CVPR","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.\n</div> \n<p></p>"},{"id":80,"title":"Using the influence model to recognize functional roles in meetings","url":"https://www.researchgate.net/publication/221052344_Using_the_influence_model_to_recognize_functional_roles_in_meetings","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, an influence model is used to recognize functional roles played during meetings. Previous works on the same corpus demonstrated a high recognition accuracy using SVMs with RBF kernels. In this paper, we discuss the problems of that approach, mainly over-fitting, the curse of dimensionality and the inability to generalize to different group configurations. We present results obtained with an influence modeling method that avoid these problems and ensures both greater robustness and generalization capability.\n</div> \n<p></p>"},{"id":81,"title":"Using Audio and Video Features to Classify the Most Dominant Person in a Group Meeting","url":"https://www.researchgate.net/publication/41387160_Using_Audio_and_Video_Features_to_Classify_the_Most_Dominant_Person_in_a_Group_Meeting","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The automated extraction of semantically meaningful information from multi-modal data is becoming increasingly necessary due to the escalation of captured data for archival. A novel area of multi-modal data labelling, which has received relatively little attention, is the automatic estimation of the most dominant person in a group meeting. In this paper, we provide a framework for detecting dominance in group meetings using different audio and video cues. We show that by using a simple model for dominance estimation we can obtain promising results.\n</div> \n<p></p>"},{"id":82,"title":"Smile and laughter recognition using speech processing and face recognition from conversation video","url":"https://www.researchgate.net/publication/4219163_Smile_and_laughter_recognition_using_speech_processing_and_face_recognition_from_conversation_video","abstraction":"This paper describes a method to detect smiles and laughter sounds from the video of natural dialogue. A smile is the most common facial expression observed in a dialogue. Detecting a user's smiles and laughter sounds can be useful for estimating the mental state of the user of a spoken-dialogue-based user interface. In addition, detecting laughter sound can be utilized to prevent the speech recognizer from wrongly recognizing the laughter sound as meaningful words. In this paper, a method to detect smile expression and laughter sound robustly by combining an image-based facial expression recognition method and an audio-based laughter sound recognition method. The image-based method uses a feature vector based on feature point detection from face images. The method could detect smile faces by more than 80% recall and precision rate. A method to combine a GMM-based laughter sound recognizer and the image-based method could improve the accuracy of detection of laughter sounds compared with methods that use image or sound only. As a result, more than 70% recall and precision rate of laughter sound detection was obtained from the natural conversation videos."},{"id":83,"title":"An Analysis of Rhythmic Staccato-Vocalization Based on Frequency Demodulation for Laughter Detection in Conversational Meetings","url":"https://www.researchgate.net/publication/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Human laugh is able to convey various kinds of meanings in human\n <br> communications. There exists various kinds of human laugh signal, for example:\n <br> vocalized laugh and non vocalized laugh. Following the theories of psychology,\n <br> among all the vocalized laugh type, rhythmic staccato-vocalization\n <br> significantly evokes the positive responses in the interactions. In this paper\n <br> we attempt to exploit this observation to detect human laugh occurrences, i.e.,\n <br> the laughter, in multiparty conversations from the AMI meeting corpus. First,\n <br> we separate the high energy frames from speech, leaving out the low energy\n <br> frames through power spectral density estimation. We borrow the algorithm of\n <br> rhythm detection from the area of music analysis to use that on the high energy\n <br> frames. Finally, we detect rhythmic laugh frames, analyzing the candidate\n <br> rhythmic frames using statistics. This novel approach for detection of\n <br> `positive' rhythmic human laughter performs better than the standard laughter\n <br> classification baseline.\n</div> \n<p></p>"},{"id":84,"title":"Cross-cultural Training Analysis Via Social Science and Computer Vision Methods","url":"https://www.researchgate.net/publication/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Computer Vision technology is an invaluable addition to cross-cultural communication training for military personnel. It allows trainers to assess trainees in real time and provide feedback grounded in social science research. The present study reports on a joint analysis of military cross-cultural training data by Computer Vision specialists from GE Global Research as well as analyses from Georgetown University's Social Interaction Research Group (SIRG). Data for this study were collected over 10 days at the Army Infantry Basic Officer Leaders Course (IBOLC). 80 lieutenants participated in classroom role-play scenarios designed to assess their ability to communicate cross-culturally. GE and SIRG researchers video-recorded interactions among the role players and Soldiers and correlations were observed between these automatic interpretations and those gleaned by the SIRG analysis team in order to augment understanding of the efficacy of the cross-cultural training. For the Computer Vision methods, Each person was represented as a stream of visual cues which include: position, articulated motion, facial expressions and gaze directions. The social science researchers conducted multimodal (including embodied elements such as eye gaze, hand gestures, and body positioning), mixed method (qualitative and quantitative) discourse analyses of the data. SIRG researchers developed a coding scheme, marking specific human behavioral features within each interaction. From such coding, SIRG identified key skills in cross-cultural interaction, including observation and adaptation to unfamiliar communicative norms, rapport building, and trouble recovery (for details see Logan-Terry &amp; Damari, forthcoming). Various correlations between raw computer vision measurements and the social science coding scheme was observed. Such results represent a significant step towards establishing the efficacy of the joint analysis of automated Computer Vision and established social science methods with regards to complex social interaction analysis.\n</div> \n<p></p>"},{"id":85,"title":"Learning Social Relation Traits from Face Images","url":"https://www.researchgate.net/publication/281809119_Learning_Social_Relation_Traits_from_Face_Images","abstraction":"Social relation defines the association, e.g, warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine-grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos."},{"id":86,"title":"Measuring mimicry in task-oriented conversations: degree of mimicry is related to task difficulty","url":"https://www.researchgate.net/publication/283624647_Measuring_mimicry_in_task-oriented_conversations_degree_of_mimicry_is_related_to_task_difficulty","abstraction":"The tendency to unconsciously imitate others in conversations has been referred to as mimicry, accommodation, interpersonal adaptation, etc. During the last few years, the computing community has made significant efforts towards the automatic detection of the phenomenon, but a widely accepted approach is still missing. Given that mimicry is the unconscious tendency to imitate others, this article proposes the adoption of speaker verification methodologies that were originally conceived to spot people trying to forge the voice of others. Preliminary experiments suggest that mimicry can be detected using this methodology by measuring how much speakers converge or diverge with respect to one another in terms of acoustic evidence. As a validation of the approach, the experiments show that convergence (speakers becoming more similar in terms of acoustic properties) tends to appear more frequently when the DiapixUK task requires more time to be completed and, therefore, is more difficult. This is interpreted as an attempt to improve communication through increased coherence."},{"id":87,"title":"Laughter and Filler Detection in Naturalistic Audio","url":"https://www.researchgate.net/publication/281492290_Laughter_and_Filler_Detection_in_Naturalistic_Audio","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Laughter and fillers are common phenomenon in speech, and play an important role in communication. In this study, we present Deep Neural Network (DNN) and Convolutional Neu-ral Network (CNN) based systems to classify non-verbal cues (laughter and fillers) from verbal speech in naturalistic audio. We propose improvements over a deep learning system proposed in [1]. Particularly, we propose a simple method to combine spectral features with pitch information to capture prosodic and spectral cues for filler/laughter. Additionally, we propose using a wider time context for feature extraction so that the time evolution of the spectral and prosodic structure can also be exploited for classification. Furthermore, we propose to use CNN for classification. The new method is evaluated on conversational telephony speech (CTS, drawn from Switchboard and Fisher) data and UT-Opinion corpus. Our results shows that the new system improves the AUC (area under the curve) metric by 8.15% and 11.9% absolute for laughters, and 4.85% and 6.01% absolute for fillers, over the baseline system, for CTS and UT-Opinion data, respectively. Finally, we analyze the results to explain the difference in performance between traditional CTS data and naturalistic audio (UT-Opinion), and identify challenges that need to be addressed to make systems perform better for practical data.\n</div> \n<p></p>"},{"id":88,"title":"Systematic Analysis of Video Data from Different Human-Robot Interaction Studies: A Categorisation of Social Signals During Error Situations","url":"https://www.researchgate.net/publication/279200968_Systematic_Analysis_of_Video_Data_from_Different_Human-Robot_Interaction_Studies_A_Categorisation_of_Social_Signals_During_Error_Situations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Human-robot interactions are often affected by error situations that are caused by either the robot or the human. Therefore, robots would profit from the ability to recognise when error situations occur. We investigated the verbal and non-verbal social signals that humans show when error situations occur in human-robot interaction experiments. For that, we analysed 201 videos of five human-robot interaction user studies with varying tasks from four independent projects. The analysis shows that there are two types of error situations: social norm violations and technical failures. Social norm violations are situations in which the robot does not adhere to the underlying social script of the interaction. Technical failures are caused by technical shortcomings of the robot. The results of the video analysis show that the study participants use many head movements and very few gestures, when in an error situation with the robot. We also found that the participants talked more in the case of social norm violations and less during technical failures. Finally, the participants use fewer non-verbal social signals (for example smiling, nodding, and head shaking), when they are interacting with the robot alone and no experimenter or other human is present. The results suggest that participants do not see the robot as a social interaction partner with comparable communication skills. Our findings have implications for builders and evaluators of human-robot interaction systems. The builders need to consider including modules for recognition and classification of head movements to the robot input channels. The evaluators need to make sure that the presence of an experimenter does not skew the results of their user studies.\n</div> \n<p></p>"},{"id":89,"title":"Varying Social Cue Constellations Results in Different Attributed Social Signals in a Simulated Surveillance Task","url":"https://www.researchgate.net/publication/275952718_Varying_Social_Cue_Constellations_Results_in_Different_Attributed_Social_Signals_in_a_Simulated_Surveillance_Task","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A better understanding of human mental states in social contexts holds the potential to pave the way for implementation of robotic systems capable of more natural and intuitive interaction. In working toward such a goal, this paper reports on a study examining human perception of social signals based on manipulated sets of social cues in a simulated socio-cultural environment. Participants were presented with video vignettes of a simulated marketplace environment in which they took the perspective of an observing robot and were asked to make mental state attributions of a human avatar based on the avatar's expression of a range of social cues. Results indicated that subtly varying combinations of social cues led to participants' perception of different social signals. The different mental state attributions made were also significantly associated with what participants considered an appropriate behavioral response for the robot to exhibit in relation to the avatar. We discuss these results in the context of the development of computational-based perceptual systems to be implemented in socially intelligent robots.\n</div> \n<p></p>"},{"id":90,"title":"Analyzing Trajectories on Grassmann Manifold for Early Emotion Detection from Depth Videos","url":"https://www.researchgate.net/publication/271373736_Analyzing_Trajectories_on_Grassmann_Manifold_for_Early_Emotion_Detection_from_Depth_Videos","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper proposes a new framework for online detection of spontaneous emotions from low-resolution depth se- quences of the upper part of the body. To face the challenges of this scenario, depth videos are decomposed into subsequences, each modeled as a linear subspace, which in turn is represented as a point on a Grassmann manifold. Modeling the temporal evolution of distances between subsequences of the underlying manifold as a one-dimensional signature, termed Geometric Motion History, permits us to encompass the temporal signature into an early detection framework using Structured Output SVM, thus enabling online emotion detection. Results obtained on the publicly available Cam3D Kinect database validate the proposed solution, also demonstrating that the upper body, instead of the face alone, can improve the performance of emotion detection.\n</div> \n<p></p>"},{"id":91,"title":"Connecting Brains and Bodies: Applying Physiological Computing to Support Social Interaction","url":"https://www.researchgate.net/publication/276133356_Connecting_Brains_and_Bodies_Applying_Physiological_Computing_to_Support_Social_Interaction","abstraction":"Physiological and affective computing propose methods to improve human–machine interactions by adapting machines to the users’ states. Recently, social signal processing (SSP) has proposed to apply similar methods to human–human interactions with the hope of better understanding and modeling social interactions. Most of the social signals employed are facial expressions, body movements and speech, but studies using physiological signals remain scarce. In this paper, we motivate the use of physiological signals in the context of social interactions. Specifically, we review studies which have investigated the relationship between various physiological indices and social interactions. We then propose two main directions to apply physiological SSP: using physiological signals of individual users as new social cues displayed in the group and using inter-user physiology to measure properties of the interactions such as conflict and social presence. We conclude that physiological measures have the potential to enhance social interactions and to connect people."},{"id":92,"title":"Ensemble Nyström method for predicting conflict level from speech","url":"https://www.researchgate.net/publication/282846166_Ensemble_Nystrom_method_for_predicting_conflict_level_from_speech","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Nyström method is an efficient technique for scaling kernel learning to very large data sets with more than millions. Instead of computing kernel matrix, it is to approximate a kernel learning problem with a linear prediction problem. We propose an ensemble Nyström method for high dimensional prediction of conflict level from speech. The experiments have been conducted over SSPNet Conflict Corpus, which contains 1430 clips of 30 seconds extracted from 45 political debates (roughly 12 hours of material). The results show that the proposed system is able to learn an optimal set of low-rank approximations and combine them to achieve higher accuracy of the prediction of conflict level. The correlation coefficient can be achieved to 84.94% between the actual and predicted labels, clearly exceeding the performance of the same regression using the original full feature set, as well as the state-of-the-art methods such as random k-nearest neighbor (RKNN) features (winning system in INTERSPEECH 2013 Computational Paralinguistics Challenge), support vector regressions, and regression techniques based on Gaussian Processes.\n</div> \n<p></p>"},{"id":93,"title":"Antoniak, C.E.: Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems. Ann. Stat. 2, 1152-1174","url":"https://www.researchgate.net/publication/38357862_Antoniak_CE_Mixtures_of_Dirichlet_Processes_with_Applications_to_Bayesian_Nonparametric_Problems_Ann_Stat_2_1152-1174","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A random process called the Dirichlet process whose sample functions are almost surely probability measures has been proposed by Ferguson as an approach to analyzing nonparametric problems from a Bayesian viewpoint. An important result obtained by Ferguson in this approach is that if observations are made on a random variable whose distribution is a random sample function of a Dirichlet process, then the conditional distribution of the random measure can be easily calculated, and is again a Dirichlet process. This paper extends Ferguson's result to cases where the random measure is a mixing distribution for a parameter which determines the distribution from which observations are made. The conditional distribution of the random measure, given the observations, is no longer that of a simple Dirichlet process, but can be described as being a mixture of Dirichlet processes. This paper gives a formal definition for these mixtures and develops several theorems about their properties, the most important of which is a closure property for such mixtures. Formulas for computing the conditional distribution are derived and applications to problems in bio-assay, discrimination, regression, and mixing distributions are given.\n</div> \n<p></p>"},{"id":94,"title":"On the Statistical-Analysis of Dirty Pictures","url":"https://www.researchgate.net/publication/233950933_On_the_Statistical-Analysis_of_Dirty_Pictures","abstraction":"A continuous two-dimensional region is partitioned into a fine rectangular array of sites, or ‘pixels', each pixel having a particular '‘colour’ belonging to a prescribed finite set. The true colouring of the region is unknown but, associated with each pixel, there is a possibly multivariate record which conveys imperfect information about its colour according to a known statistical model. The aim is to reconstruct the true scene, with the additional knowledge that pixels close together tend to have the same or similar colours. In this paper, it is assumed that the local characteristics of the true scene can be represented by a non-degenerate Markov random field. Such information can be combined with the records by Bayes' theorem and the true scene can be estimated according to standard criteria. However, the computational burden is enormous and the reconstruction may reflect undesirable large-scale properties of the random field. Thus, a simple, iterative method of reconstruction is proposed, which does not depend on these large-scale characteristics. The method is illustrated by computer simulations in which the original scene is not directly related to the assumed random field. Some complications, including parameter estimation, are discussed. Potential applications are mentioned briefly."},{"id":95,"title":"Variational Methods for the Dirichlet Process","url":"https://www.researchgate.net/publication/224881748_Variational_Methods_for_the_Dirichlet_Process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Variational inference methods, including mean field methods and loopy belief propagation, have been widely used for approximate probabilistic inference in graphical models.\n</div> \n<p></p>"},{"id":96,"title":"A Fuzzy Clustering Approach Toward Hidden Markov Random Field Models for Enhanced Spatially Constrained Image Segmentation","url":"https://www.researchgate.net/publication/224327452_A_Fuzzy_Clustering_Approach_Toward_Hidden_Markov_Random_Field_Models_for_Enhanced_Spatially_Constrained_Image_Segmentation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Hidden Markov random field (HMRF) models have been widely used for image segmentation, as they appear naturally in problems where a spatially constrained clustering scheme, taking into account the mutual influences of neighboring sites, is asked for. Fuzzy c -means (FCM) clustering has also been successfully applied in several image segmentation applications. In this paper, we combine the benefits of these two approaches, by proposing a novel treatment of HMRF models, formulated on the basis of a fuzzy clustering principle. We approach the HMRF model treatment problem as an FCM-type clustering problem, effected by introducing the explicit assumptions of the HMRF model into the fuzzy clustering procedure. Our approach utilizes a fuzzy objective function regularized by Kullback--Leibler divergence information, and is facilitated by application of a mean-field-like approximation of the MRF prior. We experimentally demonstrate the superiority of the proposed approach over competing methodologies, considering a series of synthetic and real-world image segmentation applications.\n</div> \n<p></p>"},{"id":97,"title":"A dirichlet process mixture model for brain MRI tissue classification. Med Image Anal","url":"https://www.researchgate.net/publication/6545061_A_dirichlet_process_mixture_model_for_brain_MRI_tissue_classification_Med_Image_Anal","abstraction":"Accurate classification of magnetic resonance images according to tissue type or region of interest has become a critical requirement in diagnosis, treatment planning, and cognitive neuroscience. Several authors have shown that finite mixture models give excellent results in the automated segmentation of MR images of the human normal brain. However, performance and robustness of finite mixture models deteriorate when the models have to deal with a variety of anatomical structures. In this paper, we propose a nonparametric Bayesian model for tissue classification of MR images of the brain. The model, known as Dirichlet process mixture model, uses Dirichlet process priors to overcome the limitations of current parametric finite mixture models. To validate the accuracy and robustness of our method we present the results of experiments carried out on simulated MR brain scans, as well as on real MR image data. The results are compared with similar results from other well-known MRI segmentation methods."},{"id":98,"title":"Texture Analysis through a Markovian Modelling and FuzzyClassification: Application to Urban Area Extraction fromSatellite Images","url":"https://www.researchgate.net/publication/262362546_Texture_Analysis_through_a_Markovian_Modelling_and_FuzzyClassification_Application_to_Urban_Area_Extraction_fromSatellite_Images","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Herein we propose a complete procedure to analyze and classify the\n <br> texture of an image. We apply this scheme to solve a specific image processing\n <br> problem: urban areas detection in satellite images. First we propose to\n <br> analyze the texture through the modelling of the luminance field with eight\n <br> different chain-based models. We then derived a texture parameter from these\n <br> models. The effect of the lattice anisotropy is corrected by a renormalization\n <br> group technique coming from statistical physics. This parameter, which takes\n <br> into account local conditional variances of the image, is compared to classical\n <br> methods of texture analysis. Afterwards we develop a modified fuzzy Cmeans\n <br> algorithm that includes an entropy term. The advantage of such an algorithm\n <br> is that the number of classes does not need to be known a priori. Besides\n <br> this algorithm provides us with further information, i.e. the probability\n <br> that a given pixel belongs to a given cluster. Finally we introduce this\n <br> information in a Markovian model of segmentation. Some results on SPOT5\n <br> simulated images, SPOT3 images and ERS1 radar images are presented. These\n <br> images are provided by the French National Space Agency (CNES) and\n <br> the European Space Agency (ESA).\n</div> \n<p></p>"},{"id":99,"title":"Estimation of Parameters in Hidden Markov Models","url":"https://www.researchgate.net/publication/259014834_Estimation_of_Parameters_in_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Parameter estimation from noisy versions of realizations of Markov\n <br> models is extremely difficult in all but very simple examples. The paper\n <br> identifies these difficulties, reviews ways of coping with them in\n <br> practice, and discusses in detail a class of methods with a Monte Carlo\n <br> flavour. Their performance on simple examples suggests that they should\n <br> be valuable, practically feasible procedures in the context of a range\n <br> of otherwise intractable problems. An illustration is provided based on\n <br> satellite data.\n</div> \n<p></p>"},{"id":100,"title":"Green, P.J.: On Bayesian analysis of mixtures with an unknown number of components (with discussion). J. Roy. Stat. Soc. B. 59, 731-792","url":"https://www.researchgate.net/publication/227661917_Green_PJ_On_Bayesian_analysis_of_mixtures_with_an_unknown_number_of_components_with_discussion_J_Roy_Stat_Soc_B_59_731-792","abstraction":"New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context."},{"id":101,"title":"The Infinite-Order Conditional Random Field Model for Sequential Data Modeling","url":"https://www.researchgate.net/publication/235960199_The_Infinite-Order_Conditional_Random_Field_Model_for_Sequential_Data_Modeling","abstraction":"Sequential data labeling is a fundamental task in machine learning applications, with speech and natural language processing, activity recognition in video sequences, and biomedical data analysis being characteristic examples, to name just a few. The conditional random field (CRF), a log-linear model representing the conditional distribution of the observation labels, is one of the most successful approaches for sequential data labeling and classification, and has lately received significant attention in machine learning as it achieves superb prediction performance in a variety of scenarios. Nevertheless, existing CRF formulations can capture only one-or few-timestep interactions and neglect higher order dependences, which are potentially useful in many real-life sequential data modeling applications. To resolve these issues, in this paper we introduce a novel CRF formulation, based on the postulation of an energy function which entails infinitely long time-dependences between the modeled data. Building blocks of our novel approach are: 1) the sequence memoizer (SM), a recently proposed nonparametric Bayesian approach for modeling label sequences with infinitely long time dependences, and 2) a mean-field-like approximation of the model marginal likelihood, which allows for the derivation of computationally efficient inference algorithms for our model. The efficacy of the so-obtained infinite-order CRF (CRF 1) model is experimentally demonstrated."},{"id":102,"title":"A spatially-constrained normalized Gamma process prior","url":"https://www.researchgate.net/publication/237036856_A_spatially-constrained_normalized_Gamma_process_prior","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this work, we propose a novel nonparametric Bayesian method for clustering of data with spatial inter-dependencies. Specifically, we devise a novel normalized Gamma process, regulated by a simplified (pointwise) Markov random field (Gibbsian) distribution with a countably infinite number of states. As a result of its construction, the proposed model allows for introducing spatial dependencies in the clus-tering mechanics of the normalized Gamma process, thus yielding a novel nonparametric Bayesian method for spatial data clustering. We derive an efficient truncated variational Bayesian algorithm for model inference. We examine the efficacy of our approach by considering an image segmentation appli-cation using a real-world dataset. We show that our approach outperforms related methods from the field of Bayesian nonparametrics, including the infinite hidden Markov random field model, and the Dirichlet process prior.\n</div> \n<p></p>"},{"id":103,"title":"An Operational in situ Ichthyoplankton Imaging System (ISIIS)","url":"https://www.researchgate.net/publication/228900414_An_Operational_in_situ_Ichthyoplankton_Imaging_System_ISIIS","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n 1. ABSTRACT One driving factor improving the resolution of oceanographic sampling has been the observation of fine structure in the ocean. As oceanographers improve their sample resolution, the finer patterns that are discovered lead to a better understanding (and new questions) about dynamic processes in the ocean. To date, current technologies available for the study of many zooplankters remain limited in comparison to the spatial-temporal resolution and data acquisition rate available for physical oceanographic measurements, especially for the relatively rare meso-zooplankton. To overcome these challenges, we have built a towed, very high resolution digital imaging system capable of sampling water volumes sufficient for accurate quantification of meso-zooplankton in situ. The images are high quality, enabling clear identification of meso-zooplankters (e.g. larvaceans, gelatinous zooplankters, chaetognaths, larval fish), often to family or genus level. However, the efforts directed toward high speed and high-resolution imaging have the potential to create a bottleneck in data analysis. To address this problem we also have developed efficient algorithms to detect multiple regions (organisms) of interest (ROI) automatically, while filtering out noise and out-of-focus organisms, and simultaneously classify the detected organisms into pre-defined categories using shape and texture information. Here we demonstrate the current design, image quality, image analysis approach and example data analyses as an overview of the system capabilities.\n</div> \n<p></p>"},{"id":104,"title":"Markov random field-based clustering of vibration data","url":"https://www.researchgate.net/publication/224199347_Markov_random_field-based_clustering_of_vibration_data","abstraction":"A safe traversal of a mobile robot in an unknown environment requires the determination of local ground surface properties. As a first step, a broad structure of the underlying environment can be established by clustering terrain sections which exhibit similar features. In this work, we focus on an unsupervised learning approach to segment different terrain types according to the clustering of acquired vibration signals. Therefore, we present a Markov random field-based clustering approach taking the inherent temporal dependencies between consecutive measurements into account. The applied generative model assumes that the class labels of neighboring vibration segments are generated by prior distributions with similar parameters. A temporally constrained expectation maximization algorithm enables the efficient estimation of its parameters considering a predefined set of neighboring vibration segments. Since the size of the neighbor set proves to be data-dependent, we derive a general means of estimating this set size from the observed data. We show that the Markov random field clustering approach generates valid models for a variety of driving speeds even in situations of frequent terrain changes."},{"id":105,"title":"MDS-Based Multiresolution Nonlinear Dimensionality Reduction Model for Color Image Segmentation","url":"https://www.researchgate.net/publication/49776687_MDS-Based_Multiresolution_Nonlinear_Dimensionality_Reduction_Model_for_Color_Image_Segmentation","abstraction":"In this paper, we present an efficient coarse-to-fine multiresolution framework for multidimensional scaling and demonstrate its performance on a large-scale nonlinear dimensionality reduction and embedding problem in a texture feature extraction step for the unsupervised image segmentation problem. We demonstrate both the efficiency of our multiresolution algorithm and its real interest to learn a nonlinear low-dimensional representation of the texture feature set of an image which can then subsequently be exploited in a simple clustering-based segmentation algorithm. The resulting segmentation procedure has been successfully applied on the Berkeley image database, demonstrating its efficiency compared to the best existing state-of-the-art segmentation methods recently proposed in the literature."},{"id":106,"title":"IFIP Advances in Information and Communication Technology","url":"https://www.researchgate.net/publication/278694825_IFIP_Advances_in_Information_and_Communication_Technology","abstraction":"In this work, we propose a novel nonparametric Bayesian method for clustering of data with spatial interdependencies. Specifically, we devise a novel normalized Gamma process, regulated by a simplified (pointwise) Markov random field (Gibbsian) distribution with a countably infinite number of states. As a result of its construction, the proposed model allows for introducing spatial dependencies in the clustering mechanics of the normalized Gamma process, thus yielding a novel nonparametric Bayesian method for spatial data clustering. We derive an efficient truncated variational Bayesian algorithm for model inference. We examine the efficacy of our approach by considering an image segmentation application using a real-world dataset. We show that our approach outperforms related methods from the field of Bayesian nonparametrics, including the infinite hidden Markov random field model, and the Dirichlet process prior."},{"id":107,"title":"The infinite Student's t-mixture for robust modeling","url":"https://www.researchgate.net/publication/220227421_The_infinite_Student%27s_t-mixture_for_robust_modeling","abstraction":"Finite mixture models have been widely used for modeling probability distribution of real data sets due to its benefits from analytical tractability. Among the finite mixtures, the finite Student's t-mixture model (SMM) are tolerant to the untypical data (outliers). However, the SMM could not automatically determine the proper number of components, which is important and may has a significant effect on the learned model. In this paper, we propose an infinite Student's t-mixture model (iSMM) to handle this issue. This model is based on the Dirichlet process mixture which assumes the number of components in a mixture is infinite in advance, and determines the appropriate value of this number according to the observed data. Moreover, we derive an efficient variational Bayesian inference algorithm for the proposed model. Through applications in blind signal detection and image segmentation, it is shown that the iSMM possesses the advantages of both the Student's t-distribution and the Dirichlet process mixture, offering a more powerful and robust performance than competing models."},{"id":108,"title":"A Bayesian non-parametric Potts model with application to pre-surgical FMRI data","url":"https://www.researchgate.net/publication/225060683_A_Bayesian_non-parametric_Potts_model_with_application_to_pre-surgical_FMRI_data","abstraction":"The Potts model has enjoyed much success as a prior model for image segmentation. Given the individual classes in the model, the data are typically modeled as Gaussian random variates or as random variates from some other parametric distribution. In this article, we present a non-parametric Potts model and apply it to a functional magnetic resonance imaging study for the pre-surgical assessment of peritumoral brain activation. In our model, we assume that the Z-score image from a patient can be segmented into activated, deactivated, and null classes, or states. Conditional on the class, or state, the Z-scores are assumed to come from some generic distribution which we model non-parametrically using a mixture of Dirichlet process priors within the Bayesian framework. The posterior distribution of the model parameters is estimated with a Markov chain Monte Carlo algorithm, and Bayesian decision theory is used to make the final classifications. Our Potts prior model includes two parameters, the standard spatial regularization parameter and a parameter that can be interpreted as the a priori probability that each voxel belongs to the null, or background state, conditional on the lack of spatial regularization. We assume that both of these parameters are unknown, and jointly estimate them along with other model parameters. We show through simulation studies that our model performs on par, in terms of posterior expected loss, with parametric Potts models when the parametric model is correctly specified and outperforms parametric models when the parametric model in misspecified."},{"id":109,"title":"MDS-based segmentation model for the fusion of contour and texture cues in natural images","url":"https://www.researchgate.net/publication/257484782_MDS-based_segmentation_model_for_the_fusion_of_contour_and_texture_cues_in_natural_images","abstraction":"In this paper, we present an original image segmentation model based on a preliminary spatially adaptive non-linear data dimensionality reduction step integrating contour and texture cues. This new dimensionality reduction model aims at converting an input texture image into a noisy color image in order to greatly simplify its subsequent segmentation. In this latter de-texturing model, the (spatially adaptive) non-local constraints based on edge and contour cues allows us to efficiently regularize the reduced data (or the resulting de-textured color image) and to efficiently combine inhomogeneous region and edge based features in a data fusion/reduction model used as pre-processing step for a final segmentation task. In addition, a set of color/texture and edge-based adaptive spatial continuity constraints is imposed during the segmentation step. These improvements lead to an appealing and powerful two-step adaptive segmentation model, integrating contour and texture cues. Extensive experimental evaluation on the Berkeley image segmentation database demonstrates the efficiency of this hybrid segmentation model in terms of classification accuracy of pairwise pixels in the resulting segmentation map and in the precision–recall framework widespread used for evaluating contour detectors."},{"id":110,"title":"Brain MR Image Segmentation and Bias Field Correction through Class-K HMRF Model and EM Algorithm","url":"https://www.researchgate.net/publication/290594478_Brain_MR_Image_Segmentation_and_Bias_Field_Correction_through_Class-K_HMRF_Model_and_EM_Algorithm","abstraction":"Accurate brain tissue segmentation from magnetic resonance (MR) images plays an important role in both clinical practice and neuroscience research. In this paper, we extend the hidden Markov random field (HMRF) model, and propose a novel model, called Class-K HMRF model, to further improve the segmentation accuracy by incorporating more contextual information during classification. This model simultaneously takes account of spatial dependencies between image pixels and bias field, and hence can overcome the difficulties caused by noise and intensity inhomogeneity. By comparing our algorithm with state-of-the-art approaches, the experimental results demonstrate that the proposed algorithm can produce more accurate and reliable segmentations."},{"id":111,"title":"Automatically Detecting Pain in Video Through Facial Action Units","url":"https://www.researchgate.net/publication/49628576_Automatically_Detecting_Pain_in_Video_Through_Facial_Action_Units","abstraction":"In a clinical setting, pain is reported either through patient self-report or via an observer. Such measures are problematic as they are: 1) subjective, and 2) give no specific timing information. Coding pain as a series of facial action units (AUs) can avoid these issues as it can be used to gain an objective measure of pain on a frame-by-frame basis. Using video data from patients with shoulder injuries, in this paper, we describe an active appearance model (AAM)-based system that can automatically detect the frames in video in which a patient is in pain. This pain data set highlights the many challenges associated with spontaneous emotion detection, particularly that of expression and head movement due to the patient's reaction to pain. In this paper, we show that the AAM can deal with these movements and can achieve significant improvements in both the AU and pain detection performance compared to the current-state-of-the-art approaches which utilize similarity-normalized appearance features only."},{"id":112,"title":"The Consistency of Facial Expressions of Pain: A Comparison Across Modalities","url":"https://www.researchgate.net/publication/21663578_The_Consistency_of_Facial_Expressions_of_Pain_A_Comparison_Across_Modalities","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A number of facial actions have been found to be associated with pain. However, the consistency with which these actions occur during pain of different types has not been examined. This paper focuses on the consistency of facial expressions during pain induced by several modalities of nociceptive stimulation. Forty-one subjects were exposed to pain induced by electric shock, cold, pressure and ischemia. Facial actions during painful and pain-free periods were measured with the Facial Action Coding System. Four actions showed evidence of a consistent association with pain, increasing in likelihood, intensity or duration across all modalities: brow lowering, tightening and closing of the eye lids and nose wrinkling/upper lip raising. Factor analyses suggested that the facial actions reflected a general factor with a reasonably consistent pattern across modalities which could be combined into a sensitive single measure of pain expression. The findings suggest that the 4 actions identified carry the bulk of facial information about pain. They also provide evidence for the existence of a universal facial expression of pain. Implications of the findings for the measurement of pain expression are discussed.\n</div> \n<p></p>"},{"id":113,"title":"A validation model for verbal descriptor scaling of human clinical pain","url":"https://www.researchgate.net/publication/223065452_A_validation_model_for_verbal_descriptor_scaling_of_human_clinical_pain","abstraction":"Twenty-nine subjects used quantified verbal descriptors of sensory intensity (i.e., weak, mild, intense) or unpleasantness (i.e., annoying, unpleasant, distressing) to assess the intensity or unplesantness of sensations evoked by painful electrical stimulation of the tooth pulp over a broad stimulus range, and by a natural thermal tooth pulp stimulus, cold spray applied to exposed dentin. In addition, subjects matched the intensity or unpleasantness of the sensations evoked by the natural stimulus to that of the electrical stimuli by both the Method of Limits and the Method of Constant Stimuli.Quantified verbal descriptor values of either the sensory intensity or unpleasantness of the electrical stimuli were linearly related to stimulus intensity on a log scale, indicating that the relationships can be described by power functions. The quantified verbal description of the natural thermal stimulus and the intensity of the electrical stimulus directly matched to the thermal stimulus determined the coordinates of the clinical stimulus data point. This point was close to the experimental stimulus power function, indicating that the verbal magnitude of the clinical stimulus is predicted by the verbal magnitude of the specific electrical stimulus intensity that was matched to the clinical stimulus. This consistency supports the validity of the use of quantified verbal descriptors for the assessment of both experimentally controlled noxious stimulation and uncontrolled clinical pain sensations. It also supports the validity of direct matches between clinical and experimental pain sensations and the unpleasantness of these sensations. This procedure provides a useful independent validational paradigm for clinical pain assessment."},{"id":114,"title":"Comparing Active Shape Models with Active Appearance Models","url":"https://www.researchgate.net/publication/221259802_Comparing_Active_Shape_Models_with_Active_Appearance_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Statistical models of the shape and appearance of image structures can be matched to new images using both the Active Shape Model [7] algorithm and the Active Appearance Model algorithm [2]. The former searches along profiles about the current model point positions to update the current estimate of the shape of the object. The latter samples the image data under the current instance and uses the difference between model and sample to update the appearance model parameters. In this paper we compare and contrast the two algorithms, giving the results of experiments testing their performance on two data sets, one of faces, the other of structures in MR brain sections. We find that the ASM is faster and achieves more accurate feature point location than the AAM, but the AAM gives a better match to the texture.\n</div> \n<p></p>"},{"id":115,"title":"A Practical Guide to Support Vector Classication","url":"https://www.researchgate.net/publication/200085999_A_Practical_Guide_to_Support_Vector_Classication","abstraction":"Support vector machine (SVM) is a popular technique for classi?cation. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi?cant steps. In this guide, we propose a simple procedure, which usually gives reasonable results."},{"id":116,"title":"The Painful Face - Pain Expression Recognition Using Active Appearance Models","url":"https://www.researchgate.net/publication/230574153_The_Painful_Face_-_Pain_Expression_Recognition_Using_Active_Appearance_Models","abstraction":"Pain is typically assessed by patient self-report. Self-reported pain, however, is difficult to interpret and may be impaired or in some circumstances (i.e., young children and the severely ill) not even possible. To circumvent these problems behavioral scientists have identified reliable and valid facial indicators of pain. Hitherto, these methods have required manual measurement by highly skilled human observers. In this paper we explore an approach for automatically recognizing acute pain without the need for human observers. Specifically, our study was restricted to automatically detecting pain in adult patients with rotator cuff injuries. The system employed video input of the patients as they moved their affected and unaffected shoulder. Two types of ground truth were considered. Sequence-level ground truth consisted of Likert-type ratings by skilled observers. Frame-level ground truth was calculated from presence/absence and intensity of facial actions previously associated with pain. Active appearance models (AAM) were used to decouple shape and appearance in the digitized face images. Support vector machines (SVM) were compared for several representations from the AAM and of ground truth of varying granularity. We explored two questions pertinent to the construction, design and development of automatic pain detection systems. First, at what level (i.e., sequence- or frame-level) should datasets be labeled in order to obtain satisfactory automatic pain detection performance? Second, how important is it, at both levels of labeling, that we non-rigidly register the face?"},{"id":117,"title":"The CMU Pose, Illumination, and Expression Database","url":"https://www.researchgate.net/publication/3193633_The_CMU_Pose_Illumination_and_Expression_Database","abstraction":"In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database."},{"id":118,"title":"Facial Expression Analysis","url":"https://www.researchgate.net/publication/225940935_Facial_Expression_Analysis","abstraction":"Computer analysis of human face images includes detection of faces, identification of people, and understanding of expression. Among these three tasks, facial expression has been the least studied, and most of the past work on facial expression tried to recognize a small set of emotions, such as joy, disgust, and surprise. This practice may follow from the work of Darwin, who proposed that emotions have corresponding prototypic facial expressions. In everyday life, however, such prototypic expressions occur relatively infrequently; instead, emotion is communicated more often by subtle changes in one or a few discrete features. FACS-code Action Units, defined by Ekman, are one such representation accepted in the psychology community.  In collaboration with psychologists, we have been developing a system for automatically recognizing facial action units. This talk will present the current version of the system. The system uses a 3D Active Appearance Model to align a face image and transform it to a person-specific canonical coordinate frame. This transformation can remove appearance changes due to changes of head pose and relative illumination direction. In this transformed image frame, we perform detailed analysis of both facial motion and facial appearance changes, results of which are fed to an action-unit recogniser."},{"id":119,"title":"Automatically Detecting Pain Using Facial Actions.","url":"https://www.researchgate.net/publication/49796150_Automatically_Detecting_Pain_Using_Facial_Actions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Pain is generally measured by patient self-report, normally via verbal communication. However, if the patient is a child or has limited ability to communicate (i.e. the mute, mentally impaired, or patients having assisted breathing) self-report may not be a viable measurement. In addition, these self-report measures only relate to the maximum pain level experienced during a sequence so a frame-by-frame measure is currently not obtainable. Using image data from patients with rotator-cuff injuries, in this paper we describe an AAM-based automatic system which can detect pain on a frame-by-frame level. We do this two ways: directly (straight from the facial features); and indirectly (through the fusion of individual AU detectors). From our results, we show that the latter method achieves the optimal results as most discriminant features from each AU detector (i.e. shape or appearance) are used.\n</div> \n<p></p>"},{"id":120,"title":"Confidence Intervals for the Area under the ROC Curve","url":"https://www.researchgate.net/publication/215992302_Confidence_Intervals_for_the_Area_under_the_ROC_Curve","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In many applications, good ranking is a highly desirable performance for a classifier. The criterion commonly used to measure the ranking quality of a classification algorithm is the area under the ROC curve (AUC). To report it properly, it is crucial to determine an interval of confidence for its value. This paper provides confidence intervals for the AUC based on a statistical and combinatorial analysis using only simple parameters such as the error rate and the number of positive and negative examples. The analysis is distribution-independent, it makes no assumption about the distribution of the scores of negative or positive examples. The results are of practical use and can be viewed as the equivalent for AUC of the standard confidence intervals given in the case of the error rate. They are compared with previous approaches in several standard classification tasks demonstrating the benefits of our analysis.\n</div> \n<p></p>"},{"id":121,"title":"Automatic Behaviour Understanding in Medicine","url":"https://www.researchgate.net/publication/267155572_Automatic_Behaviour_Understanding_in_Medicine","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Now that Affective Computing and Social Signal Process-ing methods are becoming increasingly robust and accu-rate, novel areas of applications with significant societal im-pact are opening up for exploration. Perhaps one of the most promising areas is the application of automatic expres-sive behaviour understanding to help diagnose, monitor, and treat medical conditions that themselves alter a person's so-cial and affective signals. This work argues that this is now essentially a new area of research, called behaviomedics. It gives a definition of the area, discusses the most important groups of medical conditions that could benefit from this, and makes suggestions for future directions.\n</div> \n<p></p>"},{"id":122,"title":"Handling Data Imbalance in Automatic Facial Action Intensity Estimation","url":"https://www.researchgate.net/publication/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Automatic Action Unit (AU) intensity estimation is a key problem in facial expression analysis. But limited research attention has been paid to the inherent class imbalance, which usually leads to suboptimal performance. To handle the imbalance, we propose (1) a novel multiclass under-sampling method and (2) its use in an ensemble. We compare our approach with state of the art sampling methods used for AU intensity estimation. Multiple datasets and widely varying performance measures are used in the literature, making direct comparison difficult. To address these shortcomings, we compare different performance measures for AU intensity estimation and evaluate our proposed approach on three publicly available datasets, with a comparison to state of the art methods along with a cross dataset evaluation.\n</div> \n<p></p>"},{"id":123,"title":"Ensemble of Hankel Matrices for Face Emotion Recognition","url":"https://www.researchgate.net/publication/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition","abstraction":"In this paper, a face emotion is considered as the result of the composition of multiple concurrent signals, each corresponding to the movements of a specific facial muscle. These concurrent signals are represented by means of a set of multi-scale appearance features that might be correlated with one or more concurrent signals.  The extraction of these appearance features from a sequence of face images yields to a set of time series.  This paper proposes to use the dynamics regulating each appearance feature time series to recognize among different face emotions. To this purpose, an ensemble of Hankel matrices corresponding to the extracted time series is used for emotion classification within a framework that combines nearest neighbor and a majority vote schema. Experimental results on a public available dataset shows that the adopted representation is promising and yields state-of-the-art accuracy in emotion classification."},{"id":124,"title":"Facial Expression Analysis for Estimating Pain in Clinical Settings","url":"https://www.researchgate.net/publication/281276273_Facial_Expression_Analysis_for_Estimating_Pain_in_Clinical_Settings","abstraction":"Pain assessment is vital for effective pain management in clinical settings. It is generally obtained via patient's self-report or ob-server's assessment. Both of these approaches suffer from several drawbacks such as unavailability of self-report, idiosyncratic use and observer bias. This work aims at developing automated machine learning based approaches for estimating pain in clinical settings. We propose to use facial expression information to accomplish current goals since previous studies have demonstrated consistency between facial behavior and experienced pain. Moreover, with recent advances in computer vision it is possible to design algorithms for identifying spontaneous expressions such as pain in more naturalistic conditions. Our focus is towards designing robust computer vision models for estimating pain in videos containing patient's facial behavior. In this regard we discuss different research problem, technical approaches and challenges that needs to be addressed. In this work we particularly highlight the problem of predicting self-report measures of pain intensity since this problem is not only more challenging but also received less attention. We also discuss our efforts towards collecting an in-situ pediatric pain dataset for validating these approaches. We conclude the paper by presenting some results on both UNBC Mc-Master Pain dataset and pediatric pain dataset."},{"id":125,"title":"Context-Sensitive Conditional Ordinal Random Fields for Facial Action Intensity Estimation","url":"https://www.researchgate.net/publication/262331528_Context-Sensitive_Conditional_Ordinal_Random_Fields_for_Facial_Action_Intensity_Estimation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We address the problem of modeling intensity levels of facial actions in video sequences. The intensity sequences often exhibit a large variability due to the context factors, such as the person-specific facial expressiveness or changes in illumination. Existing methods usually attempt to normalize this variability in data using different feature-selection and/or data pre-processing schemes. Consequently, they ignore the context in which the target facial actions occur. We propose a novel Conditional Random Field (CRF) based ordinal model for context-sensitive modeling of the facial action unit intensity, where the W5+ (Who, When, What, Where, Why and How) definition of the context is used. In particular, we focus on three contextual questions: Who (the observed person), How (the changes in facial expressions), and When (the timing of the facial expression intensity). The contextual questions Who and How are modeled by means of the newly introduced covariate effects, while the contextual question When is modeled in terms of temporal correlation between the intensity levels. We also introduce a weighted softmax-margin learning of CRFs from the data with a skewed distribution of the intensity levels, as commonly encountered in spontaneous facial data. The proposed model is evaluated for intensity estimation of facial action units and facial expressions of pain from the UNBC Shoulder Pain dataset. Our experimental results show the effectiveness of the proposed approach.\n</div> \n<p></p>"},{"id":126,"title":"Transfer Learning with One-Class Data","url":"https://www.researchgate.net/publication/254257855_Transfer_Learning_with_One-Class_Data","abstraction":"When training and testing data are drawn from different distributions, most statis-tical models need to be retrained using the newly collected data. Transfer learning is a family of algorithms that improves the classifier learning in a target domain of interest by transferring the knowledge from one or multiple source domains, where the data falls in a different distribution. In this paper, we consider a new scenario of transfer learning for two-class classification, where only data samples from one of the two classes (e.g., the negative class) are available in the target domain. We introduce a regression-based one-class transfer learning algorithm to tackle this new problem. In contrast to the traditional discriminative feature selection, which seeks the best classification performance in the training data, we propose a new framework to learn the most transferable discriminative features suitable for our transfer learning. The experiment demonstrates improved per-formance in the applications of facial expression recognition and facial landmark detection."},{"id":127,"title":"Learning person-specific models for facial expression and action unit recognition","url":"https://www.researchgate.net/publication/254257602_Learning_person-specific_models_for_facial_expression_and_action_unit_recognition","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n a b s t r a c t A key assumption of traditional machine learning approach is that the test data are draw from the same distribution as the training data. However, this assumption does not hold in many real-world scenarios. For example, in facial expression recognition, the appearance of an expression may vary significantly for different people. As a result, previous work has shown that learning from adequate person-specific data can improve the expression recognition performance over the one from generic data. However, person-specific data is typically very sparse in real-world applications due to the difficulties of data collection and labeling, and learning from sparse data may suffer from serious over-fitting. In this paper, we propose to learn a person-specific model through transfer learning. By transferring the informative knowledge from other people, it allows us to learn an accurate model for a new subject with only a small amount of person-specific data. We conduct extensive experiments to compare different person-specific models for facial expression and action unit (AU) recognition, and show that transfer learning significantly improves the recognition performance with a small amount of training data. Ó 2013 Published by Elsevier B.V.\n</div> \n<p></p>"},{"id":128,"title":"Facing Imbalanced Data - Recommendations for the Use of Performance Metrics","url":"https://www.researchgate.net/publication/259972003_Facing_Imbalanced_Data_-_Recommendations_for_the_Use_of_Performance_Metrics","abstraction":"Recognizing facial action units (AUs) is important for situation analysis and automated video annotation. Previous work has emphasized face tracking and registration and the choice of features classifiers. Relatively neglected is the effect of imbalanced data for action unit detection. While the machine learning community has become aware of the problem of skewed data for training classifiers, little attention has been paid to how skew may bias performance metrics. To address this question, we conducted experiments using both simulated classifiers and three major databases that differ in size, type of FACS coding, and degree of skew. We evaluated influence of skew on both threshold metrics (Accuracy, F-score, Cohen's kappa, and Krippendorf's alpha) and rank metrics (area under the receiver operating characteristic (ROC) curve and precision-recall curve). With exception of area under the ROC curve, all were attenuated by skewed distributions, in many cases, dramatically so. While ROC was unaffected by skew, precision-recall curves suggest that ROC may mask poor performance. Our findings suggest that skew is a critical factor in evaluating performance metrics. To avoid or minimize skew-biased estimates of performance, we recommend reporting skew-normalized scores along with the obtained ones."},{"id":129,"title":"Optimization Theory of Large Systems","url":"https://www.researchgate.net/publication/3114779_Optimization_Theory_of_Large_Systems","abstraction":"Not Available"},{"id":130,"title":"The Variable Reduction Method for Nonlinear Programming","url":"https://www.researchgate.net/publication/227443563_The_Variable_Reduction_Method_for_Nonlinear_Programming","abstraction":"A first-order method for solving the problem: minimize f(x) subject to Ax - b \\geqq 0 is presented. The method contains ideas based on variable reduction with anti-zig-zagging and acceleration devices based on the Variable Metric Method. Proof of convergence to a Kuhn-Tucker Point, and statement of the rate of convergence when the strict second order sufficiency conditions hold are given."},{"id":131,"title":"The Approximate Minimization of Functionals","url":"https://www.researchgate.net/publication/3026746_The_Approximate_Minimization_of_Functionals","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Not Available\n</div> \n<p></p>"},{"id":132,"title":"Gradient methods for the minimisation of functionals","url":"https://www.researchgate.net/publication/243648552_Gradient_methods_for_the_minimisation_of_functionals","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Let tf(t) be a functional defined in the (real) Hubert space H. The problem consists in finding its minimum value and some minimum point x? (if such exists).\n</div> \n<p></p>"},{"id":133,"title":"Convergence of the steepest descent method with line searches and uniformly convex objective in reflexive Banach spaces","url":"https://www.researchgate.net/publication/281969874_Convergence_of_the_steepest_descent_method_with_line_searches_and_uniformly_convex_objective_in_reflexive_Banach_spaces","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we present some algorithms for unconstrained convex optimization problems. The development and analysis of these methods is carried out in a Banach space setting. We begin by introducing a general framework for achieving global convergence without Lipschitz conditions on the gradient, as usual in the current literature. This paper is an extension to Banach spaces to the analysis of the steepest descent method for convex optimization, most of them in less general spaces\n</div> \n<p></p>"},{"id":134,"title":"Graph Based Semi-supervised Non-negative Matrix Factorization for Document Clustering","url":"https://www.researchgate.net/publication/262315345_Graph_Based_Semi-supervised_Non-negative_Matrix_Factorization_for_Document_Clustering","abstraction":"Non-negative matrix factorization (NMF) approximates a non-negative matrix by the product of two low-rank matrices and achieves good performance in clustering. Recently, semi-supervised NMF (SS-NMF) further improves the performance by incorporating part of the labels of few samples into NMF. In this paper, we proposed a novel graph based SS-NMF (GSS-NMF). For each sample, GSS-NMF minimizes its distances to the same labeled samples and maximizes the distances against different labeled samples to incorporate the discriminative information. Since both labeled and unlabeled samples are embedded in the same reduced dimensional space, the discriminative information from the labeled samples is successfully transferred to the unlabeled samples, and thus it greatly improves the clustering performance. Since the traditional multiplicative update rule converges slowly, we applied the well-known projected gradient method to optimizing GSSNMF and the proposed algorithm can be applied to optimizing other manifold regularized NMF efficiently. Experimental results on two popular document datasets, i.e., Reuters21578 and TDT-2, show that GSS-NMF outperforms the representative SS-NMF algorithms."},{"id":135,"title":"C-logit stochastic user equilibrium model: formulations and solution algorithm","url":"https://www.researchgate.net/publication/232986704_C-logit_stochastic_user_equilibrium_model_formulations_and_solution_algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article considers the stochastic user equilibrium (SUE) problem with the route choice model based on the C-logit function. The C-logit model has a simple closed-form analytical probability expression and requires relatively lower calibration efforts and represents a more realistic route choice behaviour compared with the multinomial logit model. This article proposes two versions of the C-logit SUE model that captures the route similarity using different attributes in the commonality factors. The two versions differ with respect to the independence assumption between cost and flow. The corresponding stochastic traffic equilibrium models are called the length-based and congestion-based C-logit SUE models, respectively. To formulate the length-based C-logit SUE model, an equivalent mathematical programming formulation is proposed. For the congestion-based C-logit SUE model, we provide two equivalent variational inequality formulations. To solve the proposed formulations, a new self-adaptive gradient projection algorithm is developed. The proposed formulations and new solution algorithm are tested in two well-known networks. Numerical results demonstrate the validity of the formulations and solution algorithm.\n</div> \n<p></p>"},{"id":136,"title":"Convex optimization for exact rank recovery in topic models","url":"https://www.researchgate.net/publication/224265213_Convex_optimization_for_exact_rank_recovery_in_topic_models","abstraction":"Topic models are widely used in a variety of applications including document classification and computer vision. The number of topics in the model plays an important role in terms of accuracy. We consider the problem of estimating the number of topics. In [1], a convex optimization approach was proposed to solve the problem via a constrained nuclear norm minimization. A standard semidefinite programming (SDP) was applied to solve the convex optimization only for a small size problem (e.g. 100× 100 matrix) due to its high computational complexity. To extend the applicability of the approach to large scale problems, we propose an accelerated gradient algorithm (AGA). Numerical results show that proposed algorithm can reliably solve a wide range of large scale problems in a shorter time than SDP solvers. Moreover, algorithms applied to a fairly large size real world dataset and results are provided."},{"id":137,"title":"B?Dynamic: An Efficient Algorithm for Dynamic User Equilibrium Assignment in Activity?Travel Networks1","url":"https://www.researchgate.net/publication/220373522_B-Dynamic_An_Efficient_Algorithm_for_Dynamic_User_Equilibrium_Assignment_in_Activity-Travel_Networks1","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Multi-dimensional choice in dynamic traffic assignment (DTA)—for example, a combined model of activity location, time of participation, duration, and route choice decisions—results in exponentially increasing choice alternatives. Any efficient algorithm for solving the multi-dimensional DTA problem must avoid enumeration of alternatives. In this article an algorithm that does not enumerate paths is presented. The algorithm is a novel extension of Algorithm B ( Dial, 2006) to dynamic networks and hence referred to as Algorithm B-Dynamic. The DTA model proposed here uses a point queue model for traffic propagation that reduces computational complexity. The activity participation decision dimensions are incorporated through utility functions, which are a linear function of duration and schedule delay (early or late arrival penalty). Numerical examples are then presented to illustrate both the steps of the algorithm and its capabilities. Overall, the algorithm performed well for up to medium-sized networks. Further, the algorithm scales fairly well with increasing demand levels.\n</div> \n<p></p>"},{"id":138,"title":"Convex constrained optimization for large-scale generalized Sylvester equations","url":"https://www.researchgate.net/publication/225749316_Convex_constrained_optimization_for_large-scale_generalized_Sylvester_equations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose and study the use of convex constrained optimization techniques for solving large-scale Generalized Sylvester Equations\n <br> (GSE). For that, we adapt recently developed globalized variants of the projected gradient method to a convex constrained\n <br> least-squares approach for solving GSE. We demonstrate the effectiveness of our approach on two different applications. First,\n <br> we apply it to solve the GSE that appears after applying left and right preconditioning schemes to the linear problems associated\n <br> with the discretization of some partial differential equations. Second, we apply the new approach, combined with a Tikhonov\n <br> regularization term, to restore some blurred and highly noisy images.\n <br> \n <br> KeywordsConvex optimization–Spectral projected gradient method–Generalized Sylvester equation–Image restoration\n</div> \n<p></p>"},{"id":139,"title":"Stability of user-equilibrium route flow solutions for the traffic assignment problem","url":"https://www.researchgate.net/publication/222701933_Stability_of_user-equilibrium_route_flow_solutions_for_the_traffic_assignment_problem","abstraction":"This paper studies stability of user-equilibrium (UE) route flow solutions with respect to inputs to a traffic assignment problem, namely the travel demand and parameters in the link cost function. It shows, under certain continuity and strict monotonicity assumptions on the link cost function, that the UE link flow is a continuous function of the inputs, that the set of UE route flows is a continuous multifunction of the inputs, and that the UE route flow selected to maximize an objective function with certain properties is a continuous function of the inputs. The maximum entropy UE route flow is an example of the last. On the other hand, a UE route flow arbitrarily generated in a standard traffic assignment procedure may not bear such continuity property, as demonstrated by an example in this paper."},{"id":140,"title":"Partial Spectral Projected Gradient Method with Active-Set Strategy for Linearly Constrained Optimization","url":"https://www.researchgate.net/publication/225778113_Partial_Spectral_Projected_Gradient_Method_with_Active-Set_Strategy_for_Linearly_Constrained_Optimization","abstraction":"A method for linearly constrained optimization which modifies and generalizes recent box-constraint optimization algorithms is introduced. The new algorithm is based on a relaxed form of Spectral Projected Gradient iterations. Intercalated with these projected steps, internal iterations restricted to faces of the polytope are performed, which enhance the efficiency of the algorithm. Convergence proofs are given and numerical experiments are included and commented. Software supporting this paper is available through the Tango Project web page: http://www.ime.usp.br/?egbirgin/tango/."},{"id":141,"title":"Constrained Newton Methods for Transport Network Equilibrium Analysis","url":"https://www.researchgate.net/publication/245476471_Constrained_Newton_Methods_for_Transport_Network_Equilibrium_Analysis","abstraction":"A set of constrained Newton methods were developed for static traffic assignment problems. The Newton formula uses the gradient of the objective function to determine an improved feasible direction scaled by the second-order derivatives of the objective function. The column generation produces the active paths necessary for each origin-destination pair. These methods then select an optimal step size or make an orthogonal projection to achieve fast, accurate convergence. These Newton methods based on the constrained Newton formula utilize path information to explicitly implement Wardrop's principle in the transport network modelling and complement the traffic assignment algorithms. Numerical examples are presented to compare the performance with all possible Newton methods. The computational results show that the optimal-step Newton methods have much better convergence than the fixed-step ones, while the Newton method with the unit step size is not always efficient for traffic assignment problems. Furthermore, the optimal-step Newton methods are relatively robust for all three of the tested benchmark networks of traffic assignment problems."},{"id":142,"title":"Considerations on Parallelizing Nonnegative Matrix Factorization for Hyperspectral Data Unmixing","url":"https://www.researchgate.net/publication/224349031_Considerations_on_Parallelizing_Nonnegative_Matrix_Factorization_for_Hyperspectral_Data_Unmixing","abstraction":"Nonnegative matrix factorization (NMF) is a recently developed linear unmixing technique that assumes that the original sources and transform were positively defined. Given that the linear mixing model (LMM) for hyperspectral data requires positive endmembers and abundances, with only minor modifications, NMF can be used to solve LMM. Traditionally, NMF solutions include an iterative process resulting in considerable execution times. In this letter, we provide two novel algorithms aimed at speeding the NMF through parallel processing: the first based on the traditional multiplicative solution and the second modifying an adaptive projected gradient technique known to provide better convergence. The algorithms' implementations were tested on various data sets; the results suggest that a significant speedup can be achieved without decrease in accuracy. This supports the further use of NMF for linear unmixing."},{"id":143,"title":"INTERSPEECH 2007 The Relevance of Feature Type for the Automatic Classification of Emotional User States: Low Level Descriptors and Functionals","url":"https://www.researchgate.net/publication/221490308_INTERSPEECH_2007_The_Relevance_of_Feature_Type_for_the_Automatic_Classification_of_Emotional_User_States_Low_Level_Descriptors_and_Functionals","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we report on classification results for emotional user states (4 classes, German database of children interacting with a pet robot). Six sites computed acoustic and linguistic features independently from each other, following in part dif- ferent strategies. A total of 4244 features were pooled together and grouped into 12 low level descriptor types and 6 functional types. For each of these groups, classification results using Sup- port Vector Machines and Random Forests are reported for the full set of features, and for 150 features each with the highest individual Information Gain Ratio. The performance for the different groups varies mostly between ? 50% and ? 60%. Index Terms: emotional user states, automatic classification, feature types, functionals\n</div> \n<p></p>"},{"id":144,"title":"A database of German emotional speech","url":"https://www.researchgate.net/publication/221491017_A_database_of_German_emotional_speech","abstraction":"The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/)."},{"id":145,"title":"Being bored? Recognising natural interest by extensive audiovisual integration for real-life application","url":"https://www.researchgate.net/publication/222421400_Being_bored_Recognising_natural_interest_by_extensive_audiovisual_integration_for_real-life_application","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Automatic detection of the level of human interest is of high relevance for many technical applications, such as automatic customer care or tutoring systems. However, the recognition of spontaneous interest in natural conversations independently of the subject remains a challenge. Identification of human affective states relying on single modalities only is often impossible, even for humans, since different modalities contain partially disjunctive cues. Multimodal approaches to human affect recognition generally are shown to boost recognition performance, yet are evaluated in restrictive laboratory settings only. Herein we introduce a fully automatic processing combination of Active–Appearance–Model-based facial expression, vision-based eye-activity estimation, acoustic features, linguistic analysis, non-linguistic vocalisations, and temporal context information in an early feature fusion process. We provide detailed subject-independent results for classification and regression of the Level of Interest using Support-Vector Machines on an audiovisual interest corpus (AVIC) consisting of spontaneous, conversational speech demonstrating “theoretical” effectiveness of the approach. Further, to evaluate the approach with regards to real-life usability a user-study is conducted for proof of “practical” effectiveness.\n</div> \n<p></p>"},{"id":146,"title":"Support Vector Regression for Automatic Recognition of Spontaneous Emotions in Speech","url":"https://www.researchgate.net/publication/224711637_Support_Vector_Regression_for_Automatic_Recognition_of_Spontaneous_Emotions_in_Speech","abstraction":"We present novel methods for estimating spontaneously expressed emotions in speech. Three continuous-valued emotion primitives are used to describe emotions, namely valence, activation, and dominance. For the estimation of these primitives, support vector machines (SVMs) are used in their application for regression (support vector regression, SVR). Feature selection and parameter optimization are studied. The data was recorded from 47 speakers in a German talk-show on TV. The results were compared to a rule-based fuzzy logic classifier and a fuzzy k-nearest neighbor classifier. SVR was found to give the best results and to be suited well for emotion estimation yielding small classification errors and high correlation between estimates and reference."},{"id":147,"title":"Discriminative parameter learning for Bayesian networks","url":"https://www.researchgate.net/publication/221345676_Discriminative_parameter_learning_for_Bayesian_networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Bayesian network classiflers have been widely used for classiflcation problems. Given a flxed Bayesian network structure, pa- rameters learning can take two difierent approaches: generative and discriminative learning. While generative parameter learn- ing is more e-cient, discriminative param- eter learning is more efiective. In this pa- per, we propose a simple, e-cient, and efiective discriminative parameter learning method, called Discriminative Frequency Es- timate (DFE), which learns parameters by discriminatively computing frequencies from data. Empirical studies show that the DFE algorithm integrates the advantages of both generative and discriminative learning: it performs as well as the state-of-the-art dis- criminative parameter learning method ELR in accuracy, but is signiflcantly more e-cient.\n</div> \n<p></p>"},{"id":148,"title":"Static and Dynamic Modelling for the Recognition of Non-verbal Vocalisations in Conversational Speech","url":"https://www.researchgate.net/publication/220726872_Static_and_Dynamic_Modelling_for_the_Recognition_of_Non-verbal_Vocalisations_in_Conversational_Speech","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Non-verbal vocalisations such as laughter, breathing, hesitation, and consent play an important role in the recognition and\n <br> understanding of human conversational speech and spontaneous affect. In this contribution we discuss two different strategies\n <br> for robust discrimination of such events: dynamic modelling by a broad selection of diverse acoustic Low-Level-Descriptors\n <br> vs. static modelling by projection of these via statistical functionals onto a 0.6k feature space with subsequent de-correlation.\n <br> As classifiers we employ Hidden Markov Models, Conditional Random Fields, and Support Vector Machines, respectively. For discussion\n <br> of extensive parameter optimisation test-runs with respect to features and model topology, 2.9k non-verbals are extracted\n <br> from the spontaneous Audio-Visual Interest Corpus. 80.7% accuracy can be reported with, and 92.6% without a garbage model\n <br> for the discrimination of the named classes.\n</div> \n<p></p>"},{"id":149,"title":"EmoVoice - A framework for online recognition of emotions from voice","url":"https://www.researchgate.net/publication/200777596_EmoVoice_-_A_framework_for_online_recognition_of_emotions_from_voice","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present EmoVoice, a framework for emotional speech corpus and classier creation and for oine as well as real-time online speech emotion recognition. The framework is intended to be used by non-experts and therefore comes with an interface to create an own personal or application specic emotion recogniser. Furthermore, we describe some applications and prototypes that already use our framework to track online emotional user states from voice information.\n</div> \n<p></p>"},{"id":150,"title":"A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions","url":"https://www.researchgate.net/publication/23493444_A_Survey_of_Affect_Recognition_Methods_Audio_Visual_and_Spontaneous_Expressions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions despite the fact that deliberate behaviour differs in visual appearance, audio profile, and timing from spontaneously occurring behaviour. To address this problem, efforts to develop algorithms that can process naturally occurring human affective behaviour have recently emerged. Moreover, an increasing number of efforts are reported toward multimodal fusion for human affect analysis including audiovisual fusion, linguistic and paralinguistic fusion, and multi-cue visual fusion based on facial expressions, head movements, and body gestures. This paper introduces and surveys these recent advances. We first discuss human emotion perception from a psychological perspective. Next we examine available approaches to solving the problem of machine understanding of human affective behavior, and discuss important issues like the collection and availability of training and test data. We finally outline some of the scientific and engineering challenges to advancing human affect sensing technology.\n</div> \n<p></p>"},{"id":151,"title":"LIBSVM: A library for support vector machines","url":"https://www.researchgate.net/publication/228715647_LIBSVM_A_library_for_support_vector_machines","abstraction":"LIBSVM is a library for support vector machines (SVM). Its goal is to help users to easily use SVM as a tool. In this document, we present all its imple-mentation details. For the use of LIBSVM, the README file included in the package and the LIBSVM FAQ provide the information."},{"id":152,"title":"Speaker-Independent Speech Emotion Recognition using Gaussian and SVM Classifiers,","url":"https://www.researchgate.net/publication/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers","abstraction":"This paper presents a novel technique of human emotion recognition using the audio modality for the speaker-independent task. In order to achieve a high emotion classification performance a standard set of audio features were extracted. The feature selection was performed using the Plus l-Take Away r algorithm based on Bhattacharyya distance criterion. The feature selection was followed by feature reduction using PCA and LDA, and classification using the Gaussian and SVM classifiers. The emotion classification performance better or comparable to state-of-the art techniques and humans were achieved on the standard Berlin emotional speech database. Keywords: Audio Emotion Recognition, Feature Selection, Principal Component Analysis, Gaussian and SVM Classifiers"},{"id":153,"title":"A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems","url":"https://www.researchgate.net/publication/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems","abstraction":"Transcription of broadcast news is an interesting and challenging application for large-vocabulary continuous speech recognition (LVCSR). We present in detail the structure of a manually segmented and annotated corpus including over 160 hours of German broadcast news, and propose it as an evaluation framework of LVCSR systems. We show our own experimental results on the corpus, achieved with a state-of-the-art LVCSR decoder, measuring the effect of different feature sets and decoding parameters, and thereby demonstrate that real-time decoding of our test set is feasible on a desktop PC at 9.2% word error rate."},{"id":154,"title":"Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol","url":"https://www.researchgate.net/publication/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Second Emotion Recognition In The Wild Challenge (EmotiW) 2014 consists of an audio-video based emotion classification challenge, which mimics the real-world conditions. Traditionally, emotion recognition has been performed on data captured in constrained lab-controlled like environment. While this data was a good starting point, such lab controlled data poorly represents the environment and conditions faced in real-world situations. With the exponential increase in the number of video clips being up-loaded online, it is worthwhile to explore the performance of emotion recognition methods that work 'in the wild'. The goal of this Grand Challenge is to carry forward the common platform defined during EmotiW 2013, for evaluation of emotion recognition methods in real-world conditions. The database in the 2014 challenge is the Acted Facial Expression In Wild (AFEW) 4.0, which has been collected from movies showing close-to-real-world conditions. The paper describes the data partitions, the baseline method and the experimental protocol.\n</div> \n<p></p>"},{"id":155,"title":"Speech Emotion Recognition Using Deep Neural Network and Extreme Learning Machine","url":"https://www.researchgate.net/publication/267213794_Speech_Emotion_Recognition_Using_Deep_Neural_Network_and_Extreme_Learning_Machine","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Speech emotion recognition is a challenging problem partly because it is unclear what features are effective for the task. In this paper we propose to utilize deep neural networks (DNNs) to extract high level features from raw data and show that they are effective for speech emotion recognition. We first produce an emotion state probability distribution for each speech segment using DNNs. We then construct utterance-level features from segment-level probability distributions. These utterance level features are then fed into an extreme learning machine (ELM), a special simple and efficient single-hidden-layer neural network, to identify utterance-level emotions. The experimental results demonstrate that the proposed approach effectively learns emotional information from low-level features and leads to 20% relative accuracy improvement compared to the stateof-the-art approaches.\n</div> \n<p></p>"},{"id":156,"title":"COVAREP - A collaborative voice analysis repository for speech technologies","url":"https://www.researchgate.net/publication/262697799_COVAREP_-_A_collaborative_voice_analysis_repository_for_speech_technologies","abstraction":"Speech processing algorithms are often developed demonstrating improvements over the state-of-the-art, but sometimes at the cost of high complexity. This makes algorithm reimplementations based on literature difficult, and thus reliable comparisons between published results and current work are hard to achieve. This paper presents a new collaborative and freely available repository for speech process-ing algorithms called COVAREP, which aims at fast and easy access to new speech processing algorithms and thus facilitating research in the field. We envisage that COVAREP allows more reproducible research by strengthening complex implementations through shared contributions and openly available code which can be discussed, commented on and corrected by the community. Presently CO-VAREP contains contributions from five distinct laboratories and we encourage contributions from across the speech processing research field. In this paper, we provide an overview of the current offerings of COVAREP and also include a demonstration of the algorithms through an emotion classification experiment."},{"id":157,"title":"A two-layer model for music pleasure regression","url":"https://www.researchgate.net/publication/261235391_A_two-layer_model_for_music_pleasure_regression","abstraction":"We adopt a two-layer regression model for music pleasure regression. Pleasure orientation of a song is estimated first, and then different regressors are used to predict degree of pleasure according to the estimated orientation. By using corresponding regressors for each instance, there is a big improvement when we assume the first layer is perfect in comparison with one-layer model. By tuning the confidence threshold of the orientation classifier of the two-layer model, we get improvement over one-layer model."},{"id":158,"title":"Speaker state recognition using an HMM-based feature extraction method","url":"https://www.researchgate.net/publication/257267505_Speaker_state_recognition_using_an_HMM-based_feature_extraction_method","abstraction":"In this article we present an efficient approach to modeling the acoustic features for the tasks of recognizing various paralinguistic phenomena. Instead of the standard scheme of adapting the Universal Background Model (UBM), represented by the Gaussian Mixture Model (GMM), normally used to model the frame-level acoustic features, we propose to represent the UBM by building a monophone-based Hidden Markov Model (HMM). We present two approaches: transforming the monophone-based segmented HMM–UBM to a GMM–UBM and proceeding with the standard adaptation scheme, or to perform the adaptation directly on the HMM–UBM. Both approaches give superior results than the standard adaptation scheme (GMM–UBM) in both the emotion recognition task and the alcohol detection task. Furthermore, with the proposed method we were able to achieve better results than the current state-of-the-art systems in both tasks."},{"id":159,"title":"Automatic Speaker Age and Gender Recognition Using Acoustic and Prosodic Level Information Fusion","url":"https://www.researchgate.net/publication/229439065_Automatic_Speaker_Age_and_Gender_Recognition_Using_Acoustic_and_Prosodic_Level_Information_Fusion","abstraction":"The paper presents a novel automatic speaker age and gender identification approach which combines seven different methods at both acoustic and prosodic levels to improve the baseline performance. The three baseline subsystems are (1) Gaussian mixture model (GMM) based on mel-frequency cepstral coefficient (MFCC) features, (2) Support vector machine (SVM) based on GMM mean supervectors and (3) SVM based on 450-dimensional utterance level features including acoustic, prosodic and voice quality information. In addition, we propose four subsystems: (1) SVM based on UBM weight posterior probability supervectors using the Bhattacharyya probability product kernel, (2) Sparse representation based on UBM weight posterior probability supervectors, (3) SVM based on GMM maximum likelihood linear regression (MLLR) matrix supervectors and (4) SVM based on the polynomial expansion coefficients of the syllable level prosodic feature contours in voiced speech segments. Contours of pitch, time domain energy, frequency domain harmonic structure energy and formant for each syllable (segmented using energy information in the voiced speech segment) are considered for analysis in subsystem (4). The proposed four subsystems have been demonstrated to be effective and able to achieve competitive results in classifying different age and gender groups. To further improve the overall classification performance, weighted summation based fusion of these seven subsystems at the score level is demonstrated. Experiment results are reported on the development and test set of the 2010 Interspeech Paralinguistic Challenge aGender database. Compared to the SVM baseline system (3), which is the baseline system suggested by the challenge committee, the proposed fusion system achieves 5.6% absolute improvement in unweighted accuracy for the age task and 4.2% for the gender task on the development set. On the final test set, we obtain 3.1% and 3.8% absolute improvement, respectively."},{"id":160,"title":"Automated Acoustic Classification of Bird Species from Real -Field Recordings","url":"https://www.researchgate.net/publication/260591658_Automated_Acoustic_Classification_of_Bird_Species_from_Real_-Field_Recordings","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We report on a recent progress with the development of an automated bioacoustic bird recognizer, which is part of a long-term project , aiming at the establishment of an automated biodiversity monitoring system at the Hymettus Mountain near Athens. In particular, employing a classical audio processing strategy, which has been proved quite successful in various audio recognition applications, we evaluate the appropriateness of six classifiers on the bird species recognition task. In the experimental evaluation of the acoustic bird recognizer, we made use of real-field audio recordings of two bird species, which are known to be present at the Hymettus Mountain. Encouraging recognition accuracy was obtained on the real-field data, and further experiments with additive noise demonstrated significant noise robustness in low SNR conditions.\n</div> \n<p></p>"},{"id":161,"title":"AVEC 2012 - The continuous audio/visual emotion challenge - Aon","url":"https://www.researchgate.net/publication/262203674_AVEC_2012_-_The_continuous_audiovisual_emotion_challenge_-_Aon","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The second international Audio/Visual Emotion Challenge and Workshop 2012 (AVEC 2012) is introduced shortly. 34 teams from 12 countries signed up for the Challenge. The SEMAINE database serves for prediction of four-dimensional continuous affect in audio and video. For the eligible participants, final scores for the Fully-Continuous Sub-Challenge ranged between a correlation coefficient between gold standard and prediction of 0.174 and 0.456, and for Word-Level Sub-Challenge between 0.113 and 0.280.\n</div> \n<p></p>"},{"id":162,"title":"Markov Chain Sampling Methods for Dirichlet Process Mixture Models","url":"https://www.researchgate.net/publication/229100346_Markov_Chain_Sampling_Methods_for_Dirichlet_Process_Mixture_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article reviews Markov chain methods for sampling from the posterior\n <br> distribution of a Dirichlet process mixture model and presents two\n <br> new classes of methods. One new approach is to make Metropolis-Hastings\n <br> updates of the indicators specifying which mixture component is associated\n <br> with each observation, perhaps supplemented with a partial form of\n <br> Gibbs sampling. The other new approach extends Gibbs sampling for\n <br> these indicators by using a set of auxiliary parameters. These methods\n <br> are simple to implement and are more efficient than previous ways\n <br> of handling general Dirichlet process mixture models with non-conjugate\n <br> priors.\n</div> \n<p></p>"},{"id":163,"title":"Bayesian Agglomerative Clustering with Coalescents","url":"https://www.researchgate.net/publication/45860527_Bayesian_Agglomerative_Clustering_with_Coalescents","abstraction":"We introduce a new Bayesian model for hierarchical clustering based on a prior over trees called Kingman's coalescent. We develop novel greedy and sequential Monte Carlo inferences which operate in a bottom-up agglomerative fashion. We show experimentally the superiority of our algorithms over others, and demonstrate our approach in document clustering and phylolinguistics. Comment: NIPS 2008"},{"id":164,"title":"Gibbs Sampling Methods for Stick - breaking Priors","url":"https://www.researchgate.net/publication/27290603_Gibbs_Sampling_Methods_for_Stick_-_breaking_Priors","abstraction":"This article presents two Gibbs sampling methods for é t- ting Bayesian nonparametric and semiparametric hierarchical models that are based on a general class of priors that we call stick-breaking priors. The two types of Gibbs samplers are quite different in nature. The é rst method is applicable when the prior can be characterized by a generalized Pólya urn mechanism and it involves drawing samples from the poste- rior of a hierarchical model formed by marginalizing over the prior. Our Pólya urn Gibbs sampler is a direct extension of the widely used Pólya urn sampler developed by Escobar (1988, 1994), MacEachern (1994), and Escobar and West (1995) for"},{"id":165,"title":"Bayesian Density Estimation and Inference Using Mixtures","url":"https://www.researchgate.net/publication/2741222_Bayesian_Density_Estimation_and_Inference_Using_Mixtures","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ...\n</div> \n<p></p>"},{"id":166,"title":"Sampling the Dirichlet Mixture Model with Slices","url":"https://www.researchgate.net/publication/46451640_Sampling_the_Dirichlet_Mixture_Model_with_Slices","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Generalized linear mixed models provide a flexible framework for modeling a range of data, although with non-Gaussian response variables the likelihood cannot be obtained in closed form. Markov chain Monte Carlo methods solve this problem by sampling from a series of simpler conditional distributions that can be evaluated. The R package MCMCglmm implements such an algorithm for a range of model fitting problems. More than one response variable can be analyzed simultaneously, and these variables are allowed to follow Gaussian, Poisson, multi(bi)nominal, exponential, zero-inflated and censored distributions. A range of variance structures are permitted for the random effects, including interactions with categorical or continuous variables (i.e., random regression), and more complicated variance structures that arise through shared ancestry, either through a pedigree or through a phylogeny. Missing values are permitted in the response variable(s) and data can be known up to some level of measurement error as in meta-analysis. All simu- lation is done in C/ C++ using the CSparse library for sparse linear systems.\n</div> \n<p></p>"},{"id":167,"title":"Infinite Structured Hidden Semi-Markov Models","url":"https://www.researchgate.net/publication/263582646_Infinite_Structured_Hidden_Semi-Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper reviews recent advances in Bayesian nonparametric techniques for\n <br> constructing and performing inference in infinite hidden Markov models. We\n <br> focus on variants of Bayesian nonparametric hidden Markov models that enhance a\n <br> posteriori state-persistence in particular. This paper also introduces a new\n <br> Bayesian nonparametric framework for generating left-to-right and other\n <br> structured, explicit-duration infinite hidden Markov models that we call the\n <br> infinite structured hidden semi-Markov model.\n</div> \n<p></p>"},{"id":168,"title":"Pattern discovery in continuous speech using Block Diagonal Infinite HMM","url":"https://www.researchgate.net/publication/269164447_Pattern_discovery_in_continuous_speech_using_Block_Diagonal_Infinite_HMM","abstraction":"We propose the application of a recently introduced inference method, the Block Diagonal Infinite Hidden Markov Model (BDiHMM), to the problem of learning the topology of a Hidden Markov Model (HMM) from continuous speech in an unsupervised way. We test the method on the TiDigits continuous digit database and analyse the emerging patterns corresponding to the blocks of states inferred by the model. We show how the complexity of these patterns increases with the amount of observations and number of speakers. We also show that the patterns correspond to sub-word units that constitute stable and discriminative representations of the words contained in the speech material."},{"id":169,"title":"Modeling the Complex Dynamics and Changing Correlations of Epileptic Events","url":"https://www.researchgate.net/publication/260428846_Modeling_the_Complex_Dynamics_and_Changing_Correlations_of_Epileptic_Events","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Patients with epilepsy can manifest short, sub-clinical epileptic \"bursts\" in\n <br> addition to full-blown clinical seizures. We believe the relationship between\n <br> these two classes of events---something not previously studied\n <br> quantitatively---could yield important insights into the nature and intrinsic\n <br> dynamics of seizures. A goal of our work is to parse these complex epileptic\n <br> events into distinct dynamic regimes. A challenge posed by the intracranial EEG\n <br> (iEEG) data we study is the fact that the number and placement of electrodes\n <br> can vary between patients. We develop a Bayesian nonparametric Markov switching\n <br> process that allows for (i) shared dynamic regimes between a variable numbers\n <br> of channels, (ii) asynchronous regime-switching, and (iii) an unknown\n <br> dictionary of dynamic regimes. We encode a sparse and changing set of\n <br> dependencies between the channels using a Markov-switching Gaussian graphical\n <br> model for the innovations process driving the channel dynamics and demonstrate\n <br> the importance of this model in parsing and out-of-sample predictions of iEEG\n <br> data. We show that our model produces intuitive state assignments that can help\n <br> automate clinical analysis of seizures and enable the comparison of\n <br> sub-clinical bursts and full clinical seizures.\n</div> \n<p></p>"},{"id":170,"title":"TRASMIL: A local anomaly detection framework based on trajectory segmentation and multi-instance learning","url":"https://www.researchgate.net/publication/259142582_TRASMIL_A_local_anomaly_detection_framework_based_on_trajectory_segmentation_and_multi-instance_learning","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Local anomaly detection refers to detecting small anomalies or outliers that exist in some subsegments of events or behaviors. Such local anomalies are easily overlooked by most of the existing approaches since they are designed for detecting global or large anomalies. In this paper, an accurate and flexible three-phase framework TRASMIL is proposed for local anomaly detection based on TRAjectory Segmentation and Multi-Instance Learning. Firstly, every motion trajectory is segmented into independent sub-trajectories, and a metric with Diversity and Granularity is proposed to measure the quality of segmentation. Secondly, the segmented sub-trajectories are modeled by a sequence learning model. Finally, multi-instance learning is applied to detect abnormal trajectories and sub-trajectories which are viewed as bags and instances, respectively. We validate the TRASMIL framework in terms of 16 different algorithms built on the three-phase framework. Substantial experiments show that algorithms based on the TRASMIL framework outperform existing methods in effectively detecting the trajectories with local anomalies in terms of the whole trajectory. In particular, the MDL-C algorithm (the combination of HDP-HMM with MDL segmentation and Citation kNN) achieves the highest accuracy and recall rates. We further show that TRASMIL is generic enough to adopt other algorithms for identifying local anomalies.\n</div> \n<p></p>"},{"id":171,"title":"Unsupervised Track Classification Based on Hierarchical Dirichlet Processes","url":"https://www.researchgate.net/publication/256589304_Unsupervised_Track_Classification_Based_on_Hierarchical_Dirichlet_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n An unsupervised track classification approach is applied to sonar multistatic multitarget tracking. Appropriate discriminative and aggregative features are derived from beamformed and normalized matched-filtered data as recorded from a linear array towed behind an AUV. A clustering algorithm based on Hierarchical Dirichlet Processes is proposed for unsupervised classification of tracks. Overall improvement of target tracking is demonstrated via the Optimal Subpattern Assignment metric.\n</div> \n<p></p>"},{"id":172,"title":"Inference in Hidden Markov Models with Explicit State Duration Distributions","url":"https://www.researchgate.net/publication/221667869_Inference_in_Hidden_Markov_Models_with_Explicit_State_DurationDistributions","abstraction":"In this letter we borrow from the inference techniques developed for unbounded state-cardinality (nonparametric) variants of the HMM and use them to develop a tuning-parameter free, black-box inference procedure for Explicit-state-duration hidden Markov models (EDHMM). EDHMMs are HMMs that have latent states consisting of both discrete state-indicator and discrete state-duration random variables. In contrast to the implicit geometric state duration distribution possessed by the standard HMM, EDHMMs allow the direct parameterisation and estimation of per-state duration distributions. As most duration distributions are defined over the positive integers, truncation or other approximations are usually required to perform EDHMM inference."},{"id":173,"title":"Exact Sampling and Decoding in High-Order Hidden Markov Models","url":"https://www.researchgate.net/publication/229088276_Exact_Sampling_and_Decoding_in_High-Order_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present a method for exact optimization and sampling from high order Hidden Markov Models (HMMs), which are generally handled by approximation techniques. Motivated by adaptive rejection sampling and heuristic search, we propose a strategy based on sequentially refining a lower-order language model that is an upper bound on the true model we wish to decode and sample from. This allows us to build tractable variable-order HMMs. The ARPA format for language models is extended to enable an efficient use of the max-backoff quantities required to compute the upper bound. We evaluate our approach on two problems: a SMS-retrieval task and a POS tagging experiment using 5-gram models. Results show that the same approach can be used for exact optimization and sampling, while explicitly constructing only a fraction of the total implicit state-space.\n</div> \n<p></p>"},{"id":174,"title":"Bayesian nonparametric spectrogram modeling based on infinite factorial infinite hidden Markov model","url":"https://www.researchgate.net/publication/221016740_Bayesian_nonparametric_spectrogram_modeling_based_on_infinite_factorial_infinite_hidden_Markov_model","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a Bayesian nonparametric latent source discovery method for music signal analysis. In audio signal analysis, an important goal is to decompose music signals into individual notes, with applications such as music transcription, source separation or note-level manipulation. Recently, the use of latent variable decompositions, especially nonnegative matrix factorization (NMF), has been a very active area of research. These methods are facing two, mutually dependent, problems: first, instrument sounds often exhibit time-varying spectra, and grasping this time-varying nature is an important factor to characterize the diversity of each instrument; moreover, in many cases we do not know in advance the number of sources and which instruments are played. Conventional decompositions generally fail to cope with these issues as they suffer from the difficulties of automatically determining the number of sources and automatically grouping spectra into single events. We address both these problems by developing a Bayesian nonparametric fusion of NMF and hidden Markov model (HMM). Our model decomposes music spectrograms in an automatically estimated number of components, each of which consisting in an HMM whose number of states is also automatically estimated from the data.\n</div> \n<p></p>"},{"id":175,"title":"On-Line Learning for the Infinite Hidden Markov Model","url":"https://www.researchgate.net/publication/220504941_On-Line_Learning_for_the_Infinite_Hidden_Markov_Model","abstraction":"We develop a sequential Monte Carlo algorithm for the infinite hidden Markov model (iHMM) that allows us to perform on-line inferences on both system states and structural (static) parameters. The algorithm described here provides a natural alternative to Markov chain Monte Carlo samplers previously developed for the iHMM, and is particularly helpful in applications where data is collected sequentially and model parameters need to be continuously updated. We illustrate our approach in the context of both a simulation study and a financial application."},{"id":176,"title":"Variational Methods in Image Segmentation : With 7 image processing experiments","url":"https://www.researchgate.net/publication/46953913_Variational_Methods_in_Image_Segmentation_With_7_image_processing_experiments","abstraction":"Bibliogr. s. 215-237"},{"id":177,"title":"Unsupervised texture segmentation in a deterministic annealing framework","url":"https://www.researchgate.net/publication/3192873_Unsupervised_texture_segmentation_in_a_deterministic_annealing_framework","abstraction":"We present a novel optimization framework for unsupervised texture segmentation that relies on statistical tests as a measure of homogeneity. Texture segmentation is formulated as a data clustering problem based on sparse proximity data. Dissimilarities of pairs of textured regions are computed from a multiscale Gabor filter image representation. We discuss and compare a class of clustering objective functions which is systematically derived from invariance principles. As a general optimization framework, we propose deterministic annealing based on a mean-field approximation. The canonical way to derive clustering algorithms within this framework as well as an efficient implementation of mean-field annealing and the closely related Gibbs sampler are presented. We apply both annealing variants to Brodatz-like microtexture mixtures and real-word images"},{"id":178,"title":"A variational model for image classification and restoration","url":"https://www.researchgate.net/publication/3193107_A_variational_model_for_image_classification_and_restoration","abstraction":"We present a variational model devoted to image classification coupled with an edge-preserving regularization process. The discrete nature of classification (i.e., to attribute a label to each pixel) has led to the development of many probabilistic image classification models, but rarely to variational ones. In the last decade, the variational approach has proven its efficiency in the field of edge-preserving restoration. We add a classification capability which contributes to provide images composed of homogeneous regions with regularized boundaries, a region being defined as a set of pixels belonging to the same class. The soundness of our model is based on the works developed on the phase transition theory in mechanics. The proposed algorithm is fast, easy to implement, and efficient. We compare our results on both synthetic and satellite images with the ones obtained by a stochastic model using a Potts regularization"},{"id":179,"title":"Bayesian Nonparametric Inference for Random Distributions and Related Functions","url":"https://www.researchgate.net/publication/4993246_Bayesian_Nonparametric_Inference_for_Random_Distributions_and_Related_Functions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In recent years, Bayesian nonparametric inference, both theoretical and computational, has witnessed considerable advances. However, these advances have not received a full critical and comparative analysis of their scope, impact and limitations in statistical modelling; many aspects of the theory and methods remain a mystery to practitioners and many open questions remain. In this paper, we discuss and illustrate the rich modelling and analytic possibilities that are available to the statistician within the Bayesian nonparametric and/or semiparametric framework.\n</div> \n<p></p>"},{"id":180,"title":"Stability-Based Validation of Clustering Solutions","url":"https://www.researchgate.net/publication/8575986_Stability-Based_Validation_of_Clustering_Solutions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Data clustering describes a set of frequently employed techniques in exploratory data analysis to extract \"natural\" group structure in data. Such groupings need to be validated to separate the signal in the data from spurious structure. In this context, finding an appropriate number of clusters is a particularly important model selection question. We introduce a measure of cluster stability to assess the validity of a cluster model. This stability measure quantifies the reproducibility of clustering solutions on a second sample, and it can be interpreted as a classification risk with regard to class labels produced by a clustering algorithm. The preferred number of clusters is determined by minimizing this classification risk as a function of the number of clusters. Convincing results are achieved on simulated as well as gene expression data sets. Comparisons to other methods demonstrate the competitive performance of our method and its suitability as a general validation tool for clustering solutions in real-world problems.\n</div> \n<p></p>"},{"id":181,"title":"Histogram clustering for unsupervised segmentation and image retrieval","url":"https://www.researchgate.net/publication/222469073_Histogram_clustering_for_unsupervised_segmentation_and_image_retrieval","abstraction":"This paper introduces a novel statistical latent class model for probabilistic grouping of distributional and histogram data. Adopting the Bayesian framework, we propose to perform annealed maximum a posteriori estimation to compute optimal clustering solutions. In order to accelerate the optimization process, an efficient multiscale formulation is developed. We present a prototypical application of this method for unsupervised segmentation of textured images based on local distributions of Gabor coefficients. Benchmark results indicate superior performance compared to K-means clustering and proximity-based algorithms. In a second application the histogram clustering method is utilized to structure image databases for improved image retrieval."},{"id":182,"title":"Efficient MCMC Schemes for Robust Model Extensions Using Encompassing Dirichlet Process Mixture Models","url":"https://www.researchgate.net/publication/2469229_Efficient_MCMC_Schemes_for_Robust_Model_Extensions_Using_Encompassing_Dirichlet_Process_Mixture_Models","abstraction":"We propose that one consider sensitivity analysis by embedding standard parametric models in model extensions dened by replacing a parametric probability model with a nonparametric extension. The nonparametric model could replace the entire probability model, or some level of a hierarchical model. Specically, we dene nonparametric extensions of a parametric probability model using Dirichlet process (DP) priors. Similar approaches have been used in the literature to implement formal model t diagnostics (Carota, Parmigiani and Polson, 1996). In this paper we discuss at an operational level how such extensions can be implemented. Assuming that inference in the original parametric model is implemented by Markov chain Monte Carlo (MCMC) simulation, we show how minimal additional code can turn the same program into an implementation of MCMC in the larger encompassing model, allowing formal sensitivity analysis with respect to prior and likelihood assumptions. If the base me..."},{"id":183,"title":"Bagging for path-based clustering","url":"https://www.researchgate.net/publication/3193631_Bagging_for_path-based_clustering","abstraction":"A resampling scheme for clustering with similarity to bootstrap aggregation (bagging) is presented. Bagging is used to improve the quality of path-based clustering, a data clustering method that can extract elongated structures from data in a noise robust way. The results of an agglomerative optimization method are influenced by small fluctuations of the input data. To increase the reliability of clustering solutions, a stochastic resampling method is developed to infer consensus clusters. A related reliability measure allows us to estimate the number of clusters, based on the stability of an optimized cluster solution under resampling. The quality of path-based clustering with resampling is evaluated on a large image data set of human segmentations."},{"id":184,"title":"Estimating Normal Means with a Dirichlet Process Prior","url":"https://www.researchgate.net/publication/238684341_Estimating_Normal_Means_with_a_Dirichlet_Process_Prior","abstraction":"In this article, the Dirichlet process prior is used to provide a nonparametric Bayesian estimate of a vector of normal means. In the past there have been computational difficulties with this model. This article solves the computational difficulties by developing a “Gibbs sampler” algorithm. The estimator developed in this article is then compared to parametric empirical Bayes estimators (PEB) and nonparametric empirical Bayes estimators (NPEB) in a Monte Carlo study. The Monte Carlo study demonstrates that in some conditions the PEB is better than the NPEB and in other conditions the NPEB is better than the PEB. The Monte Carlo study also shows that the estimator developed in this article produces estimates that are about as good as the PEB when the PEB is better and produces estimates that are as good as the NPEB estimator when that method is better."},{"id":185,"title":"Bayes procedures for adaptive inference in inverse problems for the white noise model","url":"https://www.researchgate.net/publication/230859317_Bayes_procedures_for_adaptive_inference_in_inverse_problems_for_the_white_noise_model","abstraction":"We study empirical and hierarchical Bayes approaches to the problem of estimating an infinite-dimensional parameter in mildly ill-posed inverse problems. We consider a class of prior distributions indexed by a hyperparameter that quantifies regularity. We prove that both methods we consider succeed in automatically selecting this parameter optimally, resulting in optimal convergence rates for truths with Sobolev or analytic \"smoothness\", without using knowledge about this regularity. Both methods are illustrated by simulation examples."},{"id":186,"title":"Indian Sign Language Recognition using Skin Colour Detection","url":"https://www.researchgate.net/publication/264783992_Indian_Sign_Language_Recognition_using_Skin_Colour_Detection","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Sign language recognition is an important topic of current research in today's\n <br> scenario. The systems developed till now worldwide in this area are under\n <br> scanner for standardization. But, the situation of automatic gesture recognition\n <br> of Indian Sign Language is in its early stage. So, the focus of this research is to\n <br> develop an automatic gesture recognition system for Indian Sign Language. In\n <br> this paper, the focus is on the dataset development, application of various\n <br> feature extraction methods in combination with skin colour detection and\n <br> analysis of results of k-Nearest Neighbor and Neural Network classifiers in\n <br> specific words belongs to computer terminologies of Indian Sign Language.\n <br> The skin colour detection method helps us to extract hand and face portions of\n <br> images from input video frames. This ignores other components available in\n <br> image frames, like body parts and image background, which are not desired\n <br> features for the classification. The best results obtained from k-Nearest\n <br> Neighbor classifier is 97.04% with image pixel feature extraction method. An\n <br> accuracy rate of 97.00% obtained in combination of image pixel feature\n <br> extraction method and Neural Network classifier.\n</div> \n<p></p>"},{"id":187,"title":"Probabilistic image segmentation with closedness constraints","url":"https://www.researchgate.net/publication/221110148_Probabilistic_image_segmentation_with_closedness_constraints","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a novel graphical model for probabilistic image segmentation that contributes both to aspects of perceptual grouping in connection with image segmentation, and to globally optimal inference with higher-order graphical models. We represent image partitions in terms of cellular complexes in order to make the duality between connected regions and their contours explicit. This allows us to formulate a graphical model with higher-order factors that represent the requirement that all contours must be closed. The model induces a probability measure on the space of all partitions, concentrated on perceptually meaningful segmentations. We give a complete polyhedral characterization of the resulting global inference problem in terms of the multicut polytope and efficiently compute global optima by a cutting plane method. Competitive results for the Berkeley segmentation benchmark confirm the consistency of our approach.\n</div> \n<p></p>"},{"id":188,"title":"Majorization-minimization mixture model determination in image segmentation","url":"https://www.researchgate.net/publication/224254746_Majorization-minimization_mixture_model_determination_in_image_segmentation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A new Bayesian model for image segmentation based on a Gaussian mixture model is proposed. The model structure allows the automatic determination of the number of segments while ensuring spatial smoothness of the final output. This is achieved by defining two separate mixture weight sets: the first set of weights is spatially variant and incorporates an MRF edge-preserving smoothing prior; the second set of weights is governed by a Dirichlet prior in order to prune unnecessary mixture components. The model is trained using variational inference and the Majorization-Minimization (MM) algorithm, resulting in closed-form parameter updates. The algorithm was successfully evaluated in terms of various segmentation indices using the Berkeley image data base.\n</div> \n<p></p>"},{"id":189,"title":"Nonparametric Bayesian models through probit stick-breaking processes","url":"https://www.researchgate.net/publication/228562511_Nonparametric_Bayesian_models_through_probit_stick-breaking_processes","abstraction":"We describe a novel class of Bayesian nonparametric priors based on stick-breaking constructions where the weights of the process are constructed as probit transformations of normal random variables. We show that these priors are extremely flexible, allowing us to generate a great variety of models while preserv-ing computational simplicity. Particular emphasis is placed on the construction of rich temporal and spatial processes, which are applied to two problems in finance and ecology."},{"id":190,"title":"Inferring Interaction Networks using the IBP applied to microRNA Target Prediction","url":"https://www.researchgate.net/publication/228469494_Inferring_Interaction_Networks_using_the_IBP_applied_to_microRNA_Target_Prediction","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Determining interactions between entities and the overall organization and clus-tering of nodes in networks is a major challenge when analyzing biological and social network data. Here we extend the Indian Buffet Process (IBP), a nonpara-metric Bayesian model, to integrate noisy interaction scores with properties of individual entities for inferring interaction networks and clustering nodes within these networks. We present an application of this method to study how microR-NAs regulate mRNAs in cells. Analysis of synthetic and real data indicates that the method improves upon prior methods, correctly recovers interactions and clusters, and provides accurate biological predictions.\n</div> \n<p></p>"},{"id":191,"title":"Logistic Stick-Breaking Process","url":"https://www.researchgate.net/publication/220320766_Logistic_Stick-Breaking_Process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A logistic stick-breaking process (LSBP) is proposed for non-parametric clustering of general spatially- or temporally-dependent data, imposing the belief that proximate data are more likely to be clustered together. The sticks in the LSBP are realized via multiple logistic regression functions, with shrinkage priors employed to favor contiguous and spatially localized segments. The LSBP is also extended for the simultaneous processing of multiple data sets, yielding a hierarchical logistic stick-breaking process (H-LSBP). The model parameters (atoms) within the H-LSBP are shared across the multiple learning tasks. Efficient variational Bayesian inference is derived, and comparisons are made to related techniques in the literature. Experimental analysis is performed for audio waveforms and images, and it is demonstrated that for segmentation applications the LSBP yields generally homogeneous segments with sharp boundaries.\n</div> \n<p></p>"},{"id":192,"title":"Learning Multiscale Representations of Natural Scenes Using Dirichlet Processes","url":"https://www.researchgate.net/publication/4301700_Learning_Multiscale_Representations_of_Natural_Scenes_Using_Dirichlet_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We develop nonparametric Bayesian models for multiscale representations of images depicting natural scene categories. Individual features or wavelet coefficients are marginally described by Dirichlet process (DP) mixtures, yielding the heavy-tailed marginal distributions characteristic of natural images. Dependencies between features are then captured with a hidden Markov tree, and Markov chain Monte Carlo methods used to learn models whose latent state space grows in complexity as more images are observed. By truncating the potentially infinite set of hidden states, we are able to exploit efficient belief propagation methods when learning these hierarchical Dirichlet process hidden Markov trees (HDP-HMTs) from data. We show that our generative models capture interesting qualitative structure in natural scenes, and more accurately categorize novel images than models which ignore spatial relationships among features.\n</div> \n<p></p>"},{"id":193,"title":"The ICSI RT07s Speaker Diarization System","url":"https://www.researchgate.net/publication/221545724_The_ICSI_RT07s_Speaker_Diarization_System","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we present the ICSI speaker diarization system. This system was used in the 2007 National Institute of Standards\n <br> and Technology (NIST) Rich Transcription evaluation. The ICSI system automatically performs both speaker segmentation and\n <br> clustering without any prior knowledge of the identities or the number of speakers. Our system uses “standard” speech processing \n <br> components and techniques such as HMMs, agglomerative clustering, and the Bayesian Information Criterion. However, we have\n <br> developed the system with an eye towards robustness and ease of portability. Thus we have avoided the use of any sort of model\n <br> that requires training on “outside” data and we have attempted to develop algorithms that require as little tuning as possible.\n <br> \n <br> The system is simular to last year’s system [1] except for three aspects. We used the most recent available version of the\n <br> beam-forming toolkit, we implemented a new speech/non-speech detector that does not require models trained on meeting data\n <br> and we performed our development on a much larger set of recordings.\n</div> \n<p></p>"},{"id":194,"title":"Exact and Approximate Sum Representations for the Dirichlet process.","url":"https://www.researchgate.net/publication/227688851_Exact_and_Approximate_Sum_Representations_for_the_Dirichlet_process","abstraction":"The Dirichlet process can be regarded as a random probability measure for which the authors examine various sum representations. They consider in particular the gamma process construction of Ferguson (1973) and the “stick-breaking” construction of Sethuraman (1994). They propose a Dirichlet finite sum representation that strongly approximates the Dirichlet process. They assess the accuracy of this approximation and characterize the posterior that this new prior leads to in the context of Bayesian nonpara-metric hierarchical models.Représentations exactes et approximatives du processus de Dirichlet par des sommesLe processus de Dirichlet constitue une mesure de probabilité aléatoire dont les auteurs examinent différentes représentations à l'aide de sommes. Us s'intéressent en particulier à la construction de Ferguson (1973) fondée sur la loi gamma et à la construction dite à “b?tons rompus” de Sethuraman (1994). Ds proposent une approximation forte du processus de Dirichlet par somme finie de type Dirichlet. Ils évaluent la qualité de cette approximation qui conduit à une loi a priori dont ils caractérisent la loi a posteriori dans le cadre des modèles bayésiens hiérarchiques non paramétriques."},{"id":195,"title":"The Nested Dirichlet Process","url":"https://www.researchgate.net/publication/227369072_The_Nested_Dirichlet_Process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In multicenter studies, subjects in different centers may have different outcome distri- butions. This article is motivated by the problem of nonparametric modeling of these distributions, borrowing information across centers while also allowing centers to be clustered. Starting with a stick- breaking representation of the Dirichlet process (DP), we replace the random atoms with random prob- ability measures drawn from a DP. This results in a nested Dirichlet process (nDP) prior, which can be placed on the collection of distributions for the different centers, with centers drawn from the same DP component automatically clustered together. Theoretical properties are discussed, and an efficient MCMC algorithm is developed for computation. The methods are illustrated using a simulation study and an application to quality of care in US hospitals.\n</div> \n<p></p>"},{"id":196,"title":"Hidden Markov Dirichlet process: modeling genetic inference in open ancestral space","url":"https://www.researchgate.net/publication/250303330_Hidden_Markov_Dirichlet_process_modeling_genetic_inference_in_open_ancestral_space","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The problem of inferring the population structure, linkage disequilibrium pattern, and\n <br> chromosomal recombination hotspots from genetic polymorphism data is essential for\n <br> understanding the origin and characteristics of genome variations, with important\n <br> applications to the genetic analysis of disease propensities and other complex traits.\n <br> Statistical genetic methodologies developed so far mostly address these problems\n <br> separately using specialized models ranging from coalescence and admixture models for\n <br> population structures, to hidden Markov models and renewal processes for recombination;\n <br> but most of these approaches ignore the inherent uncertainty in the genetic complexity\n <br> (e.g., the number of genetic founders of a population) of the data and the close\n <br> statistical and biological relationships among objects studied in these problems. We\n <br> present a new statistical framework called hidden Markov Dirichlet process (HMDP) to\n <br> jointly model the genetic recombinations among a possibly infinite number of founders and\n <br> the coalescence-with-mutation events in the resulting genealogies. The HMDP posits that a\n <br> haplotype of genetic markers is generated by a sequence of recombination events that\n <br> select an ancestor for each locus from an unbounded set of founders according to a\n <br> 1st-order Markov transition process. Conjoining this process with a mutation model, our\n <br> method accommodates both between-lineage recombination and within-lineage sequence\n <br> variations, and leads to a compact and natural interpretation of the population structure \n <br> and inheritance process underlying haplotype data. We have developed an efficient sampling\n <br> algorithm for HMDP based on a two-level nested Pólya urn scheme, and we present\n <br> experimental results on joint inference of population structure, linkage disequilibrium,\n <br> and recombination hotspots based on HMDP. On both simulated and real SNP haplotype data,\n <br> our method performs competitively or significantly better than extant methods in\n <br> uncovering the recombination hotspots along chromosomal loci; and in addition it also\n <br> infers the ancestral genetic patterns and offers a highly accurate map of ancestral\n <br> compositions of modern populations.\n</div> \n<p></p>"},{"id":197,"title":"Bayesian Methods for Hidden Markov Models: Recursive Computing in the 21st Century","url":"https://www.researchgate.net/publication/4745686_Bayesian_Methods_for_Hidden_Markov_Models_Recursive_Computing_in_the_21st_Century","abstraction":"This paper presents additional evidence on the international nature of the “Great Moderation:\" the apparent structural decline in the variance of GDP growth first documented in the United States. We find evidence of a similar reduction in volatility in the other G-7 countries and Australia. However, the timing and nature of the moderation varies considerably across countries. We also assess whether and how business cycles have changed in the period since the Great Moderation. Our results suggest that in most of these countries, the major change in business cycles has been a noticeably slower rate of GDP growth in expansions. We find little evidence of milder recessions in the post-Moderation period. Although a lower variance of growth will result in longer expansions and rarer recessions, the evidence presented here suggests that recessions have been just as severe, on average, when they do occur."},{"id":198,"title":"Rabiner, L.: A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. Proc. IEEE 77(2), 257-286","url":"https://www.researchgate.net/publication/2984124_Rabiner_L_A_Tutorial_on_Hidden_Markov_Models_and_Selected_Applications_in_Speech_Recognition_Proc_IEEE_772_257-286","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This tutorial provides an overview of the basic theory of hidden\n <br> Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and\n <br> gives practical details on methods of implementation of the theory along\n <br> with a description of selected applications of the theory to distinct\n <br> problems in speech recognition. Results from a number of original\n <br> sources are combined to provide a single source of acquiring the\n <br> background required to pursue further this area of research. The author\n <br> first reviews the theory of discrete Markov chains and shows how the\n <br> concept of hidden states, where the observation is a probabilistic\n <br> function of the state, can be used effectively. The theory is\n <br> illustrated with two simple examples, namely coin-tossing, and the\n <br> classic balls-in-urns system. Three fundamental problems of HMMs are\n <br> noted and several practical techniques for solving these problems are\n <br> given. The various types of HMMs that have been studied, including\n <br> ergodic as well as left-right models, are described\n</div> \n<p></p>"},{"id":199,"title":"Hierarchical Dirichlet Processes","url":"https://www.researchgate.net/publication/32896369_Hierarchical_Dirichlet_Processes","abstraction":"We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the \"Chinese restaurant franchise.\" We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling."},{"id":200,"title":"Accelerometer based Activity Classification with Variational Inference on Sticky HDP-SLDS","url":"https://www.researchgate.net/publication/283043558_Accelerometer_based_Activity_Classification_with_Variational_Inference_on_Sticky_HDP-SLDS","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n As part of daily monitoring of human activities, wearable sensors and devices\n <br> are becoming increasingly popular sources of data. With the advent of\n <br> smartphones equipped with acceloremeter, gyroscope and camera; it is now\n <br> possible to develop activity classification platforms everyone can use\n <br> conveniently. In this paper, we propose a fast inference method for an\n <br> unsupervised non-parametric time series model namely variational inference for\n <br> sticky HDP-SLDS(Hierarchical Dirichlet Process Switching Linear Dynamical\n <br> System). We show that the proposed algorithm can differentiate various indoor\n <br> activities such as sitting, walking, turning, going up/down the stairs and\n <br> taking the elevator using only the acceloremeter of an Android smartphone\n <br> Samsung Galaxy S4. We used the front camera of the smartphone to annotate\n <br> activity types precisely. We compared the proposed method with Hidden Markov\n <br> Models with Gaussian emission probabilities on a dataset of 10 subjects. We\n <br> showed that the efficacy of the stickiness property. We further compared the\n <br> variational inference to the Gibbs sampler on the same model and show that\n <br> variational inference is faster in one order of magnitude.\n</div> \n<p></p>"},{"id":201,"title":"Sequential Bayesian inference for implicit hidden Markov models and current limitations","url":"https://www.researchgate.net/publication/276923083_Sequential_Bayesian_inference_for_implicit_hidden_Markov_models_and_current_limitations","abstraction":"Hidden Markov models can describe time series arising in various fields of science, by treating the data as noisy measurements of an arbitrarily complex Markov process. Sequential Monte Carlo (SMC) methods have become standard tools to estimate the hidden Markov process given the observations and a fixed parameter value. We review some of the recent developments allowing the inclusion of parameter uncertainty as well as model uncertainty. The shortcomings of the currently available methodology are emphasised from an algorithmic complexity perspective. The statistical objects of interest for time series analysis are illustrated on a toy \"Lotka-Volterra\" model used in population ecology. Some open challenges are discussed regarding the scalability of the reviewed methodology to longer time series, higher-dimensional state spaces and more flexible models."},{"id":202,"title":"An Adaptive Online HDP-HMM for Segmentation and Classification of Sequential Data","url":"https://www.researchgate.net/publication/273472293_An_Adaptive_Online_HDP-HMM_for_Segmentation_and_Classification_of_Sequential_Data","abstraction":"In the recent years, the desire and need to understand sequential data has been increasing, with particular interest in sequential contexts such as patient monitoring, understanding daily activities, video surveillance, stock market and the like. Along with the constant flow of data, it is critical to classify and segment the observations on-the-fly, without being limited to a rigid number of classes. In addition, the model needs to be capable of updating its parameters to comply with possible evolutions. This interesting problem, however, is not adequately addressed in the literature since many studies focus on offline classification over a pre-defined class set. In this paper, we propose a principled solution to this gap by introducing an adaptive online system based on Markov switching models with hierarchical Dirichlet process priors. This infinite adaptive online approach is capable of segmenting and classifying the sequential data over unlimited number of classes, while meeting the memory and delay constraints of streaming contexts. The model is further enhanced by introducing a learning rate, responsible for balancing the extent to which the model sustains its previous learning (parameters) or adapts to the new streaming observations. Experimental results on several variants of stationary and evolving synthetic data and two video datasets, TUM Assistive Kitchen and collatedWeizmann, show remarkable performance in segmentation and classification, particularly for evolutionary sequences with changing distributions and/or containing new, unseen classes."},{"id":203,"title":"Inferring Traffic Signal Phases From Turning Movement Counters Using Hidden Markov Models","url":"https://www.researchgate.net/publication/273395656_Inferring_Traffic_Signal_Phases_From_Turning_Movement_Counters_Using_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This work poses the problem of estimating traffic signal phases from a sequence of maneuvers. We model the problem as an inference problem on a discrete-time hidden Markov model (HMM) in which maneuvers are observations and signal phases are hidden states. The model is calibrated from maneuver observations using either the classical Baum-Welch algorithm or a Bayesian learning algorithm. The trained model is then used to infer the traffic signal phases on the data set via the Viterbi algorithm. When training with the Bayesian learning algorithm, we set the prior distribution as a Dirichlet distribution. We identify the best parameters of the prior distribution for both fixed-time and sensor-actuated signals using numerical simulations and employ them in the field experiments. It is shown that when the model is trained by the Bayesian learning method with appropriate prior parameters from the Dirichlet distribution, the inferred phases are more accurate in both numerical and field experiments. Because the best set of prior parameters for a fixed-time intersection is different from those for sensor-actuated signals, a classification strategy to distinguish between these two types of signals is proposed. The supporting source code and data are available for download at https://github.com/reisiga2/TrafficSignalPhaseEstimation.\n</div> \n<p></p>"},{"id":204,"title":"A Spectral Algorithm for Inference in Hidden Semi-Markov Models","url":"https://www.researchgate.net/publication/263930266_A_Spectral_Algorithm_for_Inference_in_Hidden_Semi-Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Hidden semi-Markov models (HSMMs) are latent variable models which allow\n <br> latent state persistence and can be viewed as a generalization of the popular\n <br> hidden Markov models (HMMs). In this paper, we introduce a novel spectral\n <br> algorithm to perform inference in HSMMs. Unlike expectation maximization (EM),\n <br> our approach correctly estimates the probability of given observation sequence\n <br> based on a set of training sequences. Our approach is based on estimating\n <br> certain sample moments, whose order depends only logarithmically on the maximum\n <br> length of the hidden state persistence. Moreover, the algorithm requires only a\n <br> few spectral decompositions and is therefore computationally efficient.\n <br> Empirical evaluations on synthetic and real data demonstrate the promise of the\n <br> algorithm.\n</div> \n<p></p>"},{"id":205,"title":"Modeling topic control to detect influence in conversations using nonparametric topic models","url":"https://www.researchgate.net/publication/258164431_Modeling_topic_control_to_detect_influence_in_conversations_using_nonparametric_topic_models","abstraction":"Identifying influential speakers in multi-party conversations has been the focus of research in communication, sociology, and psychology for decades. It has been long acknowledged qualitatively that controlling the topic of a conversation is a sign of influence. To capture who introduces new topics in conversations, we introduce SITS—Speaker Identity for Topic Segmentation—a nonparametric hierarchical Bayesian model that is capable of discovering (1) the topics used in a set of conversations, (2) how these topics are shared across conversations, (3) when these topics change during conversations, and (4) a speaker-specific measure of “topic control”. We validate the model via evaluations using multiple datasets, including work meetings, online discussions, and political debates. Experimental results confirm the effectiveness of SITS in both intrinsic and extrinsic evaluations."},{"id":206,"title":"Bayesian approaches to acoustic modeling: A review","url":"https://www.researchgate.net/publication/259428461_Bayesian_approaches_to_acoustic_modeling_A_review","abstraction":"This paper focuses on applications of Bayesian approaches to acoustic modeling for speech recognition and related speech-processing applications. Bayesian approaches have been widely studied in the fields of statistics and machine learning, and one of their advantages is that their generalization capability is better than that of conventional approaches (e.g., maximum likelihood). On the other hand, since inference in Bayesian approaches involves integrals and expectations that are mathematically intractable in most cases and require heavy numerical computations, it is generally difficult to apply them to practical speech recognition problems. However, there have been many such attempts, and this paper aims to summarize these attempts to encourage further progress on Bayesian approaches in the speech-processing field. This paper describes various applications of Bayesian approaches to speech processing in terms of the four typical ways of approximating Bayesian inferences, i.e., maximum a posteriori approximation, model complexity control using a Bayesian information criterion based on asymptotic approximation, variational approximation, and Markov chain Monte Carlo-based sampling techniques."},{"id":207,"title":"Visual Workflow Recognition Using a Variational Bayesian Treatment of Multistream Fused Hidden Markov Models","url":"https://www.researchgate.net/publication/237035676_Visual_Workflow_Recognition_Using_a_Variational_Bayesian_Treatment_of_Multistream_Fused_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we provide a variational Bayesian (VB) treatment of multistream fused hidden Markov models (MFHMMs), and apply it in the context of active learning-based visual workflow recognition (WR). Contrary to training methods yielding point estimates, such as maximum likelihood or maximum a posteriori training, the VB approach provides an estimate of the posterior distribution over the MFHMM parameters. As a result, our approach provides an elegant solution toward the amelioration of the overfitting issues of point estimate-based methods. Additionally, it provides a measure of confidence in the accuracy of the learned model, thus allowing for the easy and cost-effective utilization of active learning in the context of MFHMMs. Two alternative active learning algorithms are considered in this paper: query by committee, which selects unlabeled data that minimize the classification variance, and a maximum information gain method that aims to maximize the alteration in model variance by proper data labeling. We demonstrate the efficacy of the proposed treatment of MFHMMs by examining two challenging WR scenarios, and show that the application of active learning, which is facilitated by our VB approach, allows for a significant reduction of the MFHMM training costs.\n</div> \n<p></p>"},{"id":208,"title":"Infinite Hidden Conditional Random Fields for Human Behavior Analysis","url":"https://www.researchgate.net/publication/260353896_Infinite_Hidden_Conditional_Random_Fields_for_Human_Behavior_Analysis","abstraction":"Hidden conditional random fields (HCRFs) are discriminative latent variable models that have been shown to successfully learn the hidden structure of a given classification problem (provided an appropriate validation of the number of hidden states). In this brief, we present the infinite HCRF (iHCRF), which is a nonparametric model based on hierarchical Dirichlet processes and is capable of automatically learning the optimal number of hidden states for a classification task. We show how we learn the model hyperparameters with an effective Markov-chain Monte Carlo sampling technique, and we explain the process that underlines our iHCRF model with the Restaurant Franchise Rating Agencies analogy. We show that the iHCRF is able to converge to a correct number of represented hidden states, and outperforms the best finite HCRFs-chosen via cross-validation-for the difficult tasks of recognizing instances of agreement, disagreement, and pain. Moreover, the iHCRF manages to achieve this performance in significantly less total training, validation, and testing time."},{"id":209,"title":"Hidden Conditional Random Fields","url":"https://www.researchgate.net/publication/6139655_Hidden_Conditional_Random_Fields","abstraction":"We present a discriminative latent variable model for classification problems in structured domains where inputs can be represented by a graph of local observations. A hidden-state Conditional Random Field framework learns a set of latent variables conditioned on local features. Observations need not be independent and may overlap in space and time."},{"id":210,"title":"Canal9: A database of political debates for analysis of social interactions","url":"https://www.researchgate.net/publication/224088111_Canal9_A_database_of_political_debates_for_analysis_of_social_interactions","abstraction":"Automatic analysis of social interactions attracts major attention in the computing community, but relatively few benchmarks are available to researchers active in the domain. This paper presents a new, publicly available, corpus of political debates including not only raw data, but a rich set of socially relevant annotations such as turn-taking (who speaks when and how much), agreement and disagreement between participants, and role played by people involved in each debate. The collection includes 70 debates for a total of 43 hours and 10 minutes of material."},{"id":211,"title":"The Infinite Factorial Hidden Markov Model","url":"https://www.researchgate.net/publication/221617836_The_Infinite_Factorial_Hidden_Markov_Model","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We introduce a new probability distribution over a potentially infinite number of binary Markov chains which we call the Markov Indian buffet process. This process extends the IBP to allow temporal dependencies in the hidden variables. We use this stochastic process to build a nonparametric extension of the factorial hidden Markov model. After constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the infinite factorial hidden Markov model can be used for blind source separation. 1\n</div> \n<p></p>"},{"id":212,"title":"What Engineering Technology Could Do for Quality of Life in Parkinson's Disease: A Review of Current Needs and Opportunities","url":"https://www.researchgate.net/publication/280908562_What_Engineering_Technology_Could_Do_for_Quality_of_Life_in_Parkinson%27s_Disease_A_Review_of_Current_Needs_and_Opportunities","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Parkinson's Disease (PD) involves well known motor symptoms such as tremor, rigidity, bradykinesia, and altered gait but there are also non-locomotory motor symptoms (e.g., changes in handwriting and speech) and even non-motor symptoms (e.g., disrupted sleep, depression) that can be measured, monitored, and possibly better managed through activity based monitoring technologies. This will enhance quality of life (QoL) in PD through improved self-monitoring, and also provide information which could be shared with a health care provider to help better manage treatment. Until recently, non-motor symptoms (\"soft signs\") had been generally overlooked in clinical management yet these are of primary importance to patients and their QoL. Day-to-day variability of the condition, the high variability in symptoms between patients, and the isolated snapshots of a patient in periodic clinic visits makes better monitoring essential to the proper management of PD. Continuously monitored patterns of activity, social interactions, and daily activities could provide a rich source of information on status changes, guiding self correction and clinical management. The same tools can be useful in earlier detection of PD and will improve clinical studies. Remote medical communications in the form of telemedicine, sophisticated tracking of medication use, and assistive technologies that directly compensate for disease related challenges are examples of other near term technology solutions to PD problems. Ultimately, a sensor technology is no good if it is not used. The Parkinson's community is a sophisticated early adopter of useful technologies and a group for which engineers can provide near term gratifying benefits.\n</div> \n<p></p>"},{"id":213,"title":"Pulse spectral analysis from intensive care unit patients","url":"https://www.researchgate.net/publication/260732752_Pulse_spectral_analysis_from_intensive_care_unit_patients","abstraction":"The study analyzed the blood pressures in frequency domain from a group of ICU patients with kidney disease and compared the results with healthy control without kidney disease. Results showed 2nd harmonics had significantly larger energy distribution for patients with kidney disorder while the 4th to 6th harmonics had significantly smaller energy distribution when compared to the control group. The study provided evidence that pulse pressure harmonics are associated with individual vascular beds, such as the kidney vascular bed and indirectly support the resonance theory. The evidence will encourage more research to be done in order to generate substantial implications of pulse spectral analysis in clinical settings."},{"id":214,"title":"Emotional Expression Classification Using Time-Series Kernels","url":"https://www.researchgate.net/publication/237082421_Emotional_Expression_Classification_Using_Time-Series_Kernels","abstraction":"Recent advances in kernel methods are very promising for improving the estimation, clustering and classification of spatio-temporal processes. Facial expression estimation can take advantage of such methods if one considers marker positions and their motion in 3D space. We applied support vector classification with kernels derived from dynamic time-warping similarity measures. We achieved excellent results using only the 'motion pattern' of the PCA compressed representation of the marker point vector, the socalled shape parameters. Beyond the classification of full motion patterns, the classification of the initial phase of an emotion (i) is competitive with methods relying on textural information and (ii) has complementary advantages to texture based algorithms reinforcing the need for mixed approaches."},{"id":215,"title":"A Survey on Visual Human Action Recognition","url":"https://www.researchgate.net/publication/263965917_A_Survey_on_Visual_Human_Action_Recognition","abstraction":"Visual Human Action Recognition is a universal hot topic of image processing, computer vision, pattern recognition, machine learning and artificial intelligence with wide application in video surveillance, human-computer interaction, virtual reality, content based video retrieval, video coding, etc. In this paper, we analyze the state-of-the-arts and advances of this field from perspectives of feature extraction, action recognition and benchmark datasets and competitions. In addition, the problems, difficulties, challenges and valuable future directions of human action recognition are presented."},{"id":216,"title":"Infinite Latent Conditional Random Fields","url":"https://www.researchgate.net/publication/262330359_Infinite_Latent_Conditional_Random_Fields","abstraction":"In this paper, we present Infinite Latent Conditional Random Fields (ILCRFs) that model the data through a mixture of CRFs generated from Dirichlet processes. Each CRF represents one possible explanation of the data. In addition to visible nodes and edges that exist in classic CRFs, it generatively models the distribution of different CRF structures over the latent nodes and corresponding edges, imposing no restriction on the number of both nodes and types of edges. We apply ILCRFs to several applications, such as robotic scene arrangement and scene labeling, where a scene is modeled through, not only objects, but also latent human poses and human-object relations. In extensive experiments, we show that our model outperforms the state-of-the-art results as well as helps a robot placing objects in a new scene."},{"id":217,"title":"The human behavior recognition based on improved R transform and discriminative random fields model","url":"https://www.researchgate.net/publication/287882482_The_human_behavior_recognition_based_on_improved_R_transform_and_discriminative_random_fields_model","abstraction":"To solve the problem of human behavior recognition, an approach based on improved R transform and discriminative random fields model is proposed; the improved R transform has the ability of scale invariance, the two-dimensional planar function does not need to be scale standardization when extracting the features, enhancing the features' robustness. The discriminative random fields improved the Hidden Conditional Random Fields' performance, this model got the behavior's internal dynamic characteristics and the external dynamic contacts. On the public human behavior database, by comparing the experimental results, it demonstrated that this method had better recognition results than the previous methods."},{"id":218,"title":"Learning Harmonium Models With Infinite Latent Features","url":"https://www.researchgate.net/publication/262149085_Learning_Harmonium_Models_With_Infinite_Latent_Features","abstraction":"Undirected latent variable models represent an important class of graphical models that have been successfully developed to deal with various tasks. One common challenge in learning such models is to determine the number of hidden units that are unknown a priori. Although Bayesian nonparametrics have provided promising results in bypassing the model selection problem in learning directed Bayesian Networks, very little effort has been made toward applying Bayesian nonparametrics to learn undirected latent variable models. In this paper, we present the infinite exponential family Harmonium (iEFH), a bipartite undirected latent variable model that automatically determines the number of latent units from an unbounded pool. We also present two important extensions of iEFH to 1) multiview iEFH for dealing with heterogeneous data, and 2) infinite maximum-margin Harmonium (iMMH) for incorporating supervising side information to learn predictive latent features. We develop variational inference algorithms to learn model parameters. Our methods are computationally competitive because of the avoidance of selecting the number of latent units. Our extensive experiments on real image datasets and text datasets appear to demonstrate the benefits of iEFH and iMMH inherited from Bayesian nonparametrics and max-margin learning. Such results were not available until now and contribute to expanding the scope of Bayesian nonparametrics to learn the structures of undirected latent variable models."},{"id":219,"title":"Spatio-temporal Event Classification Using Time-Series Kernel Based Structured Sparsity","url":"https://www.researchgate.net/publication/264120989_Spatio-temporal_Event_Classification_Using_Time-Series_Kernel_Based_Structured_Sparsity","abstraction":"In many behavioral domains, such as facial expression and gesture, sparse structure is prevalent. This sparsity would be well suited for event detection but for one problem. Features typically are confounded by alignment error in space and time. As a consequence, high-dimensional representations such as SIFT and Gabor features have been favored despite their much greater computational cost and potential loss of information. We propose a Kernel Structured Sparsity (KSS) method that can handle both the temporal alignment problem and the structured sparse reconstruction within a common framework, and it can rely on simple features. We characterize spatio-temporal events as time-series of motion patterns and by utilizing time-series kernels we apply standard structured-sparse coding techniques to tackle this important problem. We evaluated the KSS method using both gesture and facial expression datasets that include spontaneous behavior and differ in degree of difficulty and type of ground truth coding. KSS outperformed both sparse and non-sparse methods that utilize complex image features and their temporal extensions. In the case of early facial event classification KSS had 10% higher accuracy as measured by F 1 score over kernel SVM methods."},{"id":220,"title":"On Some Invariant Criteria for Grouping Data","url":"https://www.researchgate.net/publication/243620125_On_Some_Invariant_Criteria_for_Grouping_Data","abstraction":"This paper deals with methods of “cluster analysis”. In particular we attack the problem of exploring the structure of multivariate data in search of “clusters”.The approach taken is to use a computer procedure to obtain the “best” partition of n objects into g groups. A number of mathematical criteria for “best” are discussed and related to statistical theory. A procedure for optimizing the criteria is outlined. Some of the criteria are compared with respect to their behavior on actual data. Results of data analysis are presented and discussed."},{"id":221,"title":"Multivariate Clustering Procedures with Variable Metrics","url":"https://www.researchgate.net/publication/265372318_Multivariate_Clustering_Procedures_with_Variable_Metrics","abstraction":"Several multivariate clustering methods are analyzed in which each cluster may have a different metric depending on its covariance matrix. Numerical experiments show that the only reliable method among these is one using a metric suggested by Rohlf [1970] based on the within cluster covariance matrix normalized for unit determinant. (12 references.)"},{"id":222,"title":"A Cluster Analysis Method for Grouping Means in the Analysis of Variance","url":"https://www.researchgate.net/publication/268494464_A_Cluster_Analysis_Method_for_Grouping_Means_in_the_Analysis_of_Variance","abstraction":"It is sometimes useful in an analysis of variance to split the treatments into reasonably homogeneous groups. Multiple comparison procedures are often used for this purpose, but a more direct method is to use the techniques of cluster analysis. This approach is illustrated for several sets of data, and a likelihood ratio test is developed for judging the significance of differences among the resulting groups."},{"id":223,"title":"Classification by Maximum Posterior Probability","url":"https://www.researchgate.net/publication/38358158_Classification_by_Maximum_Posterior_Probability","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The problem of classifying each of $n$ observations to one of two sub-populations is considered. The classification rule examined chooses that classification with maximum posterior probability. Limiting behavior of the rule is given and several examples are presented which show that the rule can lead to classifying all observations to the same subpopulation. Three simulation studies are reported to indicate that this extreme behavior may occur in small samples.\n</div> \n<p></p>"},{"id":224,"title":"Pattern Clustering by Multivariate Mixture Analysis, Multivariate Behavioral Res., 5, 329-350","url":"https://www.researchgate.net/publication/239665622_Pattern_Clustering_by_Multivariate_Mixture_Analysis_Multivariate_Behavioral_Res_5_329-350","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Cluster analysis is reformulated as a problem of estimating the para- meters of a mixture of multivariate distributions. The maximum-likelihood theory and numerical solution techniques are developed for a fairly general class of distributions. The theory is applied to mixtures of multivariate nor- mals (NORMIX) and mixtures of multivariate Bernoulli distributions (Latent Classes). The feasibility of the procedures is demonstrated by two examples of computer solutions for normal mixture models of the Fisher Iris data and of artifjcially generated clusters with unequal covariance matrices.\n</div> \n<p></p>"},{"id":225,"title":"Miscellanea: Generalized Ewens-Pitman model for Bayesian clustering","url":"https://www.researchgate.net/publication/267568486_Miscellanea_Generalized_Ewens-Pitman_model_for_Bayesian_clustering","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n 5 SUMMARY We propose a Bayesian method for clustering from discrete data structures that commonly arise in genetics and other applications. This method is equivariant with respect to relabeling units; unsampled units do not interfere with sampled data; and missing data do not hinder infer-ence. Cluster inference using the posterior mode performs well on simulated and real data sets, 10 and the posterior predictive distribution enables supervised learning based on a partial clustering of the sample.\n</div> \n<p></p>"},{"id":226,"title":"Bayesian Degree-Corrected Stochastic Block Models for Community Detection","url":"https://www.researchgate.net/publication/256762204_Bayesian_Degree-Corrected_Stochastic_Block_Models_for_Community_Detection","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Community detection in networks has drawn much attention in diverse fields,\n <br> especially social sciences. Given its significance, there has been a large body\n <br> of literature among which many are not statistically based. In this paper, we\n <br> propose a novel stochastic blockmodel based on a logistic regression setup with\n <br> node correction terms to better address this problem. We follow a Bayesian\n <br> approach that explicitly captures the community behavior via prior\n <br> specification. We then adopt a data augmentation strategy with latent\n <br> Polya-Gamma variables to obtain posterior samples. We conduct inference based\n <br> on a canonically mapped centroid estimator that formally addresses label\n <br> non-identifiability. We demonstrate the novel proposed model and estimation on\n <br> real-world as well as simulated benchmark networks and show that the proposed\n <br> model and estimator are more flexible, representative, and yield smaller error\n <br> rates when compared to the MAP estimator from classical degree-corrected\n <br> stochastic blockmodels.\n</div> \n<p></p>"},{"id":227,"title":"Bayesian Estimation for Nonstandard Loss Functions Using a Parametric Family of Estimators","url":"https://www.researchgate.net/publication/254062888_Bayesian_Estimation_for_Nonstandard_Loss_Functions_Using_a_Parametric_Family_of_Estimators","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Bayesian estimation with other loss functions than the standard hit-or-miss loss or the quadratic loss often yields optimal Bayesian estimators (OBEs) that can only be formulated as optimization problems and which have to be solved for each new observation. The contribution of this paper is to introduce a new parametric family of estimators to circumvent this problem. By restricting the estimator to lie in this family, we split the estimation problem into two parts: In a first step, we have to find the best estimator with respect to the Bayes risk for a given nonstandard loss function, which has to be done only once. The second step then calculates the estimate for an observation using importance sampling. The computational complexity of this second step is therefore comparable to that of an MMSE estimator if the MMSE estimator also uses Monte Carlo integration. We study the proposed parametric family using two examples and show that the estimator family gives for both a good approximation of the OBE.\n</div> \n<p></p>"},{"id":228,"title":"Clustering Using Objective Functions and Stochastic Search","url":"https://www.researchgate.net/publication/4993356_Clustering_Using_Objective_Functions_and_Stochastic_Search","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A new approach to clustering multivariate data, based on a multilevel linear mixed model, is proposed. A key feature of the model is that observations from the same cluster are correlated, because they share cluster-specific random effects. The inclusion of cluster-specific random effects allows parsimonious departure from an assumed base model for cluster mean profiles. This departure is captured statistically via the posterior expectation, or best linear unbiased predictor. One of the parameters in the model is the true underlying partition of the data, and the posterior distribution of this parameter, which is known up to a normalizing constant, is used to cluster the data. The problem of finding partitions with high posterior probability is not amenable to deterministic methods such as the EM algorithm. Thus, we propose a stochastic search algorithm that is driven by a Markov chain that is a mixture of two Metropolis-Hastings algorithms-one that makes small scale changes to individual objects and another that performs large scale moves involving entire clusters. The methodology proposed is fundamentally different from the well-known finite mixture model approach to clustering, which does not explicitly include the partition as a parameter, and involves an independent and identically distributed structure. Copyright 2008 Royal Statistical Society.\n</div> \n<p></p>"},{"id":229,"title":"How many clusters?","url":"https://www.researchgate.net/publication/242306947_How_many_clusters","abstraction":"The title poses a deceptively simple question that must be addressed by any statistical model or computational algorithm for the clustering of points. Two distinct interpretations are possible, one connected with the number of clusters in the sample and one with the number in the population. Under suitable conditions, these questions may have essentially the same answer, but it is logically possible for one answer to be finite and the other infinite. This paper reformulates the standard Dirichlet allocation model as a cluster process in such a way that these and related questions can be addressed directly. Our conclusion is that the data are sometimes informative for clustering points in the sample, but they seldom contain much information about parameters such as the number of clusters in the population."},{"id":230,"title":"Bayesian Model-Based Clustering Procedures","url":"https://www.researchgate.net/publication/228672773_Bayesian_Model-Based_Clustering_Procedures","abstraction":"This paper establishes a general formulation for Bayesian model-based clustering, in which subset la-bels are exchangeable, and items are also exchangeable, possibly up to covariate effects. The notational framework is rich enough to encompass a variety of existing procedures, including some recently discussed methodologies involving stochastic search or hierarchical clustering, but more importantly allows the formu-lation of clustering procedures that are optimal with respect to a specified loss function. Our focus is on loss functions based on pairwise coincidences, that is, whether pairs of items are clustered into the same subset or not. Optimisation of the posterior expected loss function can be formulated as a binary integer programming problem, which can be readily solved by standard software when clustering a modest number of items, but quickly becomes impractical as problem scale increases. To combat this, a new heuristic item-swapping algorithm is introduced. This performs well in our numerical experiments, on both simulated and real data examples. The paper includes a comparison of the statistical performance of the (approximate) optimal clustering with earlier methods that are model-based but ad hoc in their detailed definition."},{"id":231,"title":"Traffic state prediction using Markov chain models","url":"https://www.researchgate.net/publication/236670503_Traffic_state_prediction_using_Markov_chain_models","abstraction":"Motorway traffic management and control relies on models that estimate and predict traffic conditions. In this paper, a methodology for the identification and short-term prediction of the traffic state is presented. The methodology combines model-based clustering, variable-length Markov chains and nearest neighbor classification. An application of the methodology for short-term speed prediction in a freeway network in Irvine, CA, shows encouraging results."},{"id":232,"title":"Clustering Based on a Multilayer Mixture Model","url":"https://www.researchgate.net/publication/238879801_Clustering_Based_on_a_Multilayer_Mixture_Model","abstraction":"In model-based clustering, the density of each cluster is usually assumed to be a certain basic parametric distribution, for example, the normal distribution. In practice, it is often difficult to decide which parametric distribution is suitable to characterize a cluster, especially for multivariate data. Moreover, the densities of individual clusters may be multimodal themselves, and therefore cannot be accurately modeled by basic parametric distributions. This article explores a clustering approach that models each cluster by a mixture of normals. The resulting overall model is a multilayer mixture of normals. Algorithms to estimate the model and perform clustering are developed based on the classification maximum likelihood (CML) and mixture maximum likelihood (MML) criteria. BIC and ICL-BIC are examined for choosing the number of normal components per cluster. Experiments on both simulated and real data are presented."},{"id":233,"title":"Two-Mode Cluster Analysis via Hierarchical Bayes","url":"https://www.researchgate.net/publication/226146328_Two-Mode_Cluster_Analysis_via_Hierarchical_Bayes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This manuscript introduces a new Bayesian finite mixture methodology for the joint clustering of row and column stimuli/objects associated with two-mode asymmetric proximity, dominance, or profile data. That is, common clusters are derived which partition both the row and column stimuli/objects simultaneously into the same derived set of clusters. In this manner, interrelationships between both sets of entities (rows and columns) are easily ascertained. We describe the technical details of the proposed two-mode clustering methodology including its Bayesian mixture formulation and a Bayes factor heuristic for model selection. Lastly, a marketing application is provided examining consumer preferences for various brands of luxury automobiles.\n</div> \n<p></p>"},{"id":234,"title":"Model-Based Clustering, Discriminant Analysis, and Density Estimation","url":"https://www.researchgate.net/publication/2350641_Model-Based_Clustering_Discriminant_Analysis_and_Density_Estimation","abstraction":"Cluster analysis is the automated search for groups of related observations in a data set. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as How many clusters are there?\", Which clustering method should be used?\" and How should outliers be handled?\". We outline a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, mineeld detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology, a..."},{"id":235,"title":"Dealing With Label-Switching in Mixture Models","url":"https://www.researchgate.net/publication/2321640_Dealing_With_Label-Switching_in_Mixture_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In a Bayesian analysis of finite mixture models, parameter estimation and clustering are sometimes less straightforward that might be expected. In particular, the common practice of estimating parameters by their posterior mean, and summarising joint posterior distributions by marginal distributions, often leads to nonsensical answers. This is due to the so-called \"labelswitching \" problem, which is caused by symmetry in the likelihood of the model parameters. A frequent response to this problem is to remove the symmetry using artificial identifiability constraints. We demonstrate that this fails in general to solve the problem, and describe an alternative class of approaches, relabelling algorithms, which arise from attempting to minimise the posterior expected loss under a class of loss functions. We describe in detail one particularly simple and general relabelling algorithm, and illustrate its success in dealing with the labelswitching problem on two examples. KEYWORDS: ...\n</div> \n<p></p>"},{"id":236,"title":"Bayesian Variable Selection in Clustering High-Dimensional Data","url":"https://www.researchgate.net/publication/4741972_Bayesian_Variable_Selection_in_Clustering_High-Dimensional_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Over the last decade, technological advances have generated an explosion of data with substantially smaller sample size relative to the number of covariates (p » n). A common goal in the analysis of such data involves uncovering the group structure of the observations and identifying the discriminating variables. In this article we propose a methodology for addressing these problems simultaneously. Given a set of variables, we formulate the clustering problem in terms of a multivariate normal mixture model with an unknown number of components and use the reversible-jump Markov chain Monte Carlo technique to define a sampler that moves between different dimensional spaces. We handle the problem of selecting a few predictors among the prohibitively vast number of variable subsets by introducing a binary exclusion/inclusion latent vector, which gets updated via stochastic search techniques. We specify conjugate priors and exploit the conjugacy by integrating out some of the parameters. We describe strategies for posterior inference and explore the performance of the methodology with simulated and real datasets.\n</div> \n<p></p>"},{"id":237,"title":"Bayesian Clustering of Transcription Factor Binding Motifs","url":"https://www.researchgate.net/publication/4743431_Bayesian_Clustering_of_Transcription_Factor_Binding_Motifs","abstraction":"Genes are often regulated in living cells by proteins called transcription factors (TFs) that bind directly to short segments of DNA in close proximity to specific genes. These binding sites have a conserved nucleotide appearance, which is called a motif. Several recent studies of transcriptional regulation require the reduction of a large collection of motifs into clusters based on the similarity of their nucleotide composition. We present a principled approach to this clustering problem based upon a Bayesian hierarchical model that accounts for both within- and between-motif variability. We use a Dirichlet process prior distribution that allows the number of clusters to vary and we also present a novel generalization that allows the core width of each motif to vary. This clustering model is implemented, using a Gibbs sampling strategy, on several collections of transcription factor motif matrices. Our clusters provide a means by which to organize transcription factors based on binding motif similarities, which can be used to reduce motif redundancy within large databases such as JASPAR and TRANSFAC. Finally, our clustering procedure has been used in combination with discovery of evolutionarily-conserved motifs to predict co-regulated genes. An alternative to our Dirichlet process prior distribution is explored but shows no substantive difference in the clustering results for our datasets. Our Bayesian clustering model based on the Dirichlet process has several advantages over traditional clustering methods that could make our procedure appropriate and useful for many clustering applications."},{"id":238,"title":"Finding Groups in Data: An Introduction To Cluster Analysis","url":"https://www.researchgate.net/publication/220695963_Finding_Groups_in_Data_An_Introduction_To_Cluster_Analysis","abstraction":"This is a book. We are not allowed to upload or share it, sorry."},{"id":239,"title":"Bayesian variable selection in clustering via dirichlet process mixture models","url":"https://www.researchgate.net/publication/26902369_Bayesian_variable_selection_in_clustering_via_dirichlet_process_mixture_models","abstraction":"The increased collection of high-dimensional data in various fields has raised a strong interest in clustering algorithms and variable selection procedures. In this disserta- tion, I propose a model-based method that addresses the two problems simultane- ously. I use Dirichlet process mixture models to define the cluster structure and to introduce in the model a latent binary vector to identify discriminating variables. I update the variable selection index using a Metropolis algorithm and obtain inference on the cluster structure via a split-merge Markov chain Monte Carlo technique. I evaluate the method on simulated data and illustrate an application with a DNA microarray study. I also show that the methodology can be adapted to the problem of clustering functional high-dimensional data. There I employ wavelet thresholding methods in order to reduce the dimension of the data and to remove noise from the observed curves. I then apply variable selection and sample clustering methods in the wavelet domain. Thus my methodology is wavelet-based and aims at clustering the curves while identifying wavelet coefficients describing discriminating local features. I exemplify the method on high-dimensional and high-frequency tidal volume traces measured under an induced panic attack model in normal humans."},{"id":240,"title":"Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models","url":"https://www.researchgate.net/publication/228643791_Sequentially-allocated_merge-split_sampler_for_conjugate_and_nonconjugate_Dirichlet_process_mixture_models","abstraction":"This paper proposes a new efficient merge-split sampler for both conjugate and nonconju-gate Dirichlet process mixture (DPM) models. These Bayesian nonparametric models are usually fit using Markov chain Monte Carlo (MCMC) or sequential importance sampling (SIS). The latest generation of Gibbs and Gibbs-like samplers for both conjugate and nonconjugate DPM models effectively update the model parameters, but can have difficulty in updating the clustering of the data. To overcome this deficiency, merge-split samplers have been developed, but until now these have been limited to conjugate or conditionally-conjugate DPM models. This paper proposes a new MCMC sampler, called the sequentially-allocated merge-split (SAMS) sampler. The sam-pler borrows ideas from sequential importance sampling. Splits are proposed by sequentially allocating observations to one of two split components using allocation probabilities that condi-tion on previously allocated data. The SAMS sampler is applicable to general nonconjugate DPM models as well as conjugate models. Further, the proposed sampler is substantially more efficient than existing conjugate and nonconjugate samplers."},{"id":241,"title":"Computational Statistics and Data Analysis","url":"https://www.researchgate.net/publication/280831448_Computational_Statistics_and_Data_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A new Bayesian approach for quantifying spatial clustering is proposed that employs a mixture of gamma distributions to model the squared distance of points to their second nearest neighbors. The method is designed to answer questions arising in biophysical research on nanoclusters of Ras proteins. It takes into account the presence of disturbing metacluster structures as well as non-clustering objects, both common among Ras clusters. Its focus lies on estimating the proportion of points lying in clusters, the mean cluster size and the mean cluster radius without depending on prior knowledge of the parameters. The performance of the model compared to other cluster methods is demonstrated in a comprehensive simulation study, employing a specific new class of spatial point processes, the double Matérn cluster process. Further results and arguments as well as data and code are available as supplementary material.\n</div> \n<p></p>"},{"id":242,"title":"Discovering transcriptional modules by Bayesian data integration","url":"https://www.researchgate.net/publication/44655656_Discovering_transcriptional_modules_by_Bayesian_data_integration","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present a method for directly inferring transcriptional modules (TMs) by integrating gene expression and transcription factor binding (ChIP-chip) data. Our model extends a hierarchical Dirichlet process mixture model to allow data fusion on a gene-by-gene basis. This encodes the intuition that co-expression and co-regulation are not necessarily equivalent and hence we do not expect all genes to group similarly in both datasets. In particular, it allows us to identify the subset of genes that share the same structure of transcriptional modules in both datasets.\n <br> We find that by working on a gene-by-gene basis, our model is able to extract clusters with greater functional coherence than existing methods. By combining gene expression and transcription factor binding (ChIP-chip) data in this way, we are better able to determine the groups of genes that are most likely to represent underlying TMs.\n <br> If interested in the code for the work presented in this article, please contact the authors.\n <br> Supplementary data are available at Bioinformatics online.\n</div> \n<p></p>"},{"id":243,"title":"Bayesian mixtures for cluster analysis and flexible modeling of distributions","url":"https://www.researchgate.net/publication/44922221_Bayesian_mixtures_for_cluster_analysis_and_flexible_modeling_of_distributions","abstraction":"Finite mixture models assume that a distribution is a combination of several parametric distributions. They offer a compromise between the interpretability of parametric models and the flexibility of nonparametric models. This thesis considers a Bayesian approach to these models, which has several advantages. For example, using only weak prior information, it can solve problems with unbounded likelihood functions, that can occur in mixture models. The Bayesian approach also allows an elegant extension of finite to (countable) infinite mixture models. Depending on the application, the components of mixture models can either be viewed as just a means to the flexible modeling of a distribution or as defining subgroups of a population with different parametric distributions. Regarding the former case consistency results for Bayesian mixtures are stated. An example concerning the flexible modeling of a random effects distribution in a logistic regression is also given. The application considers the goalkeeper's effect in saving a penalty. In the latter case mixture models can be used for clustering. Bayesian mixtures then allow the estimation of the number of clusters at the same time as the cluster-specific parameters. For cluster analysis the standard approach for fitting Bayesian mixtures, Markov Chain Monte Carlo (MCMC), unfortunately leads to inferential difficulties. The labels associated with the clusters can change during the MCMC run, a phenomenon called label-switching. The problem gets severe, if the number of clusters is allowed to vary. Existing methods to deal with label-switching and a varying number of components are reviewed and new approaches are proposed for both situations. The first consists of a variant of the relabeling algorithm of Stephens (2000). The variant is more general, as it applies to drawn clusterings and not drawn parameter values. Therefore it does not depend on the specific form of the component distributions. The second approach is based on pairwise posterior probabilities and is an improvement of a commonly used loss function due to Binder (1978). Minimization of this loss is shown to be equivalent to maximizing the posterior expected Rand index with the true clustering. As the adjusted Rand index is preferable to the raw index, the maximization of the posterior expected adjusted Rand is proposed. The new approaches are compared to the previous methods on simulated and real data. The real data used for cluster analysis are two gene expression data sets and Fisher's iris data."},{"id":244,"title":"Additive mixed models with Dirichlet process mixture and P-spline priors","url":"https://www.researchgate.net/publication/40272219_Additive_mixed_models_with_Dirichlet_process_mixture_and_P-spline_priors","abstraction":"Longitudinal data often require a combination of flexible trends and individual-specific random effects. In this paper, we propose a fully Bayesian approach based on Markov chain Monte Carlo simulation techniques that allows for the semiparametric specification of both the trend function and the random effects distribution. Bayesian penalized splines are considered for the former, while a Dirichlet process mixture (DPM) specification allows for an adaptive amount of deviations from normality for the latter. We investigate the advantages of DPM prior structures for random effects in terms of a simulation study and present a challenging application that requires semiparametric mixed modeling."},{"id":245,"title":"Bayesian correlated clustering to integrate multiple datasets","url":"https://www.researchgate.net/publication/232225549_Bayesian_correlated_clustering_to_integrate_multiple_datasets","abstraction":"Motivation: The integration of multiple datasets remains a key challenge in systems biology and genomic medicine. Modern high-throughput technologies generate a broad array of different data types, providing distinct—but often complementary—information. We present a Bayesian method for the unsupervised integrative modelling of multiple datasets, which we refer to as MDI (Multiple Dataset Integration). MDI can integrate information from a wide range of different datasets and data types simultaneously (including the ability to model time series data explicitly using Gaussian processes). Each dataset is modelled using a Dirichlet-multinomial allocation (DMA) mixture model, with dependencies between these models captured through parameters that describe the agreement among the datasets. Results: Using a set of six artificially constructed time series datasets, we show that MDI is able to integrate a significant number of datasets simultaneously, and that it successfully captures the underlying structural similarity between the datasets. We also analyse a variety of real Saccharomyces cerevisiae datasets. In the two-dataset case, we show that MDI’s performance is comparable with the present state-of-the-art. We then move beyond the capabilities of current approaches and integrate gene expression, chromatin immunoprecipitation–chip and protein–protein interaction data, to identify a set of protein complexes for which genes are co-regulated during the cell cycle. Comparisons to other unsupervised data integration techniques—as well as to non-integrative approaches—demonstrate that MDI is competitive, while also providing information that would be difficult or impossible to extract using other methods. Availability: A Matlab implementation of MDI is available from http://www2.warwick.ac.uk/fac/sci/systemsbiology/research/software/. Contact: D.L.Wild@warwick.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online."},{"id":246,"title":"Dirichlet process mixture models for unsupervised clustering of symptoms in Parkinson's disease","url":"https://www.researchgate.net/publication/254297949_Dirichlet_process_mixture_models_for_unsupervised_clustering_of_symptoms_in_Parkinson%27s_disease","abstraction":"In this paper, the goal of identifying disease subgroups based on differences in observed symptom profile is considered. Commonly referred to as phenotype identification, solutions to this task often involve the application of unsupervised clustering techniques. In this paper, we investigate the application of a Dirichlet process mixture model for this task. This model is defined by the placement of the Dirichlet process on the unknown components of a mixture model, allowing for the expression of uncertainty about the partitioning of observed data into homogeneous subgroups. To exemplify this approach, an application to phenotype identification in Parkinson's disease is considered, with symptom profiles collected using the Unified Parkinson's Disease Rating Scale."},{"id":247,"title":"Clustering in linear mixed models with Dirichlet process mixtures using EM algorithm","url":"https://www.researchgate.net/publication/268381633_Clustering_in_linear_mixed_models_with_Dirichlet_process_mixtures_using_EM_algorithm","abstraction":"In linear mixed models, the assumption of normally distributed random effects is often inappropriate and unnecessarily restrictive. The proposed approximate Dirichlet process mixture assumes a hierarchical Gaussian mixture that is based on the truncated version of the stick breaking presentation of the Dirichlet process. In addition to the weakening of distributional assumptions, the specification allows to identify clusters of observations with a similar random effects structure. An Expectation-Maximization algorithm is given that solves the estimation problem and that, in certain respects, may exhibit advantages over Markov chain Monte Carlo approaches when modelling with Dirichlet processes. The method is evaluated in a simulation study and applied to the dynamics of unemployment in Germany as well as lung function growth data."},{"id":248,"title":"Sampling from Dirichlet process mixture models with unknown concentration parameter: Mixing issues in large data implementations","url":"https://www.researchgate.net/publication/236118984_Sampling_from_Dirichlet_process_mixture_models_with_unknown_concentration_parameter_Mixing_issues_in_large_data_implementations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the question of Markov chain Monte Carlo sampling from a general\n <br> stick-breaking Dirichlet process mixture model, with concentration parameter\n <br> alpha. This paper introduces a Gibbs sampling algorithm that combines the slice\n <br> sampling approach of Walker (2007) and the retrospective sampling approach of\n <br> Papaspiliopoulos and Roberts (2008). Our general algorithm is implemented as\n <br> efficient open source C++ software, available as an R package, and is based on\n <br> a blocking strategy similar to that suggested by Papaspiliopoulos (2008) and\n <br> implemented by Yau et al (2011).\n <br> We discuss the difficulties of achieving good mixing in MCMC samplers of this\n <br> nature and investigate sensitivity to initialisation. We additionally consider\n <br> the challenges when an additional layer of hierarchy is added such that joint\n <br> inference is to be made on alpha. We introduce a new label switching move and\n <br> compute the marginal model posterior to help to surmount these difficulties.\n <br> Our work is illustrated using a profile regression (Molitor et al, 2010)\n <br> application, where we demonstrate good mixing behaviour for both synthetic and\n <br> real examples.\n</div> \n<p></p>"},{"id":249,"title":"Bayesian Consensus Clustering","url":"https://www.researchgate.net/publication/256291282_Bayesian_Consensus_Clustering","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In biomedical research a growing number of platforms and technologies are used to measure diverse but related information, and the task of clustering a set of objects based on multiple sources of data arises in several applications. Most current approaches to multi-source clustering either independently determine a separate clustering for each data source or determine a single \"joint\" clustering for all data sources. There is a need for more flexible approaches that simultaneously model the dependence and the heterogeneity of the data sources.\n <br> We propose an integrative statistical model that permits a separate clustering of the objects for each data source. These separate clusterings adhere loosely to an overall consensus clustering, and hence they are not independent. We describe a computationally scalable Bayesian framework for simultaneous estimation of both the consensus clustering and the source-specific clusterings. We demonstrate that this flexible approach is more robust than joint clustering of all data sources, and is more powerful than clustering each data source independently. We present an application to subtype identification of breast cancer tumor samples using publicly available data from The Cancer Genome Atlas.\n <br> R code with instructions and examples is available at http://people.duke.edu/%7Eel113/software.html.\n <br> Eric.Lock@duke.edu.\n</div> \n<p></p>"},{"id":250,"title":"A predictive view of Bayesian clustering","url":"https://www.researchgate.net/publication/222420950_A_predictive_view_of_Bayesian_clustering","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This work considers probability models for partitions of a set of n elements using a predictive approach, i.e., models that are specified in terms of the conditional probability of either joining an already existing cluster or forming a new one. The inherent structure can be motivated by resorting to hierarchical models of either parametric or nonparametric nature. Parametric examples include the product partition models (PPMs) and the model-based approach of Dasgupta and Raftery (J. Amer. Statist. Assoc. 93 (1998) 294), while nonparametric alternatives include the Dirichlet process, and more generally, the species sampling models (SSMs). Under exchangeability, PPMs and SSMs induce the same type of partition structure. The methods are discussed in the context of outlier detection in normal linear regression models and of (univariate) density estimation.\n</div> \n<p></p>"},{"id":251,"title":"Models beyond the Dirichlet process","url":"https://www.researchgate.net/publication/46451644_Models_beyond_the_Dirichlet_process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Bayesian nonparametric inference is a relatively young area of research and it has recently undergone a strong development. Most of its success can be explained by the considerable degree of exibility it ensures in statistical modelling, if compared to parametric alternatives, and by the emergence of new and ecient simulation techniques that make nonparametric models amenable to concrete use in a number of applied statistical problems. Since its introduction in 1973 by T.S. Ferguson, the Dirichlet process has emerged as a cornerstone in Bayesian nonparametrics. Nonetheless, in some cases of interest for statistical applications the Dirichlet process is not an adequate prior choice and alternative nonparametric models need to be devised. In this paper we provide a review of Bayesian nonparametric models that go beyond the Dirichlet process.\n</div> \n<p></p>"},{"id":252,"title":"Bayesian clustering and product partition models","url":"https://www.researchgate.net/publication/4993277_Bayesian_clustering_and_product_partition_models","abstraction":"We present a decision theoretic formulation of product partition models (PPMs) that allows a formal treatment of different decision problems such as estimation or hypothesis testing \"and\" clustering methods simultaneously. A key observation in our construction is the fact that PPMs can be formulated in the context of model selection. The underlying partition structure in these models is closely related to that arising in connection with Dirichlet processes. This allows a straightforward adaptation of some computational strategies-originally devised for nonparametric Bayesian problems-to our framework. The resulting algorithms are more flexible than other competing alternatives that are used for problems involving PPMs. We propose an algorithm that yields Bayes estimates of the quantities of interest and the groups of experimental units. We explore the application of our methods to the detection of outliers in normal and Student \"t\" regression models, with clustering structure equivalent to that induced by a Dirichlet process prior. We also discuss the sensitivity of the results considering different prior distributions for the partitions. Copyright 2003 Royal Statistical Society."},{"id":253,"title":"A Bayesian Analysis of Some Non-Parametric Problems","url":"https://www.researchgate.net/publication/38357634_A_Bayesian_Analysis_of_Some_Non-Parametric_Problems","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let $\\mathscr{X}$ be a space and $\\mathscr{A}$ a $\\sigma$-field of subsets, and let $\\alpha$ be a finite non-null measure on $(\\mathscr{X}, \\mathscr{A})$. Then a stochastic process $P$ indexed by elements $A$ of $\\mathscr{A}$, is said to be a Dirichlet process on $(\\mathscr{X}, \\mathscr{A})$ with parameter $\\alpha$ if for any measurable partition $(A_1, \\cdots, A_k)$ of $\\mathscr{X}$, the random vector $(P(A_1), \\cdots, P(A_k))$ has a Dirichlet distribution with parameter $(\\alpha(A_1), \\cdots, \\alpha(A_k)). P$ may be considered a random probability measure on $(\\mathscr{X}, \\mathscr{A})$, The main theorem states that if $P$ is a Dirichlet process on $(\\mathscr{X}, \\mathscr{A})$ with parameter $\\alpha$, and if $X_1, \\cdots, X_n$ is a sample from $P$, then the posterior distribution of $P$ given $X_1, \\cdots, X_n$ is also a Dirichlet process on $(\\mathscr{X}, \\mathscr{A})$ with a parameter $\\alpha + \\sum^n_1 \\delta_{x_i}$, where $\\delta_x$ denotes the measure giving mass one to the point $x$. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on $(\\mathscr{X}, \\mathscr{A})$. This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space $\\mathscr{X}$. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where $\\mathscr{X}$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general $\\mathscr{X}$. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that $P$ chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a $P$ chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis $H_0$ that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter $\\alpha$ itself a uniform measure on [0, 1], and if we are given a sample of size $n \\geqq 2$, the only nontrivial nonrandomized Bayes rule is to reject $H_0$ if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.\n</div> \n<p></p>"},{"id":254,"title":"The infinite HMM for unsupervised PoS tagging","url":"https://www.researchgate.net/publication/221012999_The_infinite_HMM_for_unsupervised_PoS_tagging","abstraction":"We extend previous work on fully unsu- pervised part-of-speech tagging. Using a non-parametric version of the HMM, called the infinite HMM (iHMM), we ad- dress the problem of choosing the number of hidden states in unsupervised Markov models for PoS tagging. We experi- ment with two non-parametric priors, the Dirichlet and Pitman-Yor processes, on the Wall Street Journal dataset using a paral- lelized implementation of an iHMM in- ference algorithm. We evaluate the re- sults with a variety of clustering evalua- tion metrics and achieve equivalent or bet- ter performances than previously reported. Building on this promising result we eval- uate the output of the unsupervised PoS tagger as a direct replacement for the out- put of a fully supervised PoS tagger for the task of shallow parsing and compare the two evaluations."},{"id":255,"title":"A fully Bayesian approach to unsupervised part-of-speech tagging.","url":"https://www.researchgate.net/publication/220874388_A_fully_Bayesian_approach_to_unsupervised_part-of-speech_tagging","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Unsupervised learning of linguistic structure is a difficult problem. A common approach is to define a generative model and max- imize the probability of the hidden struc- ture given the observed data. Typically, this is done using maximum-likelihood es- timation (MLE) of the model parameters. We show using part-of-speech tagging that a fully Bayesian approach can greatly im- prove performance. Rather than estimating a single set of parameters, the Bayesian ap- proach integrates over all possible parame- ter values. This difference ensures that the learned structure will have high probability over a range of possible parameters, and per- mits the use of priors favoring the sparse distributions that are typical of natural lan- guage. Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discrimi- native model (Smith and Eisner, 2005), up to 14 percentage points better than MLE. We find improvements both when training from data alone, and using a tagging dictionary.\n</div> \n<p></p>"},{"id":256,"title":"Comparing Clusterings – An Information Based Distance","url":"https://www.researchgate.net/publication/23644963_Comparing_Clusterings_-_An_Information_Based_Distance","abstraction":"This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering to clustering . The basic properties of VI are presented and discussed. We focus on two kinds of properties: (1) those that help one build intuition about the new criterion (in particular, it is shown the VI is a true metric on the space of clusterings), and (2) those that pertain to the comparability of VI values over different experimental conditions. As the latter properties have rarely been discussed explicitly before, other existing comparison criteria are also examined in their light. Finally we present the VI from an axiomatic point of view, showing that it is the only \"sensible\" criterion for comparing partitions that is both aligned to the lattice and convexely additive. As a consequence, we prove an impossibility result for comparing partitions: there is no criterion for comparing partitions that simultaneously satisfies the above two desirable properties and is bounded."},{"id":257,"title":"Hierarchical Dirichlet Processes","url":"https://www.researchgate.net/publication/4742259_Hierarchical_Dirichlet_Processes","abstraction":"Bayesian nonparametric inference is a relatively young area of research and it has recently undergone a strong development. Most of its success can be explained by the considerable degree of exibility it ensures in statistical modelling, if compared to parametric alternatives, and by the emergence of new and ecient simulation techniques that make nonparametric models amenable to concrete use in a number of applied statistical problems. Since its introduction in 1973 by T.S. Ferguson, the Dirichlet process has emerged as a cornerstone in Bayesian nonparametrics. Nonetheless, in some cases of interest for statistical applications the Dirichlet process is not an adequate prior choice and alternative nonparametric models need to be devised. In this paper we provide a review of Bayesian nonparametric models that go beyond the Dirichlet process."},{"id":258,"title":"Map-Reduce for Machine Learning on Multicore.","url":"https://www.researchgate.net/publication/221617998_Map-Reduce_for_Machine_Learning_on_Multicore","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly ap- plicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model (15) can be written in a certain \"summation form,\" which allows them to be easily par- allelized on multicore computers. We adapt Google's map-reduce (7) paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regres- sion (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.\n</div> \n<p></p>"},{"id":259,"title":"Factorized Asymptotic Bayesian Hidden Markov Models","url":"https://www.researchgate.net/publication/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper addresses the issue of model selection for hidden Markov models\n <br> (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has\n <br> been recently developed for model selection on independent hidden variables\n <br> (i.e., mixture models), for time-dependent hidden variables. As with FAB in\n <br> mixture models, FAB for HMMs is derived as an iterative lower bound\n <br> maximization algorithm of a factorized information criterion (FIC). It\n <br> inherits, from FAB for mixture models, several desirable properties for\n <br> learning HMMs, such as asymptotic consistency of FIC with marginal\n <br> log-likelihood, a shrinkage effect for hidden state selection, monotonic\n <br> increase of the lower FIC bound through the iterative optimization. Further, it\n <br> does not have a tunable hyper-parameter, and thus its model selection process\n <br> can be fully automated. Experimental results shows that FAB outperforms\n <br> states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in\n <br> terms of model selection accuracy and computational efficiency.\n</div> \n<p></p>"},{"id":260,"title":"Bayesian non-parametrics and the probabilistic approach to modelling","url":"https://www.researchgate.net/publication/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Modelling is fundamental to many fields of science and engineering. A model can be thought of as a representation of possible data one could predict from a system. The probabilistic approach to modelling uses probability theory to express all aspects of uncertainty in the model. The probabilistic approach is synonymous with Bayesian modelling, which simply uses the rules of probability theory in order to make predictions, compare alternative models, and learn model parameters and structure from data. This simple and elegant framework is most powerful when coupled with flexible probabilistic models. Flexibility is achieved through the use of Bayesian non-parametrics. This article provides an overview of probabilistic modelling and an accessible survey of some of the main tools in Bayesian non-parametrics. The survey covers the use of Bayesian non-parametrics for modelling unknown functions, density estimation, clustering, time-series modelling, and representing sparsity, hierarchies, and covariance structure. More specifically, it gives brief non-technical overviews of Gaussian processes, Dirichlet processes, infinite hidden Markov models, Indian buffet processes, Kingman's coalescent, Dirichlet diffusion trees and Wishart processes.\n</div> \n<p></p>"},{"id":261,"title":"Subsampling-Based Approximate Monte Carlo for Discrete Distributions","url":"https://www.researchgate.net/publication/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Drawing a sample from a discrete distribution is one of the building\n <br> components for Monte Carlo methods. Like other sampling algorithms, discrete\n <br> sampling also suffers from high computational burden in large-scale inference\n <br> problems. We study the problem of sampling a discrete random variable with a\n <br> high degree of dependency that is typical in large-scale Bayesian inference and\n <br> graphical models, and propose an efficient approximate solution with a\n <br> subsampling approach. We make a novel connection between the discrete sampling\n <br> and Multi-Armed Bandits problems with a finite reward population and provide\n <br> three algorithms with theoretical guarantees. Empirical evaluations show the\n <br> robustness and efficiency of the approximate algorithms in both synthetic and\n <br> real-world large-scale problems.\n</div> \n<p></p>"},{"id":262,"title":"Trust Region Newton Method for Logistic Regression.","url":"https://www.researchgate.net/publication/220320172_Trust_Region_Newton_Method_for_Logistic_Regression","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM).\n</div> \n<p></p>"},{"id":263,"title":"Riemann manifold Langevin and Hamiltonian Monte Carlo methods. J. R. Stat. Soc. B 73, 123-214","url":"https://www.researchgate.net/publication/227657870_Riemann_manifold_Langevin_and_Hamiltonian_Monte_Carlo_methods_J_R_Stat_Soc_B_73_123-214","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n ? The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis–Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from http://www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.\n</div> \n<p></p>"},{"id":264,"title":"Fast and Robust Fixed-Point Algorithms for Independent Component Analysis","url":"https://www.researchgate.net/publication/220279494_Fast_and_Robust_Fixed-Point_Algorithms_for_Independent_Component_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. In this paper, we use a combination of two different approaches for linear ICA: Comon's information-theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast (objective) functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions. These algorithms optimize the contrast functions very fast and reliably.\n</div> \n<p></p>"},{"id":265,"title":"A Stochastic Approximation Method","url":"https://www.researchgate.net/publication/236736791_A_Stochastic_Approximation_Method","abstraction":"Let $M(x)$ denote the expected value at level $x$ of the response to a certain experiment. $M(x)$ is assumed to be a monotone function of $x$ but is unknown to the experimenter, and it is desired to find the solution $x = \\theta$ of the equation $M(x) = \\alpha$, where $\\alpha$ is a given constant. We give a method for making successive experiments at levels $x_1,x_2,\\cdots$ in such a way that $x_n$ will tend to $\\theta$ in probability."},{"id":266,"title":"Monte Carlo Statistical Method","url":"https://www.researchgate.net/publication/41222435_Monte_Carlo_Statistical_Method","abstraction":"La simulation est devenue dans la dernière décennie un outil essentiel du traitement statistique de modèles complexes et de la mise en oeuvre de techniques statistiques avancées, comme le bootstrap ou les méthodes d'inférence simulée. Ce livre présente les éléments de base de la simulation de lois de probabilité (génération de variables uniformes et de lois usuelles) et de leur utilisation en Statistique (intégration de Monte Carlo, optimisation stochastique). Après un bref rappel sur les chaînes de Markov, les techniques plus spécifiques de Monte Carlo par chaînes de Markov (MCMC) sont présentées en détail, à la fois du point de vue théorique (validité et convergence) et du point de vue de leur implémentation (accélération, choix de paramètres, limitations). Les algorithmes d'échantillonnage de Gibbs sont ainsi distingués des méthodes générales de Hastings-Metropolis par leur plus grande richesse théorique. Les derniers chapitres contiennent un exposé critique sur l'état de l'art en contrôle de convergence de ces algorithmes et une présentation unifiée des diverses applications des méthodes MCMC aux modèles à données manquantes. De nombreux exemples statistiques illustrent les méthodes présentées dans cet ouvrage destiné aux étudiants de deuxième et troisième cycles universitaires en Mathématiques Appliquées ainsi qu'aux chercheurs et praticiens désirant utiliser les méthodes MCMC. Monte Carlo statistical methods, particularly those based on Markov chains, are now an essential component of the standard set of techniques used by statisticians. This new edition has been revised towards a coherent and flowing coverage of these simulation techniques, with incorporation of the most recent developments in the field. In particular, the introductory coverage of random variable generation has been totally revised, with many concepts being unified through a fundamental theorem of simulation There are five completely new chapters that cover Monte Carlo control, reversible jump, slice sampling, sequential Monte Carlo, and perfect sampling. There is a more in-depth coverage of Gibbs sampling, which is now contained in three consecutive chapters. The development of Gibbs sampling starts with slice sampling and its connection with the fundamental theorem of simulation, and builds up to two-stage Gibbs sampling and its theoretical properties. A third chapter covers the multi-stage Gibbs sampler and its variety of applications. Lastly, chapters from the previous edition have been revised towards easier access, with the examples getting more detailed coverage. This textbook is intended for a second year graduate course, but will also be useful to someone who either wants to apply simulation techniques for the resolution of practical problems or wishes to grasp the fundamental principles behind those methods. The authors do not assume familiarity with Monte Carlo techniques (such as random variable generation), with computer programming, or with any Markov chain theory (the necessary concepts are developed in Chapter 6). A solutions manual, which covers approximately 40% of the problems, is available for instructors who require the book for a course. oui"},{"id":267,"title":"Sequential MCMC for Bayesian model selection","url":"https://www.researchgate.net/publication/3807947_Sequential_MCMC_for_Bayesian_model_selection","abstraction":"In this paper, we address the problem of sequential Bayesian model selection. This problem does not usually admit any closed-form analytical solution. We propose here an original sequential simulation-based method to solve the associated Bayesian computational problems. This method combines sequential importance sampling, a resampling procedure and reversible jump MCMC (Markov chain Monte Carlo) moves. We describe a generic algorithm and then apply it to the problem of sequential Bayesian model order estimation of autoregressive (AR) time series observed in additive noise"},{"id":268,"title":"A New Learning Algorithm for Blind Signal Separation","url":"https://www.researchgate.net/publication/2611505_A_New_Learning_Algorithm_for_Blind_Signal_Separation","abstraction":"A new on-line learning algorithm which minimizes a statistical dependency among outputs is derived for blind separation of mixed signals. The dependency is measured by the average mutual information (MI) of the outputs. The source signals and the mixing matrix are unknown except for the number of the sources. The Gram-Charlier expansion instead of the Edgeworth expansion is used in evaluating the MI. The natural gradient approach is used to minimize the MI. A novel activation function is proposed for the on-line learning algorithm which has an equivariant property and is easily implemented on a neural network like model. The validity of the new learning algorithm is verified by computer simulations. 3 Lab. for Information Representation, FRP, RIKEN, Wako-shi, Saitama, JAPAN 1 INTRODUCTION The problem of blind signal separation arises in many areas such as speech recognition, data communication, sensor signal processing, and medical science. Several neural network algorithms [3, 5, ..."},{"id":269,"title":"Langevin Diffusions and Metropolis-Hastings Algorithms","url":"https://www.researchgate.net/publication/226460107_Langevin_Diffusions_and_Metropolis-Hastings_Algorithms","abstraction":"We consider a class of Langevin diffusions with state-dependent volatility. The volatility of the diffusion is chosen so as to make the stationary distribution of the diffusion with respect to its natural clock, a heated version of the stationary density of interest. The motivation behind this construction is the desire to construct uniformly ergodic diffusions with required stationary densities. Discrete time algorithms constructed by Hastings accept reject mechanisms are constructed from discretisations of the algorithms, and the properties of these algorithms are investigated."},{"id":270,"title":"Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization","url":"https://www.researchgate.net/publication/288713780_Bridging_the_Gap_between_Stochastic_Gradient_MCMC_and_Stochastic_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian\n <br> analogs to popular stochastic optimization methods; however, this connection is\n <br> not well studied. We explore this relationship by applying simulated annealing\n <br> to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two\n <br> key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)\n <br> adaptive element-wise momentum weights. The zero-temperature limit gives a\n <br> novel stochastic optimization method with adaptive element-wise momentum\n <br> weights, while conventional optimization methods only have a shared, static\n <br> momentum weight. Under certain assumptions, our theoretical analysis suggests\n <br> the proposed simulated annealing approach converges close to the global optima.\n <br> Experiments on several deep neural network models show state-of-the-art results\n <br> compared to related stochastic optimization algorithms.\n</div> \n<p></p>"},{"id":271,"title":"Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks","url":"https://www.researchgate.net/publication/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks","abstraction":"Effective training of deep neural networks suffers from two main issues. The first is that the parameter spaces of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk. We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models."},{"id":272,"title":"A General Method for Robust Bayesian Modeling","url":"https://www.researchgate.net/publication/283043210_A_General_Method_for_Robust_Bayesian_Modeling","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Robust Bayesian models are appealing alternatives to standard models,\n <br> providing protection from data that contains outliers or other departures from\n <br> the model assumptions. Historically, robust models were mostly developed on a\n <br> case-by-case basis; examples include robust linear regression, robust mixture\n <br> models, and bursty topic models. In this paper we develop a general approach to\n <br> robust Bayesian modeling. We show how to turn an existing Bayesian model into a\n <br> robust model, and then develop a generic strategy for computing with it. We use\n <br> our method to study robust variants of several models, including linear\n <br> regression, Poisson regression, logistic regression, and probabilistic topic\n <br> models. We discuss the connections between our methods and existing approaches,\n <br> especially empirical Bayes and James-Stein estimation.\n</div> \n<p></p>"},{"id":273,"title":"Scalable Bayes via Barycenter in Wasserstein Space","url":"https://www.researchgate.net/publication/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","abstraction":"We propose a novel approach WASP for Bayesian inference when massive size of the data prohibits posterior computations. WASP is estimated in three steps. First, data are divided into smaller computationally tractable subsets. Second, posterior draws of parameters are obtained for every subset after modifying subset posteriors using stochastic approximation. Finally, the empirical measures of samples from each subset posterior are combined through their barycenter in the Wasserstein space of probability measures. Stochastic approximation ensures that posterior uncertainty quantification of the barycenter matches with that of the full data posterior distribution. The combining step can be conducted efficiently through a sparse linear program, which takes negligible time relative to sampling from subset posteriors, facilitating scaling to massive data. WASP is very general and allows application of existing sampling algorithms to massive data with minimal modifications. We provide theoretical conditions under which rate of convergence of WASP to the delta measure centered at the true parameter coincides with the optimal parametric rate up to a logarithmic factor. WASP is applied for scalable Bayesian computations in a nonparametric mixture model and a movie recommender database containing tens of millions of ratings."},{"id":274,"title":"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process","url":"https://www.researchgate.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process","abstraction":"Stochastic variational inference (SVI) is emerging as the most promising candidate for scaling inference in Bayesian probabilistic models to large datasets. However, the performance of these methods has been assessed primarily in the context of Bayesian topic models, particularly latent Dirichlet allocation (LDA). Deriving several new algorithms, and using synthetic, image and genomic datasets, we investigate whether the understanding gleaned from LDA applies in the setting of sparse latent factor models, specifically beta process factor analysis (BPFA). We demonstrate that the big picture is consistent: using Gibbs sampling within SVI to maintain certain posterior dependencies is extremely effective. However, we find that different posterior dependencies are important in BPFA relative to LDA. Particularly, approximations able to model intra-local variable dependence perform best."},{"id":275,"title":"Hamiltonian Monte Carlo Acceleration Using Neural Network Surrogate functions","url":"https://www.researchgate.net/publication/278733571_Hamiltonian_Monte_Carlo_Acceleration_Using_Neural_Network_Surrogate_functions","abstraction":"Relatively high computational cost for Bayesian methods often limits their application for big data analysis. In recent years, there have been many attempts to improve computational efficiency of Bayesian inference. Here we propose an efficient and scalable computational technique for a state-of-the-art Markov Chain Monte Carlo (MCMC) methods, namely, Hamiltonian Monte Carlo (HMC). The key idea is to explore and exploit the regularity in parameter space for the underlying probabilistic model to construct an effective approximation of the collective geometric and statistical properties of the whole observed data. To this end, we use shallow neural networks along with efficient learning algorithms. The choice of basis functions (or hidden units in neural networks) and the optimized learning process provides a flexible, scalable and efficient sampling algorithm. Experiments based on simulated and real data show that our approach leads to substantially more efficient sampling algorithms compared to existing state-of-the art methods."},{"id":276,"title":"Learning Deep Generative Models with Doubly Stochastic MCMC","url":"https://www.researchgate.net/publication/278413644_Learning_Deep_Generative_Models_with_Doubly_Stochastic_MCMC","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present doubly stochastic gradient MCMC, a simple and generic method for\n <br> (approximate) Bayesian inference of deep generative models in the collapsed\n <br> continuous parameter space. At each MCMC sampling step, the algorithm randomly\n <br> draws a mini-batch of data samples to estimate the gradient of log-posterior\n <br> and further estimates the intractable expectation over latent variables via a\n <br> Gibbs sampler or a neural adaptive importance sampler. We demonstrate the\n <br> effectiveness on learning deep sigmoid belief networks (DSBNs). Compared to the\n <br> state-of-the-art methods using Gibbs sampling with data augmentation, our\n <br> algorithm is much more efficient and manages to learn DSBNs on large datasets.\n</div> \n<p></p>"},{"id":277,"title":"Fast Differentially Private Matrix Factorization","url":"https://www.researchgate.net/publication/275974197_Fast_Differentially_Private_Matrix_Factorization","abstraction":"Differentially private collaborative filtering is a challenging task, both in terms of accuracy and speed. We present a simple algorithm that is provably differentially private, while offering good performance, using a novel connection of differential privacy to Bayesian posterior sampling via Stochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm lends itself to efficient implementation. By careful systems design and by exploiting the power law behavior of the data to maximize CPU cache bandwidth we are able to generate 1024 dimensional models at a rate of 8.5 million recommendations per second on a single PC."},{"id":278,"title":"Hamiltonian ABC","url":"https://www.researchgate.net/publication/273327856_Hamiltonian_ABC","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Approximate Bayesian computation (ABC) is a powerful and elegant framework\n <br> for performing inference in simulation-based models. However, due to the\n <br> difficulty in scaling likelihood estimates, ABC remains useful for relatively\n <br> low-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of\n <br> likelihood-free algorithms that apply recent advances in scaling Bayesian\n <br> learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find\n <br> that a small number forward simulations can effectively approximate the ABC\n <br> gradient, allowing Hamiltonian dynamics to efficiently traverse parameter\n <br> spaces. We also describe a new simple yet general approach of incorporating\n <br> random seeds into the state of the Markov chain, further reducing the random\n <br> walk behavior of HABC. We demonstrate HABC on several typical ABC problems, and\n <br> show that HABC samples comparably to regular Bayesian inference using true\n <br> gradients on a high-dimensional problem from machine learning.\n</div> \n<p></p>"},{"id":279,"title":"Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC","url":"https://www.researchgate.net/publication/273157486_Large-Scale_Distributed_Bayesian_Matrix_Factorization_using_Stochastic_Gradient_MCMC","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Despite having various attractive qualities such as high prediction accuracy\n <br> and the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix\n <br> Factorization has not been widely adopted because of the prohibitive cost of\n <br> inference. In this paper, we propose a scalable distributed Bayesian matrix\n <br> factorization algorithm using stochastic gradient MCMC. Our algorithm, based on\n <br> Distributed Stochastic Gradient Langevin Dynamics, can not only match the\n <br> prediction accuracy of standard MCMC methods like Gibbs sampling, but at the\n <br> same time is as fast and simple as stochastic gradient descent. In our\n <br> experiments, we show that our algorithm can achieve the same level of\n <br> prediction accuracy as Gibbs sampling an order of magnitude faster. We also\n <br> show that our method reduces the prediction error as fast as distributed\n <br> stochastic gradient descent, achieving a 4.1% improvement in RMSE for the\n <br> Netflix dataset and an 1.8% for the Yahoo music dataset.\n</div> \n<p></p>"},{"id":280,"title":"A Noisy Monte Carlo Algorithm","url":"https://www.researchgate.net/publication/1993692_A_Noisy_Monte_Carlo_Algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a Monte Carlo algorithm to promote Kennedy and Kuti's linear accept/reject algorithm which accommodates unbiased stochastic estimates of the probability to an exact one. This is achieved by adopting the Metropolis accept/reject steps for both the dynamical and noise configurations. We test it on the five state model and obtain desirable results even for the case with large noise. We also discuss its application to lattice QCD with stochastically estimated fermion determinants. Comment: 10 pages, 1 table\n</div> \n<p></p>"},{"id":281,"title":"Equation of State by Fast Computing Machines","url":"https://www.researchgate.net/publication/200105001_Equation_of_State_by_Fast_Computing_Machines","abstraction":"A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion. The Journal of Chemical Physics is copyrighted by The American Institute of Physics."},{"id":282,"title":"Reversible Jum Markov Chain Monte Carlo Computation and Bayesian Model Determination","url":"https://www.researchgate.net/publication/2670104_Reversible_Jum_Markov_Chain_Monte_Carlo_Computation_and_Bayesian_Model_Determination","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint\n <br> distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not\n <br> been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically\n <br> not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between\n <br> parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide\n <br> applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis\n <br> in one and two dimensions, and to a Bayesian comparison of binomial experiments.\n</div> \n<p></p>"},{"id":283,"title":"Group Sequential Methods in the Design and Analysis of Clinical Trials","url":"https://www.researchgate.net/publication/30975436_Group_Sequential_Methods_in_the_Design_and_Analysis_of_Clinical_Trials","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n SUMMARY In clinical trials with sequential patient entry, fixed sample size designs are unjustified on ethical grounds and sequential\n <br> designs are often impracticable. One solution is a group sequential design dividing patient entry into a number of equal-sized\n <br> groups so that the decision to stop the trial or continue is based on repeated significance tests of the accumulated data\n <br> after each group is evaluated. Exact results are obtained for a trial with two treatments and a normal response with known\n <br> variance. The design problem of determining the required size and number of groups is also considered. Simulation shows that\n <br> these normal results may be adapted to other types of response data. An example shows that group sequential designs can sometimes\n <br> be statistically superior to standard sequential designs.\n</div> \n<p></p>"},{"id":284,"title":"Monte Carlo Sampling Methods Using Markov Chains and Their Application","url":"https://www.researchgate.net/publication/31403274_Monte_Carlo_Sampling_Methods_Using_Markov_Chains_and_Their_Application","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n SUMMARY A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties\n <br> of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices\n <br> and potential applications of the methods to numerical problems arising in statistics, are discussed.\n</div> \n<p></p>"},{"id":285,"title":"Approximately Optimal One-Parameter Boundaries for Group Sequential Trials","url":"https://www.researchgate.net/publication/19595229_Approximately_Optimal_One-Parameter_Boundaries_for_Group_Sequential_Trials","abstraction":"We present a class of group sequential tests, indexed by a single parameter, that yields approximately optimal results. We also provide tables of key values to help in the design of group sequential tests that meet selected specifications."},{"id":286,"title":"Approximations of Markov Chains and High-Dimensional Bayesian Inference","url":"https://www.researchgate.net/publication/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Markov Chain Monte Carlo method is the dominant paradigm for posterior\n <br> computation in Bayesian analysis. It has long been common to control\n <br> computation time by making approximations to the Markov transition kernel.\n <br> Comparatively little attention has been paid to convergence and estimation\n <br> error in these approximating Markov Chains. We propose a framework for\n <br> assessing when to use approximations in MCMC algorithms, and how much error in\n <br> the transition kernel should be tolerated to obtain optimal estimation\n <br> performance with respect to a specified loss function and computational budget.\n <br> The results require only ergodicity of the exact kernel and control of the\n <br> kernel approximation accuracy. The theoretical framework is applied to\n <br> approximations based on random subsets of data, low-rank approximations of\n <br> Gaussian processes, and a novel approximating Markov chain for discrete mixture\n <br> models.\n</div> \n<p></p>"},{"id":287,"title":"How Can Subsampling Reduce Complexity in Sequential MCMC Methods and Deal with Big Data in Target Tracking?","url":"https://www.researchgate.net/publication/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking","abstraction":"Target tracking faces the challenge in coping with large volumes of data which requires efficient methods for real time applications. The complexity considered in this paper is when there is a large number of measurements which are required to be processed at each time step. Sequential Markov chain Monte Carlo (MCMC) has been shown to be a promising approach to target tracking in complex environments, especially when dealing with clutter. However, a large number of measurements usually results in large processing requirements. This paper goes beyond the current state-of-the-art and presents a novel Sequential MCMC approach that can overcome this challenge through adaptively subsampling the set of measurements. Instead of using the whole large volume of available data, the proposed algorithm performs a trade off between the number of measurements to be used and the desired accuracy of the estimates to be obtained in the presence of clutter. We show results with large improvements in processing time, more than 40% with a negligible loss in tracking performance, compared with the solution without subsampling."},{"id":288,"title":"Scalable MCMC for Large Data Problems using Data Subsampling and the Difference Estimator","url":"https://www.researchgate.net/publication/280033955_Scalable_MCMC_for_Large_Data_Problems_using_Data_Subsampling_and_the_Difference_Estimator","abstraction":"We propose a generic Markov Chain Monte Carlo (MCMC) algorithm to speed up computations for datasets with many observations. A key feature of our approach is the use of the highly efficient difference estimator from the survey sampling literature to estimate the log-likelihood accurately using only a small fraction of the data. Our algorithm improves on the $O(n)$ complexity of regular MCMC by operating over local data clusters instead of the full sample when computing the likelihood. The likelihood estimate is used in a Pseudo-marginal framework to sample from a perturbed posterior which is within $O(m^{-1/2})$ of the true posterior, where $m$ is the subsample size. The method is applied to a logistic regression model to predict firm bankruptcy for a large data set. We document a significant speed up in comparison to the standard MCMC on the full dataset."},{"id":289,"title":"Accelerated dimension-independent adaptive Metropolis","url":"https://www.researchgate.net/publication/278733347_Accelerated_dimension-independent_adaptive_Metropolis","abstraction":"This work considers black-box Bayesian inference over high-dimensional parameter spaces. The well-known adaptive Metropolis (AM) algorithm of (Haario etal. 2001) is extended herein to scale asymptotically uniformly with respect to the underlying parameter dimension for Gaussian targets, by respecting the variance of the target. The resulting algorithm, referred to as the dimension-independent adaptive Metropolis (DIAM) algorithm, also shows improved performance with respect to adaptive Metropolis on non-Gaussian targets. This algorithm is further improved, and the possibility of probing high-dimensional targets is enabled, via GPU-accelerated numerical libraries and periodically synchronized concurrent chains (justified a posteriori). Asymptotically in dimension, this GPU implementation exhibits a factor of four improvement versus a competitive CPU-based Intel MKL parallel version alone. Strong scaling to concurrent chains is exhibited, through a combination of longer time per sample batch (weak scaling) and yet fewer necessary samples to convergence. The algorithm performance is illustrated on several Gaussian and non-Gaussian target examples, in which the dimension may be in excess of one thousand."},{"id":290,"title":"Exploiting Multi-Core Architectures for Reduced-Variance Estimation with Intractable Likelihoods","url":"https://www.researchgate.net/publication/264936676_Exploiting_Multi-Core_Architectures_for_Reduced-Variance_Estimation_with_Intractable_Likelihoods","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Many popular statistical models for complex phenomena are intractable, in the\n <br> sense that the likelihood function cannot easily be evaluated, even up to\n <br> proportionality. Bayesian estimation in this setting remains challenging, with\n <br> a lack of computational methodology to fully exploit modern processing\n <br> capabilities. In this paper we introduce novel control variates for intractable\n <br> likelihoods that can reduce the Monte Carlo variance of Bayesian estimators, in\n <br> some cases dramatically. We prove that these control variates are well-defined,\n <br> provide a positive variance reduction and derive optimal tuning parameters that\n <br> are targeted at optimising this variance reduction. Moreover, the methodology\n <br> is highly parallelisable and offers a route to exploit multi-core processing\n <br> architectures for Bayesian computation. Results presented on the Ising model,\n <br> exponential random graphs and nonlinear stochastic differential equations\n <br> support our theoretical findings.\n</div> \n<p></p>"},{"id":291,"title":"Light and Widely Applicable MCMC: Approximate Bayesian Inference for Large Datasets","url":"https://www.researchgate.net/publication/273640222_Light_and_Widely_Applicable_MCMC_Approximate_Bayesian_Inference_for_Large_Datasets","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Light and Widely Applicable (LWA-) MCMC is a novel approximation of the\n <br> Metropolis-Hastings kernel targeting a posterior distribution defined on a\n <br> large number of observations. Inspired by Approximate Bayesian Computation, we\n <br> design a Markov chain whose transition makes use of an unknown but fixed,\n <br> fraction of the available data, where the random choice of sub-sample is guided\n <br> by the fidelity of this sub-sample to the observed data, as measured by summary\n <br> (or sufficient) statistics. LWA-MCMC is a generic and flexible approach, as\n <br> illustrated by the diverse set of examples which we explore. In each case\n <br> LWA-MCMC yields excellent performance and in some cases a dramatic improvement\n <br> compared to existing methodology.\n</div> \n<p></p>"},{"id":292,"title":"Accelerating Metropolis-Hastings algorithms by Delayed Acceptance","url":"https://www.researchgate.net/publication/273157850_Accelerating_Metropolis-Hastings_algorithms_by_Delayed_Acceptance","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n MCMC algorithms such as Metropolis-Hastings algorithms are slowed down by the\n <br> computation of complex target distributions as exemplified by huge datasets. We\n <br> offer in this paper a useful generalisation of the Delayed Acceptance approach,\n <br> devised to reduce the computational costs of such algorithms by a simple and\n <br> universal divide-and-conquer strategy. The idea behind the generic acceleration\n <br> is to divide the acceptance step into several parts, aiming at a major\n <br> reduction in computing time that out-ranks the corresponding reduction in\n <br> acceptance probability. Each of the components can be sequentially compared\n <br> with a uniform variate, the first rejection signalling that the proposed value\n <br> is considered no further. We develop moreover theoretical bounds for the\n <br> variance of associated estimators with respect to the variance of the standard\n <br> Metropolis-Hastings and detail some results on optimal scaling and general\n <br> optimisation of the procedure. We illustrate those accelerating features on a\n <br> series of examples\n</div> \n<p></p>"},{"id":293,"title":"Expectation propagation as a way of life","url":"https://www.researchgate.net/publication/269722347_Expectation_propagation_as_a_way_of_life","abstraction":"We revisit expectation propagation (EP) as a prototype for scalable algorithms that partition big datasets into many parts and analyze each part in parallel to perform inference of shared parameters. The algorithm should be particularly efficient for hierarchical models, for which the EP algorithm works on the shared parameters (hyperparameters) of the model. The central idea of EP is to work at each step with a \"tilted distribution\" that combines the likelihood for a part of the data with the \"cavity distribution,\" which is the approximate model for the prior and all other parts of the data. EP iteratively approximates the moments of the tilted distributions and incorporates those approximations into a global posterior approximation. As such, EP can be used to divide the computation for large models into manageable sizes. The computation for each partition can be made parallel with occasional exchanging of information between processes through the global posterior approximation. Moments of multivariate tilted distributions can be approximated in various ways, including, MCMC, Laplace approximations, and importance sampling."},{"id":294,"title":"Bayesian Sampling Using Stochastic Gradient Thermostats","url":"https://www.researchgate.net/publication/269692923_Bayesian_Sampling_Using_Stochastic_Gradient_Thermostats","abstraction":"Dynamics-based sampling methods, such as Hybrid Monte Carlo (HMC) and Langevin dynamics (LD), are commonly used to sample target distributions. Re-cently, such approaches have been combined with stochastic gradient techniques to increase sampling efficiency when dealing with large datasets. An outstanding problem with this approach is that the stochastic gradient introduces an unknown amount of noise which can prevent proper sampling after discretization. To rem-edy this problem, we show that one can leverage a small number of additional variables to stabilize momentum fluctuations induced by the unknown noise. Our method is inspired by the idea of a thermostat in statistical physics and is justified by a general theory."},{"id":295,"title":"Probability Inequalities for Sums of Bounded Random Variables","url":"https://www.researchgate.net/publication/228057769_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables","abstraction":"Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for Pr {S – ES ? nt} depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population."},{"id":296,"title":"Concentration Inequalities for Sub-Additive Functions Using the Entropy Method","url":"https://www.researchgate.net/publication/41781172_Concentration_Inequalities_for_Sub-Additive_Functions_Using_the_Entropy_Method","abstraction":"We obtain exponential concentration inequalities for sub-additive functions of independent random variables under weak conditions on the increments of those functions, like the existence of exponential moments for these increments. As a consequence of these general inequalities, we obtain refinements of Talagrand's inequality for empirical processes and new bounds for randomized empirical processes. These results are obtained by further developing the entropy method introduced by Ledoux."},{"id":297,"title":"Centering Sequences with Bounded Differences","url":"https://www.researchgate.net/publication/220357686_Centering_Sequences_with_Bounded_Differences","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Inequalities for martingales with bounded differences have recently proved to be very useful in combinatorics and in the mathematics of operational research and computer science. We see here that these inequalities extend in a natural way to with bounded differences, and thus include, for example, better inequalities for sequences related to sampling without replacement.\n</div> \n<p></p>"},{"id":298,"title":"Empirical Bernstein Bounds and Sample Variance Penalization","url":"https://www.researchgate.net/publication/45863486_Empirical_Bernstein_Bounds_and_Sample_Variance_Penalization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We give improved constants for data dependent and variance sensitive confidence bounds, called empirical Bernstein bounds, and extend these inequalities to hold uniformly over classes of functionswhose growth function is polynomial in the sample size n. The bounds lead us to consider sample variance penalization, a novel learning method which takes into account the empirical variance of the loss function. We give conditions under which sample variance penalization is effective. In particular, we present a bound on the excess risk incurred by the method. Using this, we argue that there are situations in which the excess risk of our method is of order 1/n, while the excess risk of empirical risk minimization is of order 1/sqrt/{n}. We show some experimental results, which confirm the theory. Finally, we discuss the potential application of our results to sample compression schemes. Comment: 10 pages, 1 figure, Proc. Computational Learning Theory Conference (COLT 2009)\n</div> \n<p></p>"},{"id":299,"title":"A Generalization of Sampling Without Replacement From a Finite Universe","url":"https://www.researchgate.net/publication/232128642_A_Generalization_of_Sampling_Without_Replacement_From_a_Finite_Universe","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a general technique for the treatment of samples drawn without replacement from finite universes when unequal selection probabilities are used. Two sampling schemes are discussed in connection with the problem of determining optimum selection probabilities according to the information available in a supplementary variable. Admittedly, these two schemes have limited application. They should prove useful, however, for the first stage of sampling with multi-stage designs, since both permit unbiased estimation of the sampling variance without resorting to additional assumptions.* Journal Paper No. J2139 of the Iowa Agricultural Experiment Station, Ames, Iowa, Project 1005. Presented to the Institute of Mathematical Statistics, March 17, 1951.\n</div> \n<p></p>"},{"id":300,"title":"Concentration inequalities for functions of independent variables","url":"https://www.researchgate.net/publication/220345492_Concentration_inequalities_for_functions_of_independent_variables","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Following the entropy method this paper presents general concentration inequalities, which can be applied to combinatorial optimization and empirical processes. The inequalities give improved concentration results for optimal traveling salesmen tours, Steiner trees, and the eigenvalues of random symmetric matrices. © 2005 Wiley Periodicals, Inc. Random Struct. Alg., 2006\n</div> \n<p></p>"},{"id":301,"title":"On Concentration-of-Measure Inequalities","url":"https://www.researchgate.net/publication/244506454_On_Concentration-of-Measure_Inequalities","abstraction":"This text contains some of the material discussed during a series of seminars heldat the Pompeu Fabra University in the spring of 1997.Department of Economics, Pompeu Fabra University, Ramon Trias Fargas 25-27, 08005 Barcelona, Spain(email: lugosi@upf.es).1 Sums of independent random variablesFirst we recall some simple inequalities about sums of independent random variables. Herewe are primarily concerned with upper bounds for the probabilities of deviations from themean.First of..."},{"id":302,"title":"Exploration–exploitation tradeoff using variance estimates in multi-armed bandits","url":"https://www.researchgate.net/publication/220151284_Exploration-exploitation_tradeoff_using_variance_estimates_in_multi-armed_bandits","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations.\n</div> \n<p></p>"},{"id":303,"title":"Exponential bounds for the hypergeometric distribution","url":"https://www.researchgate.net/publication/280589952_Exponential_bounds_for_the_hypergeometric_distribution","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We establish exponential bounds for the hypergeometric distribution which\n <br> include a finite sampling correction factor, but are otherwise analogous to\n <br> bounds for the binomial distribution due to Le\\'on and Perron (2003) and\n <br> Talagrand (1994). We also establish a convex ordering for sampling without\n <br> replacement from populations of real numbers between zero and one: a population\n <br> of all zeros or ones (and hence yielding a hypergeometric distribution in the\n <br> upper bound) gives the extreme case.\n</div> \n<p></p>"},{"id":304,"title":"Validation of Network Reconciliation","url":"https://www.researchgate.net/publication/267759898_Validation_of_Network_Reconciliation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Network reconciliation is the problem of identifying nodes in separate\n <br> networks that represent the same entity, for example matching nodes across\n <br> social networks that correspond to the same user. We introduce a technique to\n <br> compute probably approximately correct (PAC) bounds on precision and recall for\n <br> network reconciliation algorithms. The bounds require some verified matches,\n <br> but those matches may be used to develop the algorithms. The bounds do not\n <br> require knowledge of the network generation process, and they can supply\n <br> confidence levels for individual matches.\n</div> \n<p></p>"},{"id":305,"title":"The convex distance inequality for dependent random variables, with applications to the stochastic travelling salesman and other problems","url":"https://www.researchgate.net/publication/264708143_The_convex_distance_inequality_for_dependent_random_variables_with_applications_to_the_stochastic_travelling_salesman_and_other_problems","abstraction":"We prove concentration inequalities for general functions of weakly dependent random variables satisfying the Dobrushin condition. In particular, we show Talagrand’s convex distance inequality for this type of dependence. We apply our bounds to a version of the stochastic salesman problem, the Steiner tree problem, the total magnetisation of the Curie-Weiss model with external field, and exponential random graph models. Our proof uses the exchangeable pair method for proving concentration inequalities introduced by Chatterjee (2005). Another key ingredient of the proof is a subclass of (a, b)-self-bounding functions, introduced by Boucheron, Lugosi and Massart (2009)."},{"id":306,"title":"Big Learning with Bayesian Methods","url":"https://www.researchgate.net/publication/268747974_Big_Learning_with_Bayesian_Methods","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Explosive growth in data and availability of cheap computing resources have\n <br> sparked increasing interest in Big learning, an emerging subfield that studies\n <br> scalable machine learning algorithms, systems, and applications with Big Data.\n <br> Bayesian methods represent one important class of statistic methods for machine\n <br> learning, with substantial recent developments on adaptive, flexible and\n <br> scalable Bayesian learning. This article provides a survey of the recent\n <br> advances in Big learning with Bayesian methods, termed Big Bayesian Learning,\n <br> including nonparametric Bayesian methods for adaptively inferring model\n <br> complexity, regularized Bayesian inference for improving the flexibility via\n <br> posterior regularization, and scalable algorithms and systems based on\n <br> stochastic subsampling and distributed computing for dealing with large-scale\n <br> applications.\n</div> \n<p></p>"},{"id":307,"title":"Distribution-Free Detection of Structured Anomalies: Permutation and Rank-Based Scans","url":"https://www.researchgate.net/publication/280970148_Distribution-Free_Detection_of_Structured_Anomalies_Permutation_and_Rank-Based_Scans","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The scan statistic is by far the most popular method for anomaly detection,\n <br> being popular in syndromic surveillance, signal and image processing, and\n <br> target detection based on sensor networks, among other applications. The use of\n <br> the scan statistics in such settings yields an hypotheses testing procedure,\n <br> where the null hypothesis corresponds to the absence of anomalous behavior. If\n <br> the null distribution is known, then calibration of a scan-based test is\n <br> relatively easy, as it can be done by Monte-Carlo simulation. When the null\n <br> distribution is unknown, it is not clear what the best way to proceed is. We\n <br> propose two procedures. One is a calibration by permutation and the other is a\n <br> rank-based scan test, which is distribution-free and less sensitive to\n <br> outliers. Furthermore, the rank-scan test requires only a one-time calibration\n <br> for a given data size making it computationally more appealing. In both cases,\n <br> we quantify the performance loss with respect to an oracle scan test that knows\n <br> the null distribution, and show one incurs only a very small loss in the\n <br> context of a natural exponential family. These results include the classical\n <br> normal location model, as well as Poisson model popular in syndromic\n <br> surveillance. We perform numerical experiments on simulated data further\n <br> supporting our theory, and also experiments with a real dataset from genomics.\n</div> \n<p></p>"},{"id":308,"title":"Markov Logic Networks","url":"https://www.researchgate.net/publication/215990907_Markov_Logic_Networks","abstraction":"We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a first-order knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach."},{"id":309,"title":"Factor graphs and the sum-product algorithm. IEEE Trans Inf Theory","url":"https://www.researchgate.net/publication/3080334_Factor_graphs_and_the_sum-product_algorithm_IEEE_Trans_Inf_Theory","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Algorithms that must deal with complicated global functions of\n <br> many variables often exploit the manner in which the given functions\n <br> factor as a product of “local” functions, each of which\n <br> depends on a subset of the variables. Such a factorization can be\n <br> visualized with a bipartite graph that we call a factor graph, In this\n <br> tutorial paper, we present a generic message-passing algorithm, the\n <br> sum-product algorithm, that operates in a factor graph. Following a\n <br> single, simple computational rule, the sum-product algorithm\n <br> computes-either exactly or approximately-various marginal functions\n <br> derived from the global function. A wide variety of algorithms developed\n <br> in artificial intelligence, signal processing, and digital\n <br> communications can be derived as specific instances of the sum-product\n <br> algorithm, including the forward/backward algorithm, the Viterbi\n <br> algorithm, the iterative “turbo” decoding algorithm, Pearl's\n <br> (1988) belief propagation algorithm for Bayesian networks, the Kalman\n <br> filter, and certain fast Fourier transform (FFT) algorithms\n</div> \n<p></p>"},{"id":310,"title":"Learning the structure of Markov logic networks","url":"https://www.researchgate.net/publication/221345687_Learning_the_structure_of_Markov_logic_networks","abstraction":"Markov logic networks (MLNs) combine logic and probability by attaching weights to rst-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational databases, combining ideas from inductive logic pro- gramming (ILP) and feature induction in Markov net- works. The algorithm performs a beam or shortest- rst search of the space of clauses, guided by a weighted pseudo-likelihood measure. This requires computing the optimal weights for each candidate structure, but we show how this can be done ef- ciently. The algorithm can be used to learn an MLN from scratch, or to rene an existing knowledge base. We have applied it in two real-world domains, and found that it outperforms using off-the-shelf ILP sys- tems to learn the MLN structure, as well as pure ILP, purely probabilistic and purely knowledge-based ap- proaches."},{"id":311,"title":"Sampling from large graphs","url":"https://www.researchgate.net/publication/200110781_Sampling_from_large_graphs","abstraction":"Given a huge real graph, how can we derive a representa- tive sample? There are many known algorithms to compute interesting measures (shortest paths, centrality, between- ness, etc.), but several of them become impractical for large graphs. Thus graph sampling is essential. The natural questions to ask are (a) which sampling method to use, (b) how small can the sample size be, and (c) how to scale up the measurements of the sample (e.g., the di- ameter), to get estimates for the large graph. The deeper, underlying question is subtle: how do we measure success? We answer the above questions, and test our answers by thorough experiments on several, diverse datasets, spanning thousands nodes and edges. We consider several sampling methods, propose novel methods to check the goodness of sampling, and develop a set of scaling laws that describe re- lations between the properties of the original and the sample. In addition to the theoretical contributions, the practical conclusions from our work are: Sampling strategies based on edge selection do not perform well; simple uniform random node selection performs surprisingly well. Overall, best per- forming methods are the ones based on random-walks and \"forest fire\"; they match very accurately both static as well as evolutionary graph patterns, with sample sizes down to about 15% of the original graph."},{"id":312,"title":"A General Method for Reducing the Complexity of Relational Inference And its Application to MCMC","url":"https://www.researchgate.net/publication/221603575_A_General_Method_for_Reducing_the_Complexity_of_Relational_Inference_And_its_Application_to_MCMC","abstraction":"Many real-world problems are characterized by complex rela- tional structure, which can be succinctly represented in fir st- order logic. However, many relational inference algorithms proceed by first fully instantiating the first-order theory a nd then working at the propositional level. The applicability of such approaches is severely limited by the exponential time and memory cost of propositionalization. Singla and Domin- gos (2006) addressed this by developing a \"lazy\" version of the WalkSAT algorithm, which grounds atoms and clauses only as needed. In this paper we generalize their ideas to a much broader class of algorithms, including other types of SAT solvers and probabilistic inference methods like MCMC. Lazy inference is potentially applicable whenever variables and functions have default values (i.e., a value that is much more frequent than the others). In relational domains, the default is false for atoms and true for clauses. We illustrat e our framework by applying it to MC-SAT, a state-of-the-art MCMC algorithm. Experiments on a number of real-world domains show that lazy inference reduces both space and time by several orders of magnitude, making probabilistic rela- tional inference applicable in previously infeasible domains."},{"id":313,"title":"Collective Cross-Document Relation Extraction Without Labelled Data.","url":"https://www.researchgate.net/publication/221012836_Collective_Cross-Document_Relation_Extraction_Without_Labelled_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present a novel approach to relation extraction that integrates information across documents, performs global inference and requires no labelled text. In particular, we tackle relation extraction and entity identification jointly. We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia). For inference we run an efficient Gibbs sampler that leads to linear time joint inference. We evaluate our approach both for an indomain (Wikipedia) and a more realistic out-of-domain (New York Times Corpus) setting. For the in-domain setting, our joint model leads to 4% higher precision than an isolated local approach, but has no advantage over a pipeline. For the out-of-domain data, we benefit strongly from joint modelling, and observe improvements in precision of 13% over the pipeline, and 15% over the isolated baseline.\n</div> \n<p></p>"},{"id":314,"title":"Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models.","url":"https://www.researchgate.net/publication/220874817_Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models","abstraction":"Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach."},{"id":315,"title":"A Machine Learning Approach to Building Domain-Specific Search Engines","url":"https://www.researchgate.net/publication/2454332_A_Machine_Learning_Approach_to_Building_Domain-Specific_Search_Engines","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Domain-specific search engines are becoming increasingly popular because they offer increased accuracy and extra features not possible with general, Web-wide search engines. Unfortunately, they are also difficult and timeconsuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, text classification and information extraction that enables efficient spidering, populates topic hierarchies, and identifies informative text segments. Using these techniques, we have built a demonstration system: a search engine for computer science research papers available at www.cora.justresearch.com. 1 Introduction As the amount of information on the World Wide Web grows, it becomes increasingly difficult to find just what wewant. While general-purpose search engines suchas AltaVista and HotBot offer high coverage, they often provi...\n</div> \n<p></p>"},{"id":316,"title":"Bi-directional Joint Inference for Entity Resolution and Segmentation Using Imperatively-Defined Factor Graphs","url":"https://www.researchgate.net/publication/220698781_Bi-directional_Joint_Inference_for_Entity_Resolution_and_Segmentation_Using_Imperatively-Defined_Factor_Graphs","abstraction":"There has been growing interest in using joint inference across multiple subtasks as a mechanism for avoiding the cascading accumulation of errors in traditional pipelines. Several recent papers demonstrate joint inference between the segmentation of entity mentions and their de-duplication, however, they have various weaknesses: inference information flows only in one direction, the number of uncertain hypotheses is severely limited, or the subtasks are only loosely coupled. This paper presents a highly-coupled, bi-directional approach to joint inference based on efficient Markov chain Monte Carlo sampling in a relational conditional random field. The model is specified with our new probabilistic programming language that leverages imperative constructs to define factor graph structure and operation. Experimental results show that our approach provides a dramatic reduction in error while also running faster than the previous state-of-the-art system."},{"id":317,"title":"Bayesian Learning in Undirected Graphical Models: Approximate MCMC algorithms","url":"https://www.researchgate.net/publication/229157569_Bayesian_Learning_in_Undirected_Graphical_Models_Approximate_MCMCalgorithms","abstraction":"Bayesian learning in undirected graphical models|computing posterior distributions over parameters and predictive quantities is exceptionally difficult. We conjecture that for general undirected models, there are no tractable MCMC (Markov Chain Monte Carlo) schemes giving the correct equilibrium distribution over parameters. While this intractability, due to the partition function, is familiar to those performing parameter optimisation, Bayesian learning of posterior distributions over undirected model parameters has been unexplored and poses novel challenges. we propose several approximate MCMC schemes and test on fully observed binary models (Boltzmann machines) for a small coronary heart disease data set and larger artificial systems. While approximations must perform well on the model, their interaction with the sampling scheme is also important. Samplers based on variational mean- field approximations generally performed poorly, more advanced methods using loopy propagation, brief sampling and stochastic dynamics lead to acceptable parameter posteriors. Finally, we demonstrate these techniques on a Markov random field with hidden variables."},{"id":318,"title":"Sublinear Approximate Inference for Probabilistic Programs","url":"https://www.researchgate.net/publication/267983152_Sublinear_Approximate_Inference_for_Probabilistic_Programs","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Probabilistic programming languages can simplify the development of machine\n <br> learning techniques, but only if inference is sufficiently scalable.\n <br> Unfortunately, Bayesian parameter estimation for highly coupled models such as\n <br> regressions and state-space models still scales badly. This paper describes a\n <br> sublinear-time algorithm for making Metropolis-Hastings updates to latent\n <br> variables in probabilistic programs. This approach generalizes recently\n <br> introduced approximate MH techniques: instead of subsampling data items assumed\n <br> to be independent, it subsamples edges in a dynamically constructed graphical\n <br> model. It thus applies to a broader class of problems and interoperates with\n <br> general-purpose inference techniques. Empirical results are presented for\n <br> Bayesian logistic regression, nonlinear classification via joint Dirichlet\n <br> process mixtures, and parameter estimation for stochastic volatility models\n <br> (with state estimation via particle MCMC). All three applications use the same\n <br> implementation, and each requires under 20 lines of probabilistic code.\n</div> \n<p></p>"},{"id":319,"title":"Assessing confidence of knowledge base content with an experimental study in entity resolution","url":"https://www.researchgate.net/publication/262370679_Assessing_confidence_of_knowledge_base_content_with_an_experimental_study_in_entity_resolution","abstraction":"The purpose of this paper is to begin a conversation about the importance and role of confidence estimation in knowledge bases (KBs). KBs are never perfectly accurate, yet without confidence reporting their users are likely to treat them as if they were, possibly with serious real-world consequences. We define a notion of confidence based on the probability of a KB fact being true. For automatically constructed KBs we propose several algorithms for estimating this confidence from pre-existing probabilistic models of data integration and KB construction. In particular, this paper focuses on confidence estimation in entity resolution. A goal of our exposition here is to encourage creators and curators of KBs to include confidence estimates for entities and relations in their KBs."},{"id":320,"title":"Perturbation theory for Markov chains via Wasserstein distance","url":"https://www.researchgate.net/publication/273640145_Perturbation_theory_for_Markov_chains_via_Wasserstein_distance","abstraction":"Perturbation theory for Markov chains addresses the question how small differences in the transitions of Markov chains are reflected in differences between their distributions. We prove powerful and flexible bounds on the distance of the $n$th step distributions of two Markov chains when one of them satisfies a Wasserstein ergodicity condition. Our work is motivated by the recent interest in approximate Markov chain Monte Carlo (MCMC) methods in the analysis of big data sets. By using an approach based on Lyapunov functions, we provide estimates for geometrically ergodic Markov chains under weak assumptions. In an autoregressive model, our bounds cannot be improved in general. We illustrate our theory by showing quantitative estimates for approximate versions of two prominent MCMC algorithms, the Metropolis-Hastings and stochastic Langevin algorithms."},{"id":321,"title":"Theory of Classification: A Survey of Some Recent Advances","url":"https://www.researchgate.net/publication/41781432_Theory_of_Classification_A_Survey_of_Some_Recent_Advances","abstraction":"The last few years have witnessed important new developments in the theory and practice of pattern classification. We intend to survey some of the main new ideas that have lead to these important recent developments."},{"id":322,"title":"On the complexity of loading shallow neural networks","url":"https://www.researchgate.net/publication/222625567_On_the_complexity_of_loading_shallow_neural_networks","abstraction":"We formalize a notion of loading information into connectionist networks that characterizes the training of feed-forward neural networks. This problem is NP-complete, so we look for tractable subcases of the problem by placing constraints on the network architecture. The focus of these constraints is on various families of “shallow” architectures which are defined to have bounded depth and un-bounded width. We introduce a perspective on shallow networks, called the Support Cone Interaction (SCI) graph, which is helpful in distinguishing tractable from intractable subcases: When the SCI graph is a tree or is of limited bandwidth, loading can be accomplished in polynomial time; when its bandwidth is not limited we find the problem NP-complete even if the SCI graph is a simple 2-dimensional planar grid."},{"id":323,"title":"Statistical Behavior and Consistency of Classification Methods based on Convex Risk Minimization","url":"https://www.researchgate.net/publication/2560520_Statistical_Behavior_and_Consistency_of_Classification_Methods_based_on_Convex_Risk_Minimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We study how close the optimal Bayes error rate can be approximately reached using a classification algorithm that computes a classifier by minimizing a convex upper bound of the classification error function. The measurement of closeness is characterized by the loss function used in the estimation. We show that such a classification scheme can be generally regarded as a (non maximum-likelihood) conditional in-class probability estimate, and we use this analysis to compare various convex loss functions that have appeared in the literature. Furthermore, the theoretical insight allows us to design good loss functions with desirable properties. Another aspect of our analysis is to demonstrate the consistency of certain classification methods using convex risk minimization.\n</div> \n<p></p>"},{"id":324,"title":"Measuring the VC-Dimension of a Learning Machine","url":"https://www.researchgate.net/publication/220500215_Measuring_the_VC-Dimension_of_a_Learning_Machine","abstraction":"A method for measuring the capacity of learning machines is described. The method is based onfitting a theoretically derived function to empirical measurements of the maximal difference between theerror rates on two separate data sets of varying sizes. Experimental measurements of the capacity ofvarious types of linear classifiers are presented.1 Introduction.Many theoretical and experimental studies have shown the influence of the capacity of a learning machine onits generalization..."},{"id":325,"title":"Optimal aggregation of classifiers in statistical learning, Annals of Statististics, 32(1)","url":"https://www.researchgate.net/publication/38349018_Optimal_aggregation_of_classifiers_in_statistical_learning_Annals_of_Statististics_321","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Classification can be considered as nonparametric estimation of sets, where the risk is defined by means of a specific distance between sets associated with misclassification error. It is shown that the rates of convergence of classifiers depend on two parameters: the complexity of the class of candidate sets and the margin parameter. The dependence is explicitly given, indicating that optimal fast rates approaching $O(n^{-1})$ can be attained, where n is the sample size, and that the proposed classifiers have the property of robustness to the margin. The main result of the paper concerns optimal aggregation of classifiers: we suggest a classifier that automatically adapts both to the complexity and to the margin, and attains the optimal fast rates, up to a logarithmic factor.\n</div> \n<p></p>"},{"id":326,"title":"Convexity, Classification, and Risk Bounds","url":"https://www.researchgate.net/publication/4742228_Convexity_Classification_and_Risk_Bounds","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Many of the classification algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. The convexity makes these algorithms computationally e#cient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function: that it satisfy a pointwise form of Fisher consistency for classification. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise.\n</div> \n<p></p>"},{"id":327,"title":"Correction to “The Importance of Convexity in Learning With Squared Loss” [Sep 98 1974-1980]","url":"https://www.researchgate.net/publication/3087002_Correction_to_The_Importance_of_Convexity_in_Learning_With_Squared_Loss_Sep_98_1974-1980","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The paper “The Importance of Convexity in Learning with Squared Loss” gave a lower bound on the sample complexity of learning with quadratic loss using a nonconvex function class. The proof contains an error. We show that the lower bound is true under a stronger condition that holds for many cases of interest.\n</div> \n<p></p>"},{"id":328,"title":"A Few Notes on Statistical Learning Theory","url":"https://www.researchgate.net/publication/2852039_A_Few_Notes_on_Statistical_Learning_Theory","abstraction":"this article is on the theoretical side and not on the applicative one; hence, we shall not present examples which may be interesting from the practical point of view but have little theoretical significance. This survey is far from being complete and it focuses on problems the author finds interesting (an opinion which is not necessarily shared by the majority of the learning community). Relevant books which present a more evenly balanced approach are, for example [1, 4, 35, 36] The starting point of our discussion is the formulation of the learning problem. Consider a class G, consisting of real valued functions defined on a space #, and assume that each g G maps # into [0, 1]. Let T be an unknown function, T : # [0, 1] and set to be an unknown probability measure on #"},{"id":329,"title":"QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines.","url":"https://www.researchgate.net/publication/220320717_QP_Algorithms_with_Guaranteed_Accuracy_and_Run_Time_for_Support_Vector_Machines","abstraction":"We describe polynomial-time algorithms that produce approximate solutions with guaranteed ac- curacy for a class of QP problems that are used in the design of support vector machine classifiers. These algorithms employ a two-stage process where the first s tage produces an approximate so- lution to a dual QP problem and the second stage maps this approximate dual solution to an ap- proximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an approximate dual solution with accuracy (2 ? 2Kn + 8 ? ?)?2??2 p to an approximate primal solution with accuracy ?p where n is the number of data samples, Kn is the maximum kernel value over the data and ? &gt; 0 is the SVM regularization parameter. For the first stage we p resent new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accu- racy and run time. In particular, for ?-rate certifying decomposition algorithms we establish the optimality of ? = 1/(n ? 1). In addition we extend the recent ? = 1/(n ? 1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the ? = 1/(n? 1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the ?-rate certifying property of these algorithms to produce new stopping rules that are computationally efficient and that guarantee a specified accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classification probl em we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2) bound on the overall run time for the first stage. Combining the first and second stages gives an overall run tim e of O(n2(ck + 1)) where ck is an upper bound on the computation to perform a kernel evaluation. Pseudocode is presented for a complete algorithm that inputs an accuracy ?p and produces an approximate solution that satisfies this accuracy in low order polynomial time. Experiments are included to illustrate the new stopping rules and to compare the Simon and composite decomposition algorithms."},{"id":330,"title":"Theory of Classification: A Survey of Some Recent Advances","url":"https://www.researchgate.net/publication/41712113_Theory_of_Classification_A_Survey_of_Some_Recent_Advances","abstraction":"The last few years have witnessed important new developments in the theory and practice of pattern classification. We intend to survey some of the main new ideas that have led to these recent results."},{"id":331,"title":"Grammars for Games: A Gradient-Based, Game-Theoretic Framework for Optimization in Deep Learning","url":"https://www.researchgate.net/publication/291186593_Grammars_for_Games_A_Gradient-Based_Game-Theoretic_Framework_for_Optimization_in_Deep_Learning","abstraction":"Deep learning is currently the subject of intensive study. However, fundamental concepts such as representations are not formally defined -- researchers \"know them when they see them\" -- and there is no common language for describing and analyzing algorithms. This essay proposes an abstract framework that identifies the essential features of current practice and may provide a foundation for future developments. The backbone of almost all deep learning algorithms is backpropagation, which is simply a gradient computation distributed over a neural network. The main ingredients of the framework are thus, unsurprisingly: (i) game theory, to formalize distributed optimization; and (ii) communication protocols, to track the flow of zeroth and first-order information. The framework allows natural definitions of semantics (as the meaning encoded in functions), representations (as functions whose semantics is chosen to optimized a criterion) and grammars (as communication protocols equipped with first-order convergence guarantees). Much of the essay is spent discussing examples taken from the literature. The ultimate aim is to develop a graphical language for describing the structure of deep learning algorithms that backgrounds the details of the optimization procedure and foregrounds how the components interact. Inspiration is taken from probabilistic graphical models and factor graphs, which capture the essential structural features of multivariate distributions."},{"id":332,"title":"Theoretical and Empirical Analysis of a Parallel Boosting Algorithm","url":"https://www.researchgate.net/publication/280873163_Theoretical_and_Empirical_Analysis_of_a_Parallel_Boosting_Algorithm","abstraction":"Many real-world problems involve massive amounts of data. Under these circumstances learning algorithms often become prohibitively expensive, making scalability a pressing issue to be addressed. A common approach is to perform sampling to reduce the size of the dataset and enable efficient learning. Alternatively, one customizes learning algorithms to achieve scalability. In either case, the key challenge is to obtain algorithmic efficiency without compromising the quality of the results. In this paper we discuss a meta-learning algorithm (PSBML) which combines features of parallel algorithms with concepts from ensemble and boosting methodologies to achieve the desired scalability property. We present both theoretical and empirical analyses which show that PSBML preserves a critical property of boosting, specifically, convergence to a distribution centered around the margin. We then present additional empirical analyses showing that this meta-level algorithm provides a general and effective framework that can be used in combination with a variety of learning classifiers. We perform extensive experiments to investigate the tradeoff achieved between scalability and accuracy, and robustness to noise, on both synthetic and real-world data. These empirical results corroborate our theoretical analysis, and demonstrate the potential of PSBML in achieving scalability without sacrificing accuracy."},{"id":333,"title":"Editorial: Big Data for Health","url":"https://www.researchgate.net/publication/280498476_Editorial_Big_Data_for_Health","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Presents an introductory editorial on the special issue for this issue of the publication which examines the fields of biomedical and health informatics.\n</div> \n<p></p>"},{"id":334,"title":"Designing Statistical Estimators That Balance Sample Size, Risk, and Computational Cost","url":"https://www.researchgate.net/publication/272385688_Designing_Statistical_Estimators_That_Balance_Sample_Size_Risk_and_Computational_Cost","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper proposes a tradeoff between computational time, sample complexity, and statistical accuracy that applies to statistical estimators based on convex optimization. When we have a large amount of data, we can exploit excess samples to decrease statistical risk, to decrease computational cost, or to trade off between the two. We propose to achieve this tradeoff by varying the amount of smoothing applied to the optimization problem. This work uses regularized linear regression as a case study to argue for the existence of this tradeoff both theoretically and experimentally. We also apply our method to describe a tradeoff in an image interpolation problem.\n</div> \n<p></p>"},{"id":335,"title":"Asynchronous Parallel Stochastic Gradient Descent - A Numeric Core for Scalable Distributed Machine Learning Algorithms","url":"https://www.researchgate.net/publication/277023572_Asynchronous_Parallel_Stochastic_Gradient_Descent_-_A_Numeric_Core_for_Scalable_Distributed_Machine_Learning_Algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The implementation of a vast majority of machine learning (ML) algorithms\n <br> boils down to solving a numerical optimization problem. In this context,\n <br> Stochastic Gradient Descent (SGD) methods have long proven to provide good\n <br> results, both in terms of convergence and accuracy. Recently, several\n <br> parallelization approaches have been proposed in order to scale SGD to solve\n <br> very large ML problems. At their core, most of these approaches are following a\n <br> map-reduce scheme. This paper presents a novel parallel updating algorithm for\n <br> SGD, which utilizes the asynchronous single-sided communication paradigm.\n <br> Compared to existing methods, ASGD provides faster (or at least equal)\n <br> convergence, close to linear scaling and stable accuracy.\n</div> \n<p></p>"},{"id":336,"title":"Particle Swarm Optimization based dictionary learning for remote sensing big data","url":"https://www.researchgate.net/publication/273918384_Particle_Swarm_Optimization_based_dictionary_learning_for_remote_sensing_big_data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Dictionary learning, which is based on sparse coding, has been frequently applied to many tasks related to remote sensing processes. Recently, many new non-analytic dictionary-learning algorithms have been proposed. Some are based on online learning. In online learning, data can be sequentially incorporated into the computation process. Therefore, these algorithms can train dictionaries using large-scale remote sensing images. However, their accuracy is decreased for two reasons. On one hand, it is a strategy of updating all atoms at once; on the other, the direction of optimization, such as the gradient, is not well estimated because of the complexity of the data and the model. In this paper, we propose a method of improved online dictionary learning based on Particle Swarm Optimization (PSO). In our iterations, we reasonably selected special atoms within the dictionary and then introduced the PSO into the atom-updating stage of the dictionary-learning model. Furthermore, to guide the direction of the optimization, the prior reference data were introduced into the PSO model. As a result, the movement dimension of the particles is reasonably limited and the accuracy and effectiveness of the dictionary are promoted, but without heavy computational burdens. Experiments confirm that our proposed algorithm improves the performance of the algorithm for large-scale remote sensing images, and our method also has a better effect on noise suppression.\n</div> \n<p></p>"},{"id":337,"title":"Multi-Block ADMM for Big Data Optimization in Modern Communication Networks","url":"https://www.researchgate.net/publication/274730183_Multi-Block_ADMM_for_Big_Data_Optimization_in_Modern_Communication_Networks","abstraction":"In this paper, we review the parallel and distributed optimization algorithms based on the alternating direction method of multipliers (ADMM) for solving \"big data\" optimization problems in modern communication networks. We first introduce the canonical formulation of the large-scale optimization problem. Next, we describe the general form of ADMM and then focus on several direct extensions and sophisticated modifications of ADMM from $2$-block to $N$-block settings to deal with the optimization problem. The iterative schemes and convergence properties of each extension/modification are given, and the implementation on large-scale computing facilities is also illustrated. Finally, we numerate several applications in communication networks, such as the security constrained optimal power flow problem in smart grid networks and mobile data offloading problem in software defined networks (SDNs)."},{"id":338,"title":"Multi-Block ADMM for Big Data Optimization in Smart Grid","url":"https://www.researchgate.net/publication/273067814_Multi-Block_ADMM_for_Big_Data_Optimization_in_Smart_Grid","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we review the parallel and distributed optimization algorithms\n <br> based on alternating direction method of multipliers (ADMM) for solving \"big\n <br> data\" optimization problem in smart grid communication networks. We first\n <br> introduce the canonical formulation of the large-scale optimization problem.\n <br> Next, we describe the general form of ADMM and then focus on several direct\n <br> extensions and sophisticated modifications of ADMM from $2$-block to $N$-block\n <br> settings to deal with the optimization problem. The iterative schemes and\n <br> convergence properties of each extension/modification are given, and the\n <br> implementation on large-scale computing facilities is also illustrated.\n <br> Finally, we numerate several applications in power system for distributed\n <br> robust state estimation, network energy management and security constrained\n <br> optimal power flow problem.\n</div> \n<p></p>"},{"id":339,"title":"Adaptive Rejection Metropolis Sampling Within Gibbs Sampling","url":"https://www.researchgate.net/publication/224910614_Adaptive_Rejection_Metropolis_Sampling_Within_Gibbs_Sampling","abstraction":"Gibbs sampling is a powerful technique for statistical inference. It involves little more than sampling from full conditional distributions, which can be both complex and computationally expensive to evaluate. Gilks and Wild have shown that in practice full conditionals are often log-concave, and they proposed a method of adaptive rejection sampling for efficiently sampling from univariate log-concave distributions. In this paper, to deal with non-log-concave full conditional distributions, we generalize adaptive rejection sampling to include a Hastings-Metropolis algorithm step. One important field of application in which statistical models may lead to non-log-concave full conditionals is population pharmacokinetics. Here, the relationship between drug dose and blood or plasma concentration in a group of patients typically is modelled by using non-linear mixed effects models. Often, the data used for analysis are routinely collected hospital measurements, which tend to be noisy and irregular. Consequently, a robust (t-distributed) error structure is appropriate to account for outlying observations and/or patients. We propose a robust non-linear full probability model for population pharmacokinetic data. We demonstrate that our method enables Bayesian inference for this model. through an analysis of antibiotic administration in new-born babies."},{"id":340,"title":"Hierarchical Mixture Modeling With Normalized Inverse-Gaussian Priors","url":"https://www.researchgate.net/publication/4741797_Hierarchical_Mixture_Modeling_With_Normalized_Inverse-Gaussian_Priors","abstraction":"In recent years the Dirichlet process prior has experienced a great success in the context of Bayesian mixture modeling. The idea of overcoming discreteness of its realizations by exploiting it in hierarchical models, combined with the development of suitable sampling techniques, represent one of the reasons of its popularity. In this article we propose the normalized inverse-Gaussian (N—IG) process as an alternative to the Dirichlet process to be used in Bayesian hierarchical models. The N—IG prior is constructed via its finite-dimensional distributions. This prior, although sharing the discreteness property of the Dirichlet prior, is characterized by a more elaborate and sensible clustering which makes use of all the information contained in the data. Whereas in the Dirichlet case the mass assigned to each observation depends solely on the number of times that it occurred, for the N—IG prior the weight of a single observation depends heavily on the whole number of ties in the sample. Moreover, expressions corresponding to relevant statistical quantities, such as a priori moments and the predictive distributions, are as tractable as those arising from the Dirichlet process. This implies that well-established sampling schemes can be easily extended to cover hierarchical models based on the N—IG process. The mixture of N—IG process and the mixture of Dirichlet process are compared using two examples involving mixtures of normals."},{"id":341,"title":"Computational and Inferential Difficulties With Mixture Posterior Distributions","url":"https://www.researchgate.net/publication/2763514_Computational_and_Inferential_Difficulties_With_Mixture_Posterior_Distributions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper deals with both exploration and interpretation problems related to posterior distributions for mixture models. The specification of mixture posterior distributions means that the presence of k! modes is known immediately. Standard Markov chain Monte Carlo techniques usually have difficulties with well-separated modes such as occur here; the Markov chain Monte Carlo sampler stays within a neighbourhood of a local mode and fails to visit other equally important modes. We show that exploration of these modes can be imposed on the Markov chain Monte Carlo sampler using tempered transitions based on Langevin algorithms. However, as the prior distribution does not distinguish between the different components, the posterior mixture distribution is symmetric and thus standard estimators such as posterior means cannot be used. Since this is also true for most non-symmetric priors, we propose alternatives for Bayesian inference for permutation invariant posteriors, including a cluster...\n</div> \n<p></p>"},{"id":342,"title":"Markov Chain Monte Carlo in Approximate Dirichlet and Beta Two-Parameter Process Hierarchical Models","url":"https://www.researchgate.net/publication/31123926_Markov_Chain_Monte_Carlo_in_Approximate_Dirichlet_and_Beta_Two-Parameter_Process_Hierarchical_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present some easy-to-construct random probability measures which approximate the Dirichlet process and an extension which we will call the beta two-parameter process. The nature of these constructions makes it simple to implement Markov chain Monte Carlo algorithms for fitting nonparametric hierarchical models. For the Dirichlet process, we consider a truncation approximation as well as a weak limit approximation based on a mixture of Dirichlet processes. The same type of truncation approximation can also be applied to the beta two-parameter process. Both methods lead to posteriors which can be fitted using Markov chain Monte Carlo algorithms that take advantage of blocked coordinate updates. These algorithms promote rapid mixing of the Markov chain and can be readily applied to normal mean mixture models and to density estimation problems. We prefer the truncation approximations, since a simple device for monitoring the adequacy of the approximation can be easily computed from the output of the Gibbs sampler. Furthermore, for the Dirichlet process, the truncation approximation offers an exponentially higher degree of accuracy over the weak limit approximation for the same computational effort. We also find that a certain beta two-parameter process may be suitable for finite mixture modelling because the distinct number of sampled values from this process tends to match closely the number of components of the underlying mixture distribution.\n</div> \n<p></p>"},{"id":343,"title":"Controlling the reinforcement in Bayesian non-parametric mixture models","url":"https://www.researchgate.net/publication/4914296_Controlling_the_reinforcement_in_Bayesian_non-parametric_mixture_models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The paper deals with the problem of determining the number of components in a mixture model. We take a Bayesian non-parametric approach and adopt a hierarchical model with a suitable non-parametric prior for the latent structure. A commonly used model for such a problem is the mixture of Dirichlet process model. Here, we replace the Dirichlet process with a more general non-parametric prior obtained from a generalized gamma process. The basic feature of this model is that it yields a partition structure for the latent variables which is of Gibbs type. This relates to the well-known (exchangeable) product partition models. If compared with the usual mixture of Dirichlet process model the advantage of the generalization that we are examining relies on the availability of an additional parameter \"?\" belonging to the interval (0,1): it is shown that such a parameter greatly influences the clustering behaviour of the model. A value of \"?\" that is close to 1 generates a large number of clusters, most of which are of small size. Then, a reinforcement mechanism which is driven by \"?\" acts on the mass allocation by penalizing clusters of small size and favouring those few groups containing a large number of elements. These features turn out to be very useful in the context of mixture modelling. Since it is difficult to specify \"a priori\" the reinforcement rate, it is reasonable to specify a prior for \"?\". Hence, the strength of the reinforcement mechanism is controlled by the data. Copyright 2007 Royal Statistical Society.\n</div> \n<p></p>"},{"id":344,"title":"Bayesian Nonparametric Density Estimation under Length Bias","url":"https://www.researchgate.net/publication/283117669_Bayesian_Nonparametric_Density_Estimation_under_Length_Bias","abstraction":"A density estimation method in a Bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest, but from a length biased version. From a Bayesian perspective, efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant. In this paper we present a novel Bayesian nonparametric approach to the length bias sampling problem which circumvents the issue of the normalizing constant. Numerical illustrations as well as a real data example are presented and the estimator is compared against its frequentist counterpart, the kernel density estimator for indirect data of Jones (1991)."},{"id":345,"title":"A Nonparametric Model for Stationary Time Series","url":"https://www.researchgate.net/publication/281059674_A_Nonparametric_Model_for_Stationary_Time_Series","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Stationary processes are a natural choice as statistical models for time series data, owing to their good estimating properties. In practice, however, alternative models are often proposed that sacrifice stationarity in favour of the greater modelling flexibility required by many real-life applications. We present a family of time-homogeneous Markov processes with nonparametric stationary densities, which retain the desirable statistical properties for inference, while achieving substantial modelling flexibility, matching those achievable with certain non-stationary models. A latent extension of the model enables exact inference through a trans-dimensional Markov chain Monte Carlo method. Numerical illustrations are presented.\n</div> \n<p></p>"},{"id":346,"title":"A Bayesian non-parametric approach to asymmetric dynamic conditional correlation model with application to portfolio selection","url":"https://www.researchgate.net/publication/285903238_A_Bayesian_non-parametric_approach_to_asymmetric_dynamic_conditional_correlation_model_with_application_to_portfolio_selection","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A Bayesian non-parametric approach for modeling the distribution of multiple returns is proposed. More specifically, an asymmetric dynamic conditional correlation (ADCC) model is considered to estimate the time-varying correlations of financial returns where the individual volatilities are driven by GJR-GARCH models. This composite model takes into consideration the asymmetries in individual assets’ volatilities, as well as in the correlations. The errors are modeled using a Dirichlet location–scale mixture of multivariate Normals allowing for a flexible return distribution in terms of skewness and kurtosis. This gives rise to a Bayesian non-parametric ADCC (BNP-ADCC) model, as opposed to a symmetric specification, called BNP-DCC. Then these two models are compared using a sample of Apple Inc. and NASDAQ Industrial index daily returns. The obtained results reveal that for this particular data set the BNP-ADCC outperforms the BNP-DCC model. Finally, an illustrative asset allocation exercise is presented.\n</div> \n<p></p>"},{"id":347,"title":"Bayesian Semiparametric Modeling of Realized Covariance Matrices *","url":"https://www.researchgate.net/publication/268521460_Bayesian_Semiparametric_Modeling_of_Realized_Covariance_Matrices","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper introduces several new Bayesian nonparametric models suitable for cap-turing the unknown conditional distribution of realized covariance (RCOV) matrices. Existing dynamic Wishart models are extended to countably infinite mixture mod-els of Wishart and inverse-Wishart distributions. In addition to mixture models with constant weights we propose models with time-varying weights to capture time depen-dence in the unknown distribution. Each of our models can be combined with returns to provide a coherent joint model of returns and RCOV. The extensive forecast results show the new models provide very significant improvements in density forecasts for RCOV and returns and competitive point forecasts of RCOV.\n</div> \n<p></p>"},{"id":348,"title":"Multiscale Bernstein polynomials for densities","url":"https://www.researchgate.net/publication/266477603_Multiscale_Bernstein_polynomials_for_densities","abstraction":"Our focus is on constructing a multiscale nonparametric prior for densities. The Bayes density estimation literature is dominated by single scale methods, with the exception of Polya trees, which favor overly-spiky densities even when the truth is smooth. We propose a multiscale Bernstein polynomial family of priors, which produce smooth realizations that do not rely on hard partitioning of the support. At each level in an infinitely-deep binary tree, we place a beta dictionary density; within a scale the densities are equivalent to Bernstein polynomials. Using a stick-breaking characterization, stochastically decreasing weights are allocated to the finer scale dictionary elements. A slice sampler is used for posterior computation, and properties are described. The method characterizes densities with locally-varying smoothness, and can produce a sequence of coarse to fine density estimates. An extension for Bayesian testing of group differences is introduced and applied to DNA methylation array data."},{"id":349,"title":"Bayesian Nonparametric Crowdsourcing","url":"https://www.researchgate.net/publication/264085032_Bayesian_Nonparametric_Crowdsourcing","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Crowdsourcing has been proven to be an effective and efficient tool to\n <br> annotate large datasets. User annotations are often noisy, so methods to\n <br> combine the annotations to produce reliable estimates of the ground truth are\n <br> necessary. We claim that considering the existence of clusters of users in this\n <br> combination step can improve the performance. This is especially important in\n <br> early stages of crowdsourcing implementations, where the number of annotations\n <br> is low. At this stage there is not enough information to accurately estimate\n <br> the bias introduced by each annotator separately, so we have to resort to\n <br> models that consider the statistical links among them. In addition, finding\n <br> these clusters is interesting in itself as knowing the behavior of the pool of\n <br> annotators allows implementing efficient active learning strategies. Based on\n <br> this, we propose in this paper two new fully unsupervised models based on a\n <br> Chinese Restaurant Process (CRP) prior and a hierarchical structure that allows\n <br> inferring these groups jointly with the ground truth and the properties of the\n <br> users. Efficient inference algorithms based on Gibbs sampling with auxiliary\n <br> variables are proposed. Finally, we perform experiments, both on synthetic and\n <br> real databases, to show the advantages of our models over state-of-the-art\n <br> algorithms.\n</div> \n<p></p>"},{"id":350,"title":"A Bayesian Nonparametric Regression Model With Normalized Weights: A Study of Hippocampal Atrophy in Alzheimer's Disease","url":"https://www.researchgate.net/publication/261334226_A_Bayesian_Nonparametric_Regression_Model_With_Normalized_Weights_A_Study_of_Hippocampal_Atrophy_in_Alzheimer%27s_Disease","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Hippocampal volume is one of the best established biomarkers for Alzheimer’s disease. However, for appropriate use in clinical trials research, the evolution of hippocampal volume needs to be well understood. Recent theoretical models propose a sigmoidal pattern for its evolution. To support this theory, the use of Bayesian nonparametric regression mixture models seems particularly suitable due to the flexibility that models of this type can achieve and the unsatisfactory predictive properties of semiparametric methods. In this paper, our aim is to develop an interpretable Bayesian nonparametric regression model which allows inference with combinations of both continuous and discrete covariates, as required for a full analysis of the data set. Simple arguments regarding the interpretation of Bayesian nonparametric regression mixtures lead naturally to regression weights based on normalized sums. Difficulty in working with the intractable normalizing constant is overcome thanks to recent advances in MCMC methods and the development of a novel auxiliary variable scheme. We apply the new model and MCMC method to study the dynamics of hippocampal volume, and our results provide statistical evidence in support of the theoretical hypothesis.\n</div> \n<p></p>"},{"id":351,"title":"Bayesian Modeling of Temporal Dependence in Large Sparse Contingency Tables","url":"https://www.researchgate.net/publication/259989855_Bayesian_Modeling_of_Temporal_Dependence_in_Large_Sparse_Contingency_Tables","abstraction":"In many applications, it is of interest to study trends over time in relationships among categorical variables, such as age group, ethnicity, religious affiliation, political party and preference for particular policies. At each time point, a sample of individuals provide responses to a set of questions, with different individuals sampled at each time. In such settings, there tends to be abundant missing data and the variables being measured may change over time. At each time point, one obtains a large sparse contingency table, with the number of cells often much larger than the number of individuals being surveyed. To borrow information across time in modeling large sparse contingency tables, we propose a Bayesian autoregressive tensor factorization approach. The proposed model relies on a probabilistic Parafac factorization of the joint pmf characterizing the categorical data distribution at each time point, with autocorrelation included across times. Efficient computational methods are developed relying on MCMC. The methods are evaluated through simulation examples and applied to social survey data."},{"id":352,"title":"Decision Theoretic Generalizations of the PAC Model for Neural Net and Other Learning Applications","url":"https://www.researchgate.net/publication/222190436_Decision_Theoretic_Generalizations_of_the_PAC_Model_for_Neural_Net_and_Other_Learning_Applications","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We describe a generalization of the PAC learning model that is based on statistical decision theory. In this model the learner receives randomly drawn examples, each example consisting of an instance x ? X and an outcome y ? Y, and tries to find a decision rule h: X ? A, where h ? , that specifies the appropriate action a ? A to take for each instance x in order to minimize the expectation of a loss l(y, a). Here X, Y, and A are arbitrary sets, l is a real-valued function, and examples are generated according to an arbitrary joint distribution on X × Y. Special cases include the problem of learning a function from X into Y, the problem of learning the conditional probability distribution on Y given X (regression), and the problem of learning a distribution on X (density estimation). We give theorems on the uniform convergence of empirical loss estimates to true expected loss rates for certain decision rule spaces , and show how this implies learnability with bounded sample size, disregarding computational complexity. As an application, we give distribution-independent upper bounds on the sample size needed for learning with feedforward neural networks. Our theorems use a generalized notion of VC dimension that applies to classes of real-valued functions, adapted from Vapnik and Pollard's work, and a notion of capacity and metric dimension for classes of functions that map into a bounded metric space.\n</div> \n<p></p>"},{"id":353,"title":"A Completely Automatic French Curve, Fitting Spline Functions by Cross Validation","url":"https://www.researchgate.net/publication/243048495_A_Completely_Automatic_French_Curve_Fitting_Spline_Functions_by_Cross_Validation","abstraction":"The cross validation mean square error technique is used to determine the correct degree of smoothing, in fitting smoothing solines to discrete, noisy observations from some unknown smooth function. Monte Cario results snow amazing success in estimating the true smooth function as well as its derivative."},{"id":354,"title":"Hybrid system for protein structure prediction","url":"https://www.researchgate.net/publication/21541522_Hybrid_system_for_protein_structure_prediction","abstraction":"We have developed a hybrid system to predict the secondary structures (alpha-helix, beta-sheet and coil) of proteins and achieved 66.4% accuracy, with correlation coefficients of C(coil) = 0.429, C alpha = 0.470 and C beta = 0.387. This system contains three subsystems (\"experts\"): a neural network module, a statistical module and a memory-based reasoning module. First, the three experts independently learn the mapping between amino acid sequences and secondary structures from the known protein structures, then a Combiner learns to combine automatically the outputs of the experts to make final predictions. The hybrid system was tested with 107 protein structures through k-way cross-validation. Its performance was better than each expert and all previously reported methods with greater than 0.99 statistical significance. It was observed that for 20% of the residues, all three experts produced the same but wrong predictions. This may suggest an upper bound on the accuracy of secondary structure predictions based on local information from the currently available protein structures, and indicate places where non-local interactions may play a dominant role in conformation. For 64% of the residues, at least two experts were the same and correct, which shows that the Combiner performed better than majority vote. For 77% of the residues, at least one expert was correct, thus there may still be room for improvement in this hybrid approach. Rigorous evaluation procedures were used in testing the hybrid system, and statistical significance measures were developed in analyzing the differences among different methods. When measured in terms of the number of secondary structures (rather than the number of residues) that were predicted correctly, the prediction produced by the hybrid system was also better than those of individual experts."},{"id":355,"title":"Open Loop Stable Control Strategies for Robot Juggling.","url":"https://www.researchgate.net/publication/221067769_Open_Loop_Stable_Control_Strategies_for_Robot_Juggling","abstraction":"Open-loop stable control strategies for a variety of juggling tasks are explored. The specific tasks studied are paddle juggling, ball-in-a-wedge juggling, five ball juggling, and devil sticking. All dynamic tasks introduced are solved by special purpose systems. The results of these investigations may provide insight into how open loop control can serve as a useful foundation for closed loop control and, particularly, what to focus on in learning control"},{"id":356,"title":"Toward Memory-Based Reasoning.","url":"https://www.researchgate.net/publication/220424662_Toward_Memory-Based_Reasoning","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The authors consider the phenomenon of reasoning from memories of specific episodes to be the foundation of an intelligent system, rather than an adjunct to some other reasoning method. This theory contrasts with much of the current work in similarity-based learning, which tacitly assumes that learning is equivalent to the automatic generation of rules, and differs from work on 'explanation-based' and 'case-based' reasoning in that it does not depend on having a strong domain model. With the development of new parallel architectures, specifically the Connection Machine system, the operations necessary to implement this approach to reasoning have become sufficiently fast to allow experimentation. The authors describe MBRtalk, an experimental memory-based reasoning system that has been implemented on the Connection Machine, as well as the application of memory-based reasoning to other domains.\n</div> \n<p></p>"},{"id":357,"title":"Improving the Performance of Metaheuristics: An Approach Combining Response Surface Methodology and Racing Algorithms","url":"https://www.researchgate.net/publication/281843234_Improving_the_Performance_of_Metaheuristics_An_Approach_Combining_Response_Surface_Methodology_and_Racing_Algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The setup of heuristics and metaheuristics, that is, the fine-tuning of their parameters, exercises a great influence in both the solution process, and in the quality of results of optimization problems. The search for the best fit of these algorithms is an important task and a major research challenge in the field of metaheuristics. The fine-tuning process requires a robust statistical approach, in order to aid in the process understanding and also in the effective settings, as well as an efficient algorithm which can summarize the search process. This paper aims to present an approach combining design of experiments (DOE) techniques and racing algorithms to improve the performance of different algorithms to solve classical optimization problems. The results comparison considering the default metaheuristics and ones using the settings suggested by the fine-tuning procedure will be presented. Broadly, the statistical results suggest that the fine-tuning process improves the quality of solutions for different instances of the studied problems. Therefore, by means of this study it can be concluded that the use of DOE techniques combined with racing algorithms may be a promising and powerful tool to assist in the investigation, and in the fine-tuning of different algorithms. However, additional studies must be conducted to verify the effectiveness of the proposed methodology.\n</div> \n<p></p>"},{"id":358,"title":"Pareto Upper Confidence Bounds algorithms: an empirical study","url":"https://www.researchgate.net/publication/267632371_Pareto_Upper_Confidence_Bounds_algorithms_an_empirical_study","abstraction":"Many real-world stochastic environments are in-herently multi-objective environments with conflicting objec-tives. The multi-objective multi-armed bandits (MOMAB) are extensions of the classical, i.e. single objective, multi-armed bandits to reward vectors and multi-objective optimisation techniques are often required to design mechanisms with an efficient exploration / exploitation trade-off. In this paper, we propose the improved Pareto Upper Confidence Bound (iPUCB) algorithm that straightforwardly extends the single objective improved UCB algorithm to reward vectors by deleting the suboptimal arms. The goal of the improved Pareto UCB algorithm, i.e. iPUCB, is to identify the set of best arms, or the Pareto front, in a fixed budget of arm pulls. We experimen-tally compare the performance of the proposed Pareto upper confidence bound algorithm with the Pareto UCB1 algorithm and the Hoeffding race on a bi-objective example coming from an industrial control applications, i.e. the engagement of wet clutches. We propose a new regret metric based on the Kullback-Leibler divergence to measure the performance of a multi-objective multi-armed bandit algorithm. We show that iPUCB outperforms the other two tested algorithms on the given multi-objective environment."},{"id":359,"title":"Stopping Rules for Bag-of-Words Image Search and Its Application in Appearance-Based Localization","url":"https://www.researchgate.net/publication/259483079_Stopping_Rules_for_Bag-of-Words_Image_Search_and_Its_Application_in_Appearance-Based_Localization","abstraction":"We propose a technique to improve the search efficiency of the bag-of-words (BoW) method for image retrieval. We introduce a notion of difficulty for the image matching problems and propose methods that reduce the amount of computations required for the feature vector-quantization task in BoW by exploiting the fact that easier queries need less computational resources. Measuring the difficulty of a query and stopping the search accordingly is formulated as a stopping problem. We introduce stopping rules that terminate the image search depending on the difficulty of each query, thereby significantly reducing the computational cost. Our experimental results show the effectiveness of our approach when it is applied to appearance-based localization problem."},{"id":360,"title":"RCD: A Recurring Concept Drift Framework","url":"https://www.researchgate.net/publication/235724609_RCD_A_Recurring_Concept_Drift_Framework","abstraction":"This paper presents recurring concept drifts (RCD), a framework that offers an alternative approach to handle data streams that suffer from recurring concept drifts (on-line learning). It creates a new classifier to each context found and stores a sample of data used to build it. When a new concept drift occurs, the algorithm compares the new context to previous ones using a non-parametric multivariate statistical test to verify if both contexts come from the same distribution. If so, the corresponding classifier is reused. The RCD framework is compared with several algorithms (among single and ensemble approaches), in both artificial and real data sets, chosen from frequently used algorithms and data sets in the concept drift research area. We claim the proposed framework had better average ranks in data sets with abrupt and gradual concept drifts compared to both the single classifiers and the ensemble approaches that use the same base learner."},{"id":361,"title":"On the Optimality of Particle Swarm Parameters in Dynamic Environments","url":"https://www.researchgate.net/publication/256684593_On_the_Optimality_of_Particle_Swarm_Parameters_in_Dynamic_Environments","abstraction":"This paper investigates whether the optimal param-eter configurations for particle swarm optimizers (PSO) change when changes in the search landscape occur. To test this, specific environmental changes that may occur during dynamic function optimization are deliberately constructed, using the moving peaks function generator. The parameters of the charged-and quantum PSO algorithms are then optimized for the initial environment, as well as for each of the constructed problems. It is shown that the optimal parameter configurations for the various environments differ not only with respect to the initial optimal configurations, but also with respect to each other. The results lead to the conclusion that PSO parameters need to be re-optimized or self-adapted whenever environmental changes are detected."},{"id":362,"title":"Best Arm Identification: A Unified Approach to Fixed Budget and Fixed Confidence","url":"https://www.researchgate.net/publication/234061312_Best_Arm_Identification_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We study the problem of identifying the best arm(s) in the stochastic multi-armed bandit setting. This problem has been studied in the literature from two different perspectives: fixed budget and fixed confidence. We propose a unifying approach that leads to a meta-algorithm called unified gap-based exploration (UGapE), with a common structure and similar theoretical analysis for these two settings. We prove a performance bound for the two versions of the algorithm showing that the two problems are characterized by the same notion of complexity. We also show how the UGapE algorithm as well as its theoretical analysis can be extended to take into account the variance of the arms and to multiple bandits. Finally, we evaluate the performance of UGapE and compare it with a number of existing fixed budget and fixed confidence algorithms.\n</div> \n<p></p>"},{"id":363,"title":"Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms","url":"https://www.researchgate.net/publication/230700205_Auto-WEKA_Combined_Selection_and_Hyperparameter_Optimization_ofClassification_Algorithms","abstraction":"Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that addresses these issues in isolation. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection/hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance."},{"id":364,"title":"A Comparison on How Statistical Tests Deal with Concept Drifts","url":"https://www.researchgate.net/publication/230838808_A_Comparison_on_How_Statistical_Tests_Deal_with_Concept_Drifts","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n RCD is a framework proposed to deal with recurring concept drifts. It stores classifiers together with a sample of data used to train them. If a concept drift occurs, RCD tests all the stored samples with a sample of actual data, trying to verify if this is a new context or an old one that is recurring. This is performed by a non-parametric multivariate statistical test to make the verification. This paper describes how two statistical tests (KNN and Cramer) can distinguish between new and old contexts. RCD is tested with several base classifiers, in environments with different rates-of-change values, with gradual and abrupt concept drifts. Results show that RCD improves single classifiers accuracy independently of the statistical test used.\n</div> \n<p></p>"},{"id":365,"title":"Sub-sampling: Real-time vision for micro air vehicles","url":"https://www.researchgate.net/publication/220143231_Sub-sampling_Real-time_vision_for_micro_air_vehicles","abstraction":"Small robotic systems such as Micro Air Vehicles (MAVs) need to react quickly to their dynamic environments, while having only a limited amount of energy and processing onboard. In this article, sub-sampling of local image samples is investigated as a straightforward and broadly applicable approach to improve the computational efficiency of vision algorithms. In sub-sampling, only a small subset of the total number of samples is processed, leading to a significant reduction of the computational effort at the cost of a slightly lower accuracy. The possibility to change the number of extracted samples is of particular importance to autonomous robots, since it allows the designer to select not only the performance but also the execution frequency of the algorithm. The approach of sub-sampling is illustrated by introducing two novel, computationally efficient algorithms for two tasks relevant to MAVs: WiFi noise detection in camera images and onboard horizon detection for pitch and roll estimation. In the noise detection task, image lines and pixel pairs are sampled, while in the horizon detection task features from local image patches are sampled. For both tasks experiments are performed and the effects of sub-sampling are analyzed. It is demonstrated that even for small images of size 160×120 speed-ups of a factor 14 to 21 are reached, while retaining a sufficient performance for the tasks at hand."},{"id":366,"title":"Bandit Algorithms boost Brain Computer Interfaces for motor-task selection of a brain-controlled button","url":"https://www.researchgate.net/publication/279257828_Bandit_Algorithms_boost_Brain_Computer_Interfaces_for_motor-task_selection_of_a_brain-controlled_button","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Brain-computer interfaces (BCI) allow users to \"communicate\" with a computer without using their muscles. BCI based on sensori-motor rhythms use imaginary motor tasks, such as moving the right or left hand, to send control signals. The performances of a BCI can vary greatly across users but also depend on the tasks used, making the problem of appropriate task selection an important issue. This study presents a new procedure to automatically select as fast as possible a discriminant motor task for a brain-controlled button. We develop for this purpose an adaptive algorithm, UCB-classif, based on the stochastic bandit theory. This shortens the training stage, thereby allowing the exploration of a greater variety of tasks. By not wasting time on inefficient tasks, and focusing on the most promising ones, this algorithm results in a faster task selection and a more efficient use of the BCI training session. Comparing the proposed method to the standard practice in task selection, for a fixed time budget, UCB-classif leads to an improved classification rate, and for a fixed classification rate, to a reduction of the time spent in training by 50%.\n</div> \n<p></p>"},{"id":367,"title":"On Finding the Largest Mean Among Many","url":"https://www.researchgate.net/publication/238951707_On_Finding_the_Largest_Mean_Among_Many","abstraction":"Sampling from distributions to find the one with the largest mean arises in a broad range of applications, and it can be mathematically modeled as a multi-armed bandit problem in which each distribution is associated with an arm. This paper studies the sample complexity of identifying the best arm (largest mean) in a multi-armed bandit problem. Motivated by large-scale applications, we are especially interested in identifying situations where the total number of samples that are necessary and sufficient to find the best arm scale linearly with the number of arms. We present a single-parameter multi-armed bandit model that spans the range from linear to superlinear sample complexity. We also give a new algorithm for best arm identification, called PRISM, with linear sample complexity for a wide range of mean distributions. The algorithm, like most exploration procedures for multi-armed bandits, is adaptive in the sense that the next arms to sample are selected based on previous samples. We compare the sample complexity of adaptive procedures with simpler non-adaptive procedures using new lower bounds. For many problem instances, the increased sample complexity required by non-adaptive procedures is a polynomial factor of the number of arms."},{"id":368,"title":"Pure Exploration in Multi-armed Bandits Problems","url":"https://www.researchgate.net/publication/221394179_Pure_Exploration_in_Multi-armed_Bandits_Problems","abstraction":"We consider the framework of stochastic multi-armed bandit prob- lems and study the possibilities and limitations of strategies that explore sequen- tially the arms. The strategies are assessed in terms of their simple regrets, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between simple and cumulative regrets. The main result is that the required exploration-exploitation trade-offs are qualitatively dif- ferent, in view of a general lower bound on the simple regret in terms of the cumulative regret."},{"id":369,"title":"Asymptotic Behavior of Expected Sample Size in Certain One Sided Tests","url":"https://www.researchgate.net/publication/38365999_Asymptotic_Behavior_of_Expected_Sample_Size_in_Certain_One_Sided_Tests","abstraction":"Let $R$ be the set of real numbers, $\\mathscr{B}_1$ the set of Borel sets of $R$, and $\\mu$ a $\\sigma$-finite nonnegative measure on $\\mathscr{B}_1$. Let $\\Omega$ be an open real number interval (which may be infinite). Throughout we consider a Koopman-Darmois family \\begin{equation*}\\tag{1}\\{h(\\theta) \\exp (\\theta x), \\theta \\varepsilon \\Omega\\}\\end{equation*} of generalized probability density functions on the measure space $(R, \\mathscr{B}_1, \\mu)$. We consider one sided tests $T$ of the hypothesis $\\theta &lt; 0$ against the alternative $\\theta &gt; 0$. In general, in this paper, $T$ will be a sequential procedure. Associated with $T$ is a stopping variable $N$ (mention of the dependence of $N$ on $T$ is usually omitted). $N \\geqq 0. N = n$ means that sampling stopped after $n$ observations and a decision was made. In this context we consider $\\infty$ to be an integer, and $N = \\infty$ means that sampling does not stop. In the discussion of Section 1 we will assume that if $\\theta \\varepsilon \\Omega$ and $\\theta \\neq 0$ then $P_\\theta(N &lt; \\infty) = 1$, that is, sampling stops with probability one. The reason for the exclusion of $\\theta = 0$ will become apparent in Section 1. We will be concerned with two events, decide $\\theta &lt; 0$, and, decide $\\theta &gt; 0$. The main result of this paper may be stated as follows. Theorem 1. Suppose $(R, \\mathscr{B}_1, \\mu), \\Omega$, and $\\{h(\\theta) \\exp (\\theta x), \\theta \\varepsilon \\Omega\\}$ are as described above. Define \\begin{align*}\\mu_\\theta = \\int^\\infty_{-\\infty} h(\\theta)x \\exp(\\theta x)\\mu(dx), \\\\ \\tag{2} \\\\ \\sigma^2 = \\int^\\infty_{-\\infty} h(0)x^2\\mu(dx),\\end{align*} and assume $\\mu_0 = 0$. Suppose $0 &lt; \\alpha &lt; 1$ and $0 &lt; \\beta &lt; 1$ and \\begin{equation*}\\tag{3}\\sup_{\\theta &gt; 0} P_\\theta (\\text{ decide } \\theta &lt; 0) \\leqq \\beta;\\quad \\sup_{\\theta &lt; 0} P_\\theta (\\text{ decide } \\theta &gt; 0) \\leqq \\alpha.\\end{equation*} Then \\begin{align*}\\lim \\sup_{\\theta \\rightarrow 0+} \\mu^2_\\theta|\\log|\\log\\mid\\mu_\\theta|\\|^{-1}E_\\theta N \\geqq 2\\sigma^2 P_0(N &amp;= \\infty); \\\\ \\tag{4} \\lim \\sup_{\\theta \\rightarrow 0-} \\mu^2_\\theta|\\log|\\log\\mid\\mu_\\theta|\\|^{-1}E_\\theta N \\geqq 2\\sigma^2 P_0(N &amp;= \\infty).\\end{align*} If $\\alpha + \\beta &lt; 1$ there is a generalized sequential probability ratio test $T$ with stopping variable $N$ such that for the test $T$, \\begin{equation*}\\tag{5}P_0(N = \\infty) = 1 - (\\alpha + \\beta);\\quad (3) \\text{ holds };\\end{equation*} for the test $T$, \\begin{equation*}\\tag{6}\\lim_{\\theta \\rightarrow 0} \\mu^2_\\theta|\\log|\\log\\mid\\mu_\\theta|\\|^{-1}E_\\theta N = 2\\sigma^2 P_0(N = \\infty).\\end{equation*} For all tests $T$, if $P_0(N = \\infty) &gt; 0$ then $\\lim_{\\theta \\rightarrow 0} \\theta^2E_\\theta N = \\infty$. In Section 1, (7) and (8), it is shown that $P_0(N = \\infty) \\geqq 1 - \\alpha - \\beta$. Consequently the relations (4) and (5) of Theorem 1 are not vacuous. We were led to formulate Theorem 1 by a problem of constructing bounded length confidence intervals. The relationship is explained in Section 2. The proof of Theorem 1 is given in Section 3."},{"id":370,"title":"A Sequential Procedure for Selecting the Population with the Largest Mean from $k$ Normal Populations","url":"https://www.researchgate.net/publication/235709950_A_Sequential_Procedure_for_Selecting_the_Population_with_the_Largest_Mean_from_k_Normal_Populations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper sequential procedures are given for selecting the normal population with the greatest mean when (a) the $k$ populations have a common known variance or (b) the $k$ populations have a common but unknown variance, so that in each case the probability of making the correct selection exceeds a specified value when the greatest mean exceeds all other means by at least a specified amount. The procedures in the present paper all have the property that inferior populations can be eliminated from further consideration as the experiment proceeds.\n</div> \n<p></p>"},{"id":371,"title":"PAC Subset Selection in Stochastic Multi-armed Bandits","url":"https://www.researchgate.net/publication/268291396_PAC_Subset_Selection_in_Stochastic_Multi-armed_Bandits","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the problem of selecting, from among the arms of a stochastic n-armed bandit, a subset of size m of those arms with the highest expected rewards, based on efficiently sampling the arms. This \"sub-set selection\" problem finds application in a variety of areas. In the authors' previ-ous work (Kalyanakrishnan &amp; Stone, 2010), this problem is framed under a PAC setting (denoted \"Explore-m\"), and corresponding sampling algorithms are analyzed. Whereas the formal analysis therein is restricted to the worst case sample complexity of algo-rithms, in this paper, we design and ana-lyze an algorithm (\"LUCB\") with improved expected sample complexity. Interestingly LUCB bears a close resemblance to the well-known UCB algorithm for regret minimiza-tion. The expected sample complexity bound we show for LUCB is novel even for single-arm selection (Explore-1). We also give a lower bound on the worst case sample com-plexity of PAC algorithms for Explore-m.\n</div> \n<p></p>"},{"id":372,"title":"Lower Bounds on the Sample Complexity of Exploration in the Multi-armed Bandit Problem","url":"https://www.researchgate.net/publication/221497401_Lower_Bounds_on_the_Sample_Complexity_of_Exploration_in_the_Multi-armed_Bandit_Problem","abstraction":"We consider the Multi-armed bandit problem under the PAC (\"probably approximately correct\") model. It was shown by Even-Dar et al. (5) that given n arms, it suces to play the arms a total of O (n=\"2)log(1=-) times to find an \"-optimal arm with probability of at least 1¡-. Our contribution is a matching lower bound that holds for any sampling policy. We also generalize the lower bound to a Bayesian setting, and to the case where the statistics of the arms are known but the identities of the arms are not."},{"id":373,"title":"Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons","url":"https://www.researchgate.net/publication/275669988_Spectral_MLE_Top-K_Rank_Aggregation_from_Pairwise_Comparisons","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper explores the preference-based top-$K$ rank aggregation problem.\n <br> Suppose that a collection of items is repeatedly compared in pairs, and one\n <br> wishes to recover a consistent ordering that emphasizes the top-$K$ ranked\n <br> items, based on partially revealed preferences. We focus on the\n <br> Bradley-Terry-Luce (BTL) model that postulates a set of latent preference\n <br> scores underlying all items, where the odds of paired comparisons depend only\n <br> on the relative scores of the items involved.\n <br> We characterize the minimax limits on identifiability of top-$K$ ranked\n <br> items, in the presence of random and non-adaptive sampling. Our results\n <br> highlight a separation measure that quantifies the gap of preference scores\n <br> between the $K^{\\text{th}}$ and $(K+1)^{\\text{th}}$ ranked items. The minimum\n <br> sample complexity required for reliable top-$K$ ranking scales inversely with\n <br> the separation measure irrespective of other preference distribution metrics.\n <br> To approach this minimax limit, we propose a nearly linear-time ranking scheme,\n <br> called \\emph{Spectral MLE}, that returns the indices of the top-$K$ items in\n <br> accordance to a careful score estimate. In a nutshell, Spectral MLE starts with\n <br> an initial score estimate with minimal squared loss (obtained via a spectral\n <br> method), and then successively refines each component with the assistance of\n <br> coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item\n <br> identification under minimal sample complexity. The practical applicability of\n <br> Spectral MLE is further corroborated by numerical experiments.\n</div> \n<p></p>"},{"id":374,"title":"Best-Arm Identification in Linear Bandits","url":"https://www.researchgate.net/publication/265967035_Best-Arm_Identification_in_Linear_Bandits","abstraction":"We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter $\\theta^*$ and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, we point out the connection to the $G$-optimality criterion used in optimal experimental design."},{"id":375,"title":"Unimodal Bandits without Smoothness","url":"https://www.researchgate.net/publication/263545276_Unimodal_Bandits_without_Smoothness","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider stochastic bandit problems with a continuum set of arms and where\n <br> the expected reward is a continuous and unimodal function of the arm. No\n <br> further assumption is made regarding the smoothness and the structure of the\n <br> expected reward function. We propose Stochastic Pentachotomy (SP), an algorithm\n <br> for which we derive finite-time regret upper bounds. In particular, we show\n <br> that, for any expected reward function $\\mu$ that behaves as\n <br> $\\mu(x)=\\mu(x^\\star)-C|x-x^\\star|^\\xi$ locally around its maximizer $x^\\star$\n <br> for some $\\xi, C&gt;0$, the SP algorithm is order-optimal, i.e., its regret scales\n <br> as $O(\\sqrt{T\\log(T)})$ when the time horizon $T$ grows large. This regret\n <br> scaling is achieved without the knowledge of $\\xi$ and $C$. Our algorithm is\n <br> based on asymptotically optimal sequential statistical tests used to\n <br> successively prune an interval that contains the best arm with high\n <br> probability. To our knowledge, the SP algorithm constitutes the first\n <br> sequential arm selection rule that achieves a regret scaling as $O(\\sqrt{T})$\n <br> up to a logarithmic factor for non-smooth expected reward functions, as well as\n <br> for smooth functions with unknown smoothness.\n</div> \n<p></p>"},{"id":376,"title":"Near-Optimal Adaptive Compressed Sensing","url":"https://www.researchgate.net/publication/242014552_Near-Optimal_Adaptive_Compressed_Sensing","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper proposes a simple adaptive sensing and group testing algorithm for\n <br> sparse signal recovery. The algorithm, termed Compressive Adaptive Sense and\n <br> Search (CASS), is shown to be near-optimal in that it succeeds at the lowest\n <br> possible signal-to-noise-ratio (SNR) levels. Like traditional compressed\n <br> sensing based on random non-adaptive design matrices, the CASS algorithm\n <br> requires only k log n measurements to recover a k-sparse signal of dimension n.\n <br> However, CASS succeeds at SNR levels that are a factor log n less than required\n <br> by standard compressed sensing. From the point of view of constructing and\n <br> implementing the sensing operation as well as computing the reconstruction, the\n <br> proposed algorithm is substantially less computationally intensive than\n <br> standard compressed sensing. CASS is also demonstrated to perform considerably\n <br> better in practice through simulation. To the best of our knowledge, this is\n <br> the first demonstration of an adaptive compressed sensing algorithm with\n <br> near-optimal theoretical guarantees and excellent practical performance. This\n <br> paper also shows that methods like compressed sensing, group testing, and\n <br> pooling have an advantage beyond simply reducing the number of measurements or\n <br> tests -- adaptive versions of such methods can also improve detection and\n <br> estimation performance when compared to non-adaptive direct (uncompressed)\n <br> sensing.\n</div> \n<p></p>"},{"id":377,"title":"Combinatorial pure exploration of multi-armed bandits","url":"https://www.researchgate.net/publication/281854812_Combinatorial_pure_exploration_of_multi-armed_bandits","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We study the {\\em combinatorial pure exploration (CPE)} problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a \\emph{decision class}, which is a collection of subsets of arms with certain combinatorial structures such as size-K subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a non-trivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-K arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.\n</div> \n<p></p>"},{"id":378,"title":"Ordinal optimization - empirical large deviations rate estimators, and stochastic multi-armed bandits","url":"https://www.researchgate.net/publication/280104102_Ordinal_optimization_-_empirical_large_deviations_rate_estimators_and_stochastic_multi-armed_bandits","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Consider the ordinal optimization problem of finding a population amongst\n <br> many with the smallest mean when these means are unknown but population samples\n <br> can be generated via simulation. Typically, by selecting a population with the\n <br> smallest sample mean, it can be shown that the false selection probability\n <br> decays at an exponential rate. Lately researchers have sought algorithms that\n <br> guarantee that this probability is restricted to a small $\\delta$ in order\n <br> $\\log(1/\\delta)$ computational time by estimating the associated large\n <br> deviations rate function via simulation. We show that such guarantees are\n <br> misleading. Enroute, we identify the large deviations principle followed by the\n <br> empirically estimated large deviations rate function that may also be of\n <br> independent interest. Further, we show a negative result that when populations\n <br> have unbounded support, any policy that asymptotically identifies the correct\n <br> population with probability at least $1-\\delta$ for each problem instance\n <br> requires more than $O(\\log(1/\\delta))$ samples in making such a determination\n <br> in any problem instance. This suggests that some restrictions are essential on\n <br> populations to devise $O(\\log(1/\\delta))$ algorithms with $1 - \\delta$\n <br> correctness guarantees. We note that under restriction on population moments,\n <br> such methods are easily designed. We also observe that sequential methods from\n <br> stochastic multi-armed bandit literature can be adapted to devise such\n <br> algorithms.\n</div> \n<p></p>"},{"id":379,"title":"Optimally Confident UCB : Improved Regret for Finite-Armed Bandits","url":"https://www.researchgate.net/publication/280590277_Optimally_Confident_UCB_Improved_Regret_for_Finite-Armed_Bandits","abstraction":"I present the first algorithm for stochastic finite-armed bandits that simultaneously enjoys order-optimal problem-dependent regret and worst-case regret. The algorithm is based on UCB, but with a carefully chosen confidence parameter that optimally balances the risk of failing confidence intervals against the cost of excessive optimism. A brief empirical evaluation suggests the new algorithm is at least competitive with Thompson sampling."},{"id":380,"title":"Randomness Efficient Fast-Johnson-Lindenstrauss Transform with Applications in Differential Privacy and Compressed Sensing","url":"https://www.researchgate.net/publication/274703698_Randomness_Efficient_Fast-Johnson-Lindenstrauss_Transform_with_Applications_in_Differential_Privacy_and_Compressed_Sensing","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Johnson-Lindenstrauss property (JLP) of random matrices has immense applications in computer science ranging from compressed sensing, learning theory, numerical linear algebra, to privacy. This paper explores the properties and applications of a distribution of random matrices. Our distribution satisfies JLP with desirable properties like fast matrix-vector multiplication, bounded sparsity, and optimal subspace embedding. We can sample a random matrix from this distribution using exactly 3n random bits. We show that a random matrix picked from this distribution preserves differential privacy if the input private matrix satisfies certain spectral properties. This improves the run-time of various differentially private algorithms like Blocki et al. (FOCS 2012) and Upadhyay (ASIACRYPT 13). We also show that successive applications in a specific format of a random matrix picked from our distribution also preserve privacy, and, therefore, allows faster private low-rank approximation algorithm of Upadhyay (arXiv 1409.5414). Since our distribution has bounded column sparsity, this also answers an open problem stated in Blocki et al. (FOCS 2012). We also explore the application of our distribution in manifold learning, and give the first differentially private algorithm for manifold learning if the manifold is smooth. Using the results of Baranuik et al. (Constructive Approximation: 28(3)), our result also implies a distribution of random matrices that satisfies the Restricted-Isometry Property of optimal order for small sparsity. We also show that other known distributions of sparse random matrices with the JLP does not preserve differential privacy, thereby answering one of the open problem posed by Blocki et al. (FOCS 2012). Extending on the work of Kane and Nelson (JACM: 61(1)), we also give a unified analysis of some of the known Johnson-Lindenstrauss transform. We also present a self-contained simplified proof of the known inequality on quadratic form of Gaussian variables that we use in all our proofs. This could be of independent interest.\n</div> \n<p></p>"},{"id":381,"title":"On the Effective Measure of Dimension in the Analysis Cosparse Model","url":"https://www.researchgate.net/publication/266561017_On_the_Effective_Measure_of_Dimension_in_the_Analysis_Cosparse_Model","abstraction":"Many applications have benefited remarkably from low-dimensional models in the recent decade. The fact that many signals, though high dimensional, are intrinsically low dimensional has given the possibility to recover them stably from a relatively small number of their measurements. For example, in compressed sensing with the standard (synthesis) sparsity prior and in matrix completion, the number of measurements needed is proportional (up to a logarithmic factor) to the signal's manifold dimension. Recently, a new natural low-dimensional signal model has been proposed: the cosparse analysis prior. In the noiseless case, it is possible to recover signals from this model, using a combinatorial search, from a number of measurements proportional to the signal's manifold dimension. However, if we ask for stability to noise or an efficient (polynomial complexity) solver, all the existing results demand a number of measurements which is far removed from the manifold dimension, sometimes far greater. Thus, it is natural to ask whether this gap is a deficiency of the theory and the solvers, or if there exists a real barrier in recovering the cosparse signals by relying only on their manifold dimension. In this work we show that the latter is true both in theory and in simulations. This gives a practical counter-example to the growing literature on compressed acquisition of signals based on manifold dimension."},{"id":382,"title":"Error-bounded Sampling for Analytics on Big Sparse Data","url":"https://www.researchgate.net/publication/262603168_Error-bounded_Sampling_for_Analytics_on_Big_Sparse_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Aggregation queries are at the core of business intelligence and data analytics. In the big data era, many scalable sharednothing systems have been developed to process aggregation queries over massive amount of data. Microsoft's SCOPE is a well-known instance in this category. Nevertheless, aggregation queries are still expensive, because query processing needs to consume the entire data set, which is often hundreds of terabytes. Data sampling is a technique that samples a small portion of data to process and returns an approximate result with an error bound, thereby reducing the query's execution time. While similar problems were studied in the database literature, we encountered new challenges that disable most of prior efforts: (1) error bounds are dictated by end users and cannot be compromised, (2) data is sparse, meaning data has a limited population but a wide range. For such cases, conventional uniform sampling often yield high sampling rates and thus deliver limited or no performance gains. In this paper, we propose error-bounded stratified sampling to reduce sample size. The technique relies on the insight that we may only reduce the sampling rate with the knowledge of data distributions. The technique has been implemented into Microsoft internal search query platform. Results show that the proposed approach can reduce up to 99% sample size comparing with uniform sampling, and its performance is robust against data volume and other key performance metrics.\n</div> \n<p></p>"},{"id":383,"title":"Monte Carlo Non-Local Means: Random Sampling for Large-Scale Image Filtering","url":"https://www.researchgate.net/publication/259483032_Monte_Carlo_Non-Local_Means_Random_Sampling_for_Large-Scale_Image_Filtering","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a randomized version of the non-local means (NLM) algorithm for\n <br> large-scale image filtering. The new algorithm, called Monte Carlo non-local\n <br> means (MCNLM), speeds up the classical NLM by computing a small subset of image\n <br> patch distances, which are randomly selected according to a designed sampling\n <br> pattern. We make two contributions. First, we analyze the performance of the\n <br> MCNLM algorithm and show that, for large images or large external image\n <br> databases, the random outcomes of MCNLM are tightly concentrated around the\n <br> deterministic full NLM result. In particular, our error probability bounds show\n <br> that, at any given sampling ratio, the probability for MCNLM to have a large\n <br> deviation from the original NLM solution decays exponentially as the size of\n <br> the image or database grows. Second, we derive explicit formulas for optimal\n <br> sampling patterns that minimize the error probability bound by exploiting\n <br> partial knowledge of the pairwise similarity weights. Numerical experiments\n <br> show that MCNLM is competitive with other state-of-the-art fast NLM algorithms\n <br> for single-image denoising. When applied to denoising images using an external\n <br> database containing ten billion patches, MCNLM returns a randomized solution\n <br> that is within 0.2 dB of the full NLM solution while reducing the runtime by\n <br> three orders of magnitude.\n</div> \n<p></p>"},{"id":384,"title":"A Systematic Martingale Construction with Applications to Permutation Inequalities","url":"https://www.researchgate.net/publication/232718573_A_Systematic_Martingale_Construction_with_Applications_to_PermutationInequalities","abstraction":"We illustrate a process that constructs martingales from raw material that arises naturally from the theory of sampling without replacement.The usefulness of the new martingales is illustrated by the development of maximal inequalities for permuted sequences of real numbers. Some of these inequalities are new and some are variations of classical inequalities like those introduced by A. Garsia in the study of rearrangement of orthogonal series."},{"id":385,"title":"Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms","url":"https://www.researchgate.net/publication/51914398_Explicit_Learning_Curves_for_Transduction_and_Application_to_Clusteringand_Compression_Algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Inductive learning is based on inferring a general rule from a finite data\n <br> set and using it to label new data. In transduction one attempts to solve the\n <br> problem of using a labeled training set to label a set of unlabeled points,\n <br> which are given to the learner prior to learning. Although transduction seems\n <br> at the outset to be an easier task than induction, there have not been many\n <br> provably useful algorithms for transduction. Moreover, the precise relation\n <br> between induction and transduction has not yet been determined. The main\n <br> theoretical developments related to transduction were presented by Vapnik more\n <br> than twenty years ago. One of Vapnik's basic results is a rather tight error\n <br> bound for transductive classification based on an exact computation of the\n <br> hypergeometric tail. While tight, this bound is given implicitly via a\n <br> computational routine. Our first contribution is a somewhat looser but explicit\n <br> characterization of a slightly extended PAC-Bayesian version of Vapnik's\n <br> transductive bound. This characterization is obtained using concentration\n <br> inequalities for the tail of sums of random variables obtained by sampling\n <br> without replacement. We then derive error bounds for compression schemes such\n <br> as (transductive) support vector machines and for transduction algorithms based\n <br> on clustering. The main observation used for deriving these new error bounds\n <br> and algorithms is that the unlabeled test points, which in the transductive\n <br> setting are known in advance, can be used in order to construct useful data\n <br> dependent prior distributions over the hypothesis space.\n</div> \n<p></p>"},{"id":386,"title":"Inégalités de concentration pour le sondage aléatoire simple","url":"https://www.researchgate.net/publication/44709800_Inegalites_de_concentration_pour_le_sondage_aleatoire_simple","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n En théorie des sondages, la loi de l'estimateur de Horvitz Thompson de la moyenne d'un caractère d'une population est, en pratique, approximée par une loi normale. Cette approximation est justifiée par un théorème de Hàjek, qui démontre la convergence vers une loi normale de l'estimateur de Horvitz Thompson sous certaines conditions. Il existe aussi des résultats à distance finie, notamment ceux de Hoeffding et Serfling qui permettent d'obtenir des inégalités de concentration étant donné la dispersion du caractère sur la population. Toutefois, les inégalités obtenues s'avèrent très larges et sont peu utilisées en pratique. Lors de l'exposé, on s'attachera à montrer comment obtenir des inégalités de concentration plus fines pour ce problème particulier en utilisant des outils moins puissants, mais aussi moins généraux. Dans un premier temps en utilisant les symétries de l'espace des échantillons de taille n, et dans un second temps, en munissant l'espace des échantillons de la distance de Hamming, et en considérant la mesure des boules centrées en un échantillon. Enfin, on comparera les inégalités obtenues avec les inégalités de Hoeffding et Serfling.\n</div> \n<p></p>"},{"id":387,"title":"Optimal Calibration for Multiple Testing against Local Inhomogeneity in Higher Dimension","url":"https://www.researchgate.net/publication/45866850_Optimal_Calibration_for_Multiple_Testing_against_Local_Inhomogeneity_in_Higher_Dimension","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Based on two independent samples X\n <br> 1, . . . , X\n <br> m\n <br> and X\n <br> m+1, . . . , X\n <br> n\n <br> drawn from multivariate distributions with unknown Lebesgue densities p and q respectively, we propose an exact multiple test in order to identify simultaneously regions of significant deviations between p and q. The construction is built from randomized nearest-neighbor statistics. It does not require any preliminary information about the multivariate densities such as compact support, strict positivity or smoothness and shape properties. The properly adjusted multiple testing procedure is shown to be sharp-optimal for typical arrangements of the observation values which appear with probability close to one. The proof relies on a new coupling Bernstein type exponential inequality, reflecting the non-subgaussian tail behavior of a combinatorial process. For power investigation of the proposed method a reparametrized minimax set-up is introduced, reducing the composite hypothesis “p = q” to a simple one with the multivariate mixed density (m/n)p + (1 ? m/n)q as infinite dimensional nuisance parameter. Within this framework, the test is shown to be spatially and sharply asymptotically adaptive with respect to uniform loss on isotropic Hölder classes. The exact minimax risk asymptotics are obtained in terms of solutions of the optimal recovery.\n</div> \n<p></p>"},{"id":388,"title":"Stochastic Backpropagation and Approximate Inference in Deep Generative Models","url":"https://www.researchgate.net/publication/262991675_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models","abstraction":"We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation."},{"id":389,"title":"Sample-based Non-uniform random variate generation","url":"https://www.researchgate.net/publication/234817582_Sample-based_Non-uniform_random_variate_generation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A sample of n lid random variables with a given unknown density is given. We discuss several issues related to the problem or generating a new sample of lid random variables with almost the same density. In particular, we look at sample independence, consistency, sample indistinguishability, moment matching and generator efficiency. We also introduce the notion of a replacement number, the minimum number of observations in a given sample that have to be replaced to obtain a sample with a given density.\n</div> \n<p></p>"},{"id":390,"title":"Generating Sentences from a Continuous Space","url":"https://www.researchgate.net/publication/284219020_Generating_Sentences_from_a_Continuous_Space","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The standard unsupervised recurrent neural network language model (RNNLM)\n <br> generates sentences one word at a time and does not work from an explicit\n <br> global distributed sentence representation. In this work, we present an\n <br> RNN-based variational autoencoder language model that incorporates distributed\n <br> latent representations of entire sentences. This factorization allows it to\n <br> explicitly model holistic properties of sentences such as style, topic, and\n <br> high-level syntactic features. Samples from the prior over these sentence\n <br> representations remarkably produce diverse and well-formed sentences through\n <br> simple deterministic decoding. By examining paths through this latent space, we\n <br> are able to generate coherent novel sentences that interpolate between known\n <br> sentences. We present techniques for solving the difficult learning problem\n <br> presented by this model, demonstrate strong performance in the imputation of\n <br> missing tokens, and explore many interesting properties of the latent sentence\n <br> space.\n</div> \n<p></p>"},{"id":391,"title":"Variational Auto-encoded Deep Gaussian Processes","url":"https://www.researchgate.net/publication/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We develop a scalable deep non-parametric generative model by augmenting deep\n <br> Gaussian processes with a recognition model. Inference is performed in a novel\n <br> scalable variational framework where the variational posterior distributions\n <br> are reparametrized through a multilayer perceptron. The key aspect of this\n <br> reformulation is that it prevents the proliferation of variational parameters\n <br> which otherwise grow linearly in proportion to the sample size. We derive a new\n <br> formulation of the variational lower bound that allows us to distribute most of\n <br> the computation in a way that enables to handle datasets of the size of\n <br> mainstream deep learning tasks. We show the efficacy of the method on a variety\n <br> of challenges including deep unsupervised learning and deep Bayesian\n <br> optimization.\n</div> \n<p></p>"},{"id":392,"title":"Adversarial Autoencoders","url":"https://www.researchgate.net/publication/284219537_Adversarial_Autoencoders","abstraction":"In this paper we propose a new method for regularizing autoencoders by imposing an arbitrary prior on the latent representation of the autoencoder. Our method, named \"adversarial autoencoder\", uses the recently proposed generative adversarial networks (GAN) in order to match the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior. Matching the aggregated posterior to the prior ensures that there are no \"holes\" in the prior, and generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how adversarial autoencoders can be used to disentangle style and content of images and achieve competitive generative performance on MNIST, Street View House Numbers and Toronto Face datasets."},{"id":393,"title":"MuProp: Unbiased Backpropagation for Stochastic Neural Networks","url":"https://www.researchgate.net/publication/284219954_MuProp_Unbiased_Backpropagation_for_Stochastic_Neural_Networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Deep neural networks are powerful parametric models that can be trained\n <br> efficiently using the backpropagation algorithm. Stochastic neural networks\n <br> combine the power of large parametric functions with that of graphical models,\n <br> which makes it possible to learn very complex distributions. However, as\n <br> backpropagation is not directly applicable to stochastic networks that include\n <br> discrete sampling operations within their computational graph, training such\n <br> networks remains difficult. We present MuProp, an unbiased gradient estimator\n <br> for stochastic networks, designed to make this task easier. MuProp improves on\n <br> the likelihood-ratio estimator by reducing its variance using a control variate\n <br> based on the first-order Taylor expansion of a mean-field network. Crucially,\n <br> unlike prior attempts at using backpropagation for training stochastic\n <br> networks, the resulting estimator is unbiased and well behaved. Our experiments\n <br> on structured output prediction and discrete latent variable modeling\n <br> demonstrate that MuProp yields consistently good performance across a range of\n <br> difficult tasks.\n</div> \n<p></p>"},{"id":394,"title":"Visual Language Modeling on CNN Image Representations","url":"https://www.researchgate.net/publication/283762531_Visual_Language_Modeling_on_CNN_Image_Representations","abstraction":"Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets."},{"id":395,"title":"The Variational Fair Auto Encoder","url":"https://www.researchgate.net/publication/283532090_The_Variational_Fair_Auto_Encoder","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We investigate the problem of learning representations that are invariant to\n <br> certain nuisance or sensitive factors of variation in the data while retaining\n <br> as much of the remaining information as possible. Our model is based on a\n <br> variational auto-encoding architecture with priors that encourage independence\n <br> between sensitive and latent factors of variation. Any subsequent processing,\n <br> such as classification, can then be performed on this purged latent\n <br> representation. To remove any remaining dependencies we incorporate an\n <br> additional penalty term based on the ``Maximum Mean Discrepancy'' (MMD)\n <br> measure. We discuss how these architectures can be efficiently trained on data\n <br> and show in experiments that this method is more effective than previous work\n <br> in removing unwanted sources of variation while maintaining informative latent\n <br> representations.\n</div> \n<p></p>"},{"id":396,"title":"Deep Temporal Sigmoid Belief Networks for Sequence Modeling","url":"https://www.researchgate.net/publication/282181295_Deep_Temporal_Sigmoid_Belief_Networks_for_Sequence_Modeling","abstraction":"Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences."},{"id":397,"title":"The Population Posterior and Bayesian Inference on Streams","url":"https://www.researchgate.net/publication/280243202_The_Population_Posterior_and_Bayesian_Inference_on_Streams","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Many modern data analysis problems involve inferences from streaming data.\n <br> However, streaming data is not easily amenable to the standard probabilistic\n <br> modeling approaches, which assume that we condition on finite data. We develop\n <br> population variational Bayes, a new approach for using Bayesian modeling to\n <br> analyze streams of data. It approximates a new type of distribution, the\n <br> population posterior, which combines the notion of a population distribution of\n <br> the data with Bayesian inference in a probabilistic model. We study our method\n <br> with latent Dirichlet allocation and Dirichlet process mixtures on several\n <br> large-scale data sets.\n</div> \n<p></p>"},{"id":398,"title":"When crowds hold privileges: Bayesian unsupervised representation learning with oracle constraints","url":"https://www.researchgate.net/publication/278733876_When_crowds_hold_privileges_Bayesian_unsupervised_representation_learning_with_oracle_constraints","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Representation learning systems typically rely on massive amounts of labeled\n <br> data in order to be trained effectively. Recently, high-dimensional parametric\n <br> models like convolutional neural networks have succeeded in building rich\n <br> representations using either compressive, reconstructive or supervised\n <br> criteria. However, the semantic structure inherent in observations is\n <br> oftentimes lost in the process. Human perception excels at understanding\n <br> semantics but cannot always be expressed in terms of labels. Human-in-the-loop\n <br> systems like crowdsourcing are often employed to generate similarity\n <br> constraints using an implicit similarity function encoded in human perception.\n <br> We propose to combine generative unsupervised feature learning with learning\n <br> from similarity orderings in order to learn models which take advantage of\n <br> privileged information coming from the crowd. We use a fast variational\n <br> algorithm to learn the model on standard datasets and demonstrate applicability\n <br> to two image datasets, where classification is drastically improved. We show\n <br> how triplet-samples of the crowd can supplement labels as a source of\n <br> information to shape latent spaces with rich semantic information.\n</div> \n<p></p>"},{"id":399,"title":"A Recurrent Latent Variable Model for Sequential Data","url":"https://www.researchgate.net/publication/277959228_A_Recurrent_Latent_Variable_Model_for_Sequential_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we explore the inclusion of random variables into the dynamic\n <br> latent state of a recurrent neural network (RNN) by combining elements of the\n <br> variational autoencoder. We argue that through the use of high-level latent\n <br> random variables, our variational RNN (VRNN) is able to learn to model the kind\n <br> of variability observed in highly-structured sequential data (such as speech).\n <br> We empirically evaluate the proposed model against related sequential models on\n <br> five sequence datasets, four of speech and one of handwriting. Our results show\n <br> the importance of the role random variables can play in the RNN dynamic latent\n <br> state.\n</div> \n<p></p>"},{"id":400,"title":"A decomposition model to track gene expression signatures: Preview on observer-independent classification of ovarian cancer","url":"https://www.researchgate.net/publication/6265760_A_decomposition_model_to_track_gene_expression_signatures_Preview_on_observer-independent_classification_of_ovarian_cancer","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n MOTIVATION: A number of algorithms and analytical models have been employed to reduce the multidimensional complexity of DNA array data and attempt to extract some meaningful interpretation of the results. These include clustering, principal components analysis, self-organizing maps, and support vector machine analysis. Each method assumes an implicit model for the data, many of which separate genes into distinct clusters defined by similar expression profiles in the samples tested. A point of concern is that many genes may be involved in a number of distinct behaviours, and should therefore be modelled to fit into as many separate clusters as detected in the multidimensional gene expression space. The analysis of gene expression data using a decomposition model that is independent of the observer involved would be highly beneficial to improve standard and reproducible classification of clinical and research samples. RESULTS: We present a variational independent component analysis (ICA) method for reducing high dimensional DNA array data to a smaller set of latent variables, each associated with a gene signature. We present the results of applying the method to data from an ovarian cancer study, revealing a number of tissue type-specific and tissue type-independent gene signatures present in varying amounts among the samples surveyed. The observer independent results of such molecular analysis of biological samples could help identify patients who would benefit from different treatment strategies. We further explore the application of the model to similar high-throughput studies.\n</div> \n<p></p>"},{"id":401,"title":"Independent Component Analysis of Electroencephalographic Data","url":"https://www.researchgate.net/publication/2242002_Independent_Component_Analysis_of_Electroencephalographic_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Because of the distance between the skull and brain and their different resistivities, electroencephalographic (EEG) data collected from any point on the human scalp includes activity generated within a large brain area. This spatial smearing of EEG data by volume conduction does not involve significant time delays, however, suggesting that the Independent Component Analysis (ICA) algorithm of Bell and Sejnowski [1] is suitable for performing blind source separation on EEG data. The ICA algorithm separates the problem of source identification from that of source localization. First results of applying the ICA algorithm to EEG and event-related potential (ERP) data collected during a sustained auditory detection task show: (1) ICA training is insensitive to different random seeds. (2) ICA may be used to segregate obvious artifactual EEG components (line and muscle noise, eye movements) from other sources. (3) ICA is capable of isolating overlapping EEG phenomena, including...\n</div> \n<p></p>"},{"id":402,"title":"Modeling Dyadic Data with Binary Latent Factors.","url":"https://www.researchgate.net/publication/221619194_Modeling_Dyadic_Data_with_Binary_Latent_Factors","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Abstract We introduce binary matrix factorization, a novel model for unsupervised matrix decomposition.,The decomposition,is learned by fitting a non-parametric Bayesian probabilistic model with binary latent variables to a matrix of dyadic data. Unlike bi-clustering models, which assign each row or column to a single cluster based on a categorical hidden feature, our binary fe ature model reflects the prior belief that items and attributes can be associated wit h more than one latent cluster at a time. We provide simple learning and inference r ules for this new model,and show how,to extend it to an infinite model,in which,the number,of features is not a priori fixed but is allowed to grow with the si ze of the data. 1 Distributed representations for dyadic data\n</div> \n<p></p>"},{"id":403,"title":"On Bayesian analysis of mixture with unknown number of components","url":"https://www.researchgate.net/publication/2348531_On_Bayesian_analysis_of_mixture_with_unknown_number_of_components","abstraction":"This article is a contribution to the methodology of fully Bayesian mixture modelling. We stress the word \"fully\" in two senses. First, we model the number of components and the mixture component parameters jointly and base inference about these quantities on their posterior probabilities. This is in contrast to most previous Bayesian treatments of mixture estimation, which consider models for different numbers of components separately, and use significance tests or other non-Bayesian criteria to infer the number of components. Secondly, we aim to present posterior distributions of our objects of inference (model parameters and predictive densities), and not just \"best estimates\". There are three key ideas in our treatment. First, we demonstrate that novel MCMC methods, the \"reversible jump\" samplers introduced by Green (1994, 1995), can be used to sample mixture representations with an unknown and hence varying number of components. We believe these methods are preferable on grounds of convenience,"},{"id":404,"title":"Bayesian nonparametric latent feature models","url":"https://www.researchgate.net/publication/228373683_Bayesian_nonparametric_latent_feature_models","abstraction":"We describe a flexible nonparametric approach to latent variable modelling in which the number of latent variables is unbounded. This approach is based on a probability distribution over equivalence classes of binary matrices with a finite number of rows, corresponding to the data points, and an unbounded number of columns, corresponding to the latent variables. Each data point can be associated with a subset of the possible latent variables, which we re-fer to as the latent features of that data point. The binary variables in the matrix indicate which latent feature is possessed by which data point, and there is a potentially infinite array of features. We derive the distribution over unbounded binary matrices by taking the limit of a distribution over N × K binary matrices as K ? ?. We define a simple generative processes for this distribution which we call the Indian buffet process (IBP; Griffiths and Ghahramani, 2005, 2006) by analogy to the Chinese restaurant process (Aldous, 1985; Pitman, 2002). The IBP has a single hyperparameter which controls both the number of feature per object and the total number of fea-tures. We describe a two-parameter generalization of the IBP which has addi-tional flexibility, independently controlling the number of features per object and the total number of features in the matrix. The use of this distribution as a prior in an infinite latent feature model is illustrated, and Markov chain Monte Carlo algorithms for inference are described."},{"id":405,"title":"Compressed Sensing-Based Inpainting of Aqua Moderate Resolution Imaging Spectroradiometer Band 6 Using Adaptive Spectrum-Weighted Sparse Bayesian Dictionary Learning","url":"https://www.researchgate.net/publication/260623423_Compressed_Sensing-Based_Inpainting_of_Aqua_Moderate_Resolution_Imaging_Spectroradiometer_Band_6_Using_Adaptive_Spectrum-Weighted_Sparse_Bayesian_Dictionary_Learning","abstraction":"Because of malfunction or noise in 15 out of the 20 detectors, band 6 (1.628–1.652 $mu{rm m}$) of the Moderate Resolution Imaging Spectroradiometer (MODIS) sensor aboard the Aqua satellite contains large areas of dead pixel stripes. Therefore, the corresponding high-level products of MODIS are corrupted by this periodic phenomenon. This paper proposes an improved Bayesian dictionary learning algorithm based on the burgeoning compressed sensing theory to solve this problem. Compared with other state-of-the-art methods, the proposed method can adaptively exploit the spectral relations of band 6 and other spectra. The performance of the proposed method is demonstrated by experiments on both simulated Terra and real Aqua images."},{"id":406,"title":"A Bayesian Nonparametric Approach to Image Super-Resolution","url":"https://www.researchgate.net/publication/230996489_A_Bayesian_Nonparametric_Approach_to_Image_Super-Resolution","abstraction":"Super-resolution methods form high-resolution images from low-resolution images. In this paper, we develop a new Bayesian nonparametric model for super-resolution. Our method uses a beta-Bernoulli process to learn a set of recurring visual patterns, called dictionary elements, from the data. Because it is nonparametric, the number of elements found is also determined from the data. We test the results on both benchmark and natural images, comparing with several other models from the research literature. We perform large-scale human evaluation experiments to assess the visual quality of the results. In a first implementation, we use Gibbs sampling to approximate the posterior. However, this algorithm is not feasible for large-scale data. To circumvent this, we then develop an online variational Bayes (VB) algorithm. This algorithm finds high quality dictionaries in a fraction of the time needed by the Gibbs sampler."},{"id":407,"title":"Bayesian Unification of Sound Source Localization and Separation with Permutation Resolution","url":"https://www.researchgate.net/publication/239938718_Bayesian_Unification_of_Sound_Source_Localization_and_Separation_with_Permutation_Resolution","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Sound source localization and separation with permutation resolution are essential for achieving a computational auditory scene analysis system that can extract useful information from a mixture of various sounds. Because existing methods cope separately with these problems despite their mutual dependence, the overall result with these approaches can be degraded by any failure in one of these components. This paper presents a unified Bayesian framework to solve these problems simultaneously where localization and separation are regarded as a clustering problem. Experimental results confirm that our method outperforms state-of-the-art methods in terms of the separation quality with various setups including practical reverberant environments. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.\n</div> \n<p></p>"},{"id":408,"title":"Distance Dependent Infinite Latent Feature Models","url":"https://www.researchgate.net/publication/51948589_Distance_Dependent_Infinite_Latent_Feature_Models","abstraction":"Latent feature models are widely used to decompose data into a small number of components. Bayesian nonparametric variants of these models, which use the Indian buffet process (IBP) as a prior over latent features, allow the number of features to be determined from the data. We present a generalization of the IBP, the distance dependent Indian buffet process (dd-IBP), for modeling non-exchangeable data. It relies on distances defined between data points, biasing nearby data to share more features. The choice of distance measure allows for many kinds of dependencies, including temporal and spatial. Further, the original IBP is a special case of the dd-IBP. In this paper, we develop the dd-IBP and theoretically characterize its feature-sharing properties. We derive a Markov chain Monte Carlo sampler for a linear Gaussian model with a dd-IBP prior and study its performance on several non-exchangeable data sets."},{"id":409,"title":"Covariate-dependent dictionary learning and sparse coding","url":"https://www.researchgate.net/publication/224246550_Covariate-dependent_dictionary_learning_and_sparse_coding","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A dependent hierarchical beta process (dHBP) is developed as a prior for data that may be represented in terms of a sparse set of latent features (dictionary elements), with covariate dependent feature usage. The dHBP is applicable to general covariates and data models, imposing that signals with similar covariates are likely to be manifested in terms of similar features. As an application, we consider the simultaneous sparse modeling of multiple images, with the covariate of a given image linked to its similarity to all other images (as applied in manifold learning). Efficient inference is performed using hybrid Gibbs, Metropolis-Hastings and slice sampling.\n</div> \n<p></p>"},{"id":410,"title":"Nonparametric Bayesian sparse factor models with application to gene expression modeling","url":"https://www.researchgate.net/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling","abstraction":"A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where observed data $\\mathbf{Y}$ is modeled as a linear superposition, $\\mathbf{G}$, of a potentially infinite number of hidden factors, $\\mathbf{X}$. The Indian Buffet Process (IBP) is used as a prior on $\\mathbf{G}$ to incorporate sparsity and to allow the number of latent features to be inferred. The model's utility for modeling gene expression data is investigated using randomly generated data sets based on a known sparse connectivity matrix for E. Coli, and on three biological data sets of increasing complexity."},{"id":411,"title":"Bayesian inference of the number of factors in gene-expression analysis: Application to human virus challenge studies","url":"https://www.researchgate.net/publication/227113488_Bayesian_inference_of_the_number_of_factors_in_gene-expression_analysis_Application_to_human_virus_challenge_studies","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Background\n <br> Nonparametric Bayesian techniques have been developed recently to extend the sophistication of factor models, allowing one to infer the number of appropriate factors from the observed data. We consider such techniques for sparse factor analysis, with application to gene-expression data from three virus challenge studies. Particular attention is placed on employing the Beta Process (BP), the Indian Buffet Process (IBP), and related sparseness-promoting techniques to infer a proper number of factors. The posterior density function on the model parameters is computed using Gibbs sampling and variational Bayesian (VB) analysis.\n <br> \n <br> Results\n <br> Time-evolving gene-expression data are considered for respiratory syncytial virus (RSV), Rhino virus, and influenza, using blood samples from healthy human subjects. These data were acquired in three challenge studies, each executed after receiving institutional review board (IRB) approval from Duke University. Comparisons are made between several alternative means of per-forming nonparametric factor analysis on these data, with comparisons as well to sparse-PCA and Penalized Matrix Decomposition (PMD), closely related non-Bayesian approaches.\n <br> \n <br> Conclusions\n <br> Applying the Beta Process to the factor scores, or to the singular values of a pseudo-SVD construction, the proposed algorithms infer the number of factors in gene-expression data. For real data the \"true\" number of factors is unknown; in our simulations we consider a range of noise variances, and the proposed Bayesian models inferred the number of factors accurately relative to other methods in the literature, such as sparse-PCA and PMD. We have also identified a \"pan-viral\" factor of importance for each of the three viruses considered in this study. We have identified a set of genes associated with this pan-viral factor, of interest for early detection of such viruses based upon the host response, as quantified via gene-expression data.\n</div> \n<p></p>"},{"id":412,"title":"Sparse Linear Identifiable Multivariate Modeling","url":"https://www.researchgate.net/publication/45914785_Sparse_Linear_Identifiable_Multivariate_Modeling","abstraction":"In this paper we consider sparse and identifiable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efficient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component delta-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and bench-marked on artificial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identifiable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/."},{"id":413,"title":"Nonparametric learning of dictionaries for sparse representation of sensor signals","url":"https://www.researchgate.net/publication/224114445_Nonparametric_learning_of_dictionaries_for_sparse_representation_of_sensor_signals","abstraction":"Nonparametric Bayesian techniques are considered for learning dictionaries for sparse data representations, with applications in sparse rendering of sensor data. The beta process is employed as a prior for learning the dictionary, and this nonparametric method naturally infers an appropriate dictionary size. The proposed method can learn a sparse dictionary, and may also be used to denoise a signal under test. The noise variance need not be known, and can be non-stationary. The dictionary coefficients for a given sensor signal may be employed within a classifier. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches."},{"id":414,"title":"Nonparametric factor analysis with Beta process priors","url":"https://www.researchgate.net/publication/221346465_Nonparametric_factor_analysis_with_Beta_process_priors","abstraction":"We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets."},{"id":415,"title":"Stochastic Variational Inference","url":"https://www.researchgate.net/publication/228092206_Stochastic_Variational_Inference","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We develop stochastic variational inference, a scalable algorithm for\n <br> approximating posterior distributions. We develop this technique for a large\n <br> class of probabilistic models and we demonstrate it with two probabilistic\n <br> topic models, latent Dirichlet allocation and the hierarchical Dirichlet\n <br> process topic model. Using stochastic variational inference, we analyze several\n <br> large collections of documents: 300K articles from Nature, 1.8M articles from\n <br> The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can\n <br> easily handle data sets of this size and outperforms traditional variational\n <br> inference, which can only handle a smaller subset. (We also show that the\n <br> Bayesian nonparametric topic model outperforms its parametric counterpart.)\n <br> Stochastic variational inference lets us apply complex Bayesian models to\n <br> massive data sets.\n</div> \n<p></p>"},{"id":416,"title":"Time Series Analysis by State Space Methods","url":"https://www.researchgate.net/publication/227468262_Time_Series_Analysis_by_State_Space_Methods","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This new edition updates Durbin &amp; Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.\n</div> \n<p></p>"},{"id":417,"title":"An Introduction to Variational Methods for Graphical Models","url":"https://www.researchgate.net/publication/226435002_An_Introduction_to_Variational_Methods_for_Graphical_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.\n</div> \n<p></p>"},{"id":418,"title":"Robust Stochastic Approximation Approach to Stochastic Programming","url":"https://www.researchgate.net/publication/228699264_Robust_Stochastic_Approximation_Approach_to_Stochastic_Programming","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.\n</div> \n<p></p>"},{"id":419,"title":"Contemporary Bayesian Econometrics and Statistics","url":"https://www.researchgate.net/publication/228538542_Contemporary_Bayesian_Econometrics_and_Statistics","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Tools to improve decision making in an imperfect world This publication provides readers with a thorough understanding of Bayesian analysis that is grounded in the theory of inference and optimal decision making. Contemporary Bayesian Econometrics and Statistics provides readers with state-of-the-art simulation methods and models that are used to solve complex real-world problems. Armed with a strong foundation in both theory and practical problem-solving tools, readers discover how to optimize decision making when faced with problems that involve limited or imperfect data. The book begins by examining the theoretical and mathematical foundations of Bayesian statistics to help readers understand how and why it is used in problem solving. The author then describes how modern simulation methods make Bayesian approaches practical using widely available mathematical applications software. In addition, the author details how models can be applied to specific problems, including: * Linear models and policy choices * Modeling with latent variables and missing data * Time series models and prediction * Comparison and evaluation of models The publication has been developed and fine- tuned through a decade of classroom experience, and readers will find the author's approach very engaging and accessible. There are nearly 200 examples and exercises to help readers see how effective use of Bayesian statistics enables them to make optimal decisions. MATLAB? and R computer programs are integrated throughout the book. An accompanying Web site provides readers with computer code for many examples and datasets. This publication is tailored for research professionals who use econometrics and similar statistical methods in their work. With its emphasis on practical problem solving and extensive use of examples and exercises, this is also an excellent textbook for graduate-level students in a broad range of fields, including economics, statistics, the social sciences, business, and public policy.\n</div> \n<p></p>"},{"id":420,"title":"Explaining Variational Approximations","url":"https://www.researchgate.net/publication/44843129_Explaining_Variational_Approximations","abstraction":"Variational approximations facilitate approximate inference for the parameters in complex statistical models and provide fast, deterministic alternatives to Monte Carlo methods. However, much of the contemporary literature on variational approximations is in Computer Science rather than Statistics, and uses terminology, notation and examples from the former field. In this article we explain variational approximation in statistical terms. In particular, we illustrate the ideas of variational approximation using examples that are familiar to statisticians."},{"id":421,"title":"Graphical Models, Exponential Families, and Variational Inference","url":"https://www.researchgate.net/publication/220416605_Graphical_Models_Exponential_Families_and_Variational_Inference","abstraction":"The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models."},{"id":422,"title":"Expectation Propagation for Approximate Bayesian Inference","url":"https://www.researchgate.net/publication/234108836_Expectation_Propagation_for_Approximate_Bayesian_Inference","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a new deterministic approximation technique in Bayesian\n <br> networks. This method, \"Expectation Propagation\", unifies two previous\n <br> techniques: assumed-density filtering, an extension of the Kalman filter, and\n <br> loopy belief propagation, an extension of belief propagation in Bayesian\n <br> networks. All three algorithms try to recover an approximate distribution which\n <br> is close in KL divergence to the true distribution. Loopy belief propagation,\n <br> because it propagates exact belief states, is useful for a limited class of\n <br> belief networks, such as those which are purely discrete. Expectation\n <br> Propagation approximates the belief states by only retaining certain\n <br> expectations, such as mean and variance, and iterates until these expectations\n <br> are consistent throughout the network. This makes it applicable to hybrid\n <br> networks with discrete and continuous nodes. Expectation Propagation also\n <br> extends belief propagation in the opposite direction - it can propagate richer\n <br> belief states that incorporate correlations between nodes. Experiments with\n <br> Gaussian mixture models show Expectation Propagation to be convincingly better\n <br> than methods with similar computational cost: Laplace's method, variational\n <br> Bayes, and Monte Carlo. Expectation Propagation also provides an efficient\n <br> algorithm for training Bayes point machine classifiers.\n</div> \n<p></p>"},{"id":423,"title":"Fast collapsed Gibbs sampling for latent Dirichlet allocation","url":"https://www.researchgate.net/publication/221653277_Fast_collapsed_Gibbs_sampling_for_latent_Dirichlet_allocation","abstraction":"In this paper we introduce a novel collapsed Gibbs sam- pling method for the widely used latent Dirichlet alloca- tion (LDA) model. Our new method results in significant speedups on real world text corpora. Conventional Gibbs sampling schemes for LDA require O(K) operations per sam- ple where K is the number of topics in the model. Our proposed method draws equivalent samples but requires on average significantly less then K operations per sample. On real-word corpora FastLDA can be as much as 8 times faster than the standard collapsed Gibbs sampler for LDA. No ap- proximations are necessary, and we show that our fast sam- pling scheme produces exactly the same results as the stan- dard (but slower) sampling scheme. Experiments on four real world data sets demonstrate speedups for a wide range of collection sizes. For the PubMed collection of over 8 mil- lion documents with a required computation time of 6 CPU months for LDA, our speedup of 5.7 can save 5 CPU months of computation."},{"id":424,"title":"Kullback-Leibler Proximal Variational Inference","url":"https://www.researchgate.net/publication/287209198_Kullback-Leibler_Proximal_Variational_Inference","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a new variational inference method based on the Kullback-Leibler (KL) proximal term. We make two contributions towards improving efficiency of variational inference. Firstly, we derive a KL proximal-point algorithm and show its equivalence to gradient descent with natural gradient in stochastic variational inference. Secondly, we use the proximal framework to derive efficient variational algorithms for non-conjugate models. We propose a splitting procedure to separate non-conjugate terms from conjugate ones. We then linearize the non-conjugate terms and show that the resulting subproblem admits a closed-form solution. Overall, our approach converts a non-conjugate model to subproblems that involve inference in well-known conjugate models. We apply our method to many models and derive generalizations for non-conjugate exponential family. Applications to real-world datasets show that our proposed algorithms are easy to implement, fast to converge, perform well, and reduce computations.\n</div> \n<p></p>"},{"id":425,"title":"Local Expectation Gradients for Doubly Stochastic Variational Inference","url":"https://www.researchgate.net/publication/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference","abstraction":"We introduce local expectation gradients which is a general purpose stochastic variational inference algorithm for constructing stochastic gradients through sampling from the variational distribution. This algorithm divides the problem of estimating the stochastic gradients over multiple variational parameters into smaller sub-tasks so that each sub-task exploits intelligently the information coming from the most relevant part of the variational distribution. This is achieved by performing an exact expectation over the single random variable that mostly correlates with the variational parameter of interest resulting in a Rao-Blackwellized estimate that has low variance and can work efficiently for both continuous and discrete random variables. Furthermore, the proposed algorithm has interesting similarities with Gibbs sampling but at the same time, unlike Gibbs sampling, it can be trivially parallelized."},{"id":426,"title":"Parallel variational Bayes for large datasets with an application to generalized linear mixed models","url":"https://www.researchgate.net/publication/253330477_Parallel_variational_Bayes_for_large_datasets_with_an_application_to_generalized_linear_mixed_models","abstraction":"The article develops a hybrid Variational Bayes algorithm that combines the mean-field and fixed-form Variational Bayes methods. The new estimation algorithm can be used to approximate any posterior without relying on conjugate priors. We propose a divide and recombine strategy for the analysis of large datasets, which partitions a large dataset into smaller pieces and then combines the variational distributions that have been learnt in parallel on each separate piece using the hybrid Variational Bayes algorithm. The proposed method is applied to fitting generalized linear mixed models. The computational efficiency of the parallel and hybrid Variational Bayes algorithm is demonstrated on several simulated and real datasets."},{"id":427,"title":"Markov Chain Monte Carlo and Variational Inference: Bridging the Gap","url":"https://www.researchgate.net/publication/267338836_Markov_Chain_Monte_Carlo_and_Variational_Inference_Bridging_the_Gap","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Recent advances in stochastic variational inference have made it possible to\n <br> construct variational posterior approximations containing auxiliary random\n <br> variables. This enables us to explore a new synthesis of variational inference\n <br> and Monte Carlo methods where we incorporate one or more steps of MCMC into our\n <br> variational approximation. This note describes the theoretical foundations that\n <br> make this possible and shows some promising first results.\n</div> \n<p></p>"},{"id":428,"title":"Implementing and Automating Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression","url":"https://www.researchgate.net/publication/259625316_Implementing_and_Automating_Fixed-Form_Variational_Posterior_Approximation_through_Stochastic_Linear_Regression","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We recently proposed a general algorithm for approximating nonstandard\n <br> Bayesian posterior distributions by minimization of their Kullback-Leibler\n <br> divergence with respect to a more convenient approximating distribution. In\n <br> this note we offer details on how to efficiently implement this algorithm in\n <br> practice. We also suggest default choices for the form of the posterior\n <br> approximation, the number of iterations, the step size, and other user choices.\n <br> By using these defaults it becomes possible to construct good posterior\n <br> approximations for hierarchical models completely automatically.\n</div> \n<p></p>"},{"id":429,"title":"On Using Control Variates with Stochastic Approximation for Variational Bayes and its Connection to Stochastic Linear Regression","url":"https://www.researchgate.net/publication/259578079_On_Using_Control_Variates_with_Stochastic_Approximation_for_Variational_Bayes_and_its_Connection_to_Stochastic_Linear_Regression","abstraction":"Recently, we and several other authors have written about the possibilities of using stochastic approximation techniques for fitting variational approximations to intractable Bayesian posterior distributions. Naive implementations of stochastic approximation suffer from high variance in this setting. Several authors have therefore suggested using control variates to reduce this variance, while we have taken a different but analogous approach to reducing the variance which we call stochastic linear regression. In this note we take the former perspective and derive the ideal set of control variates for stochastic approximation variational Bayes under a certain set of assumptions. We then show that using these control variates is closely related to using the stochastic linear regression approximation technique we proposed earlier. A toy example shows that our method for constructing control variates leads to stochastic estimators with much lower variance."},{"id":430,"title":"A Stochastic Variational Framework for Fitting and Diagnosing Generalized Linear Mixed Models","url":"https://www.researchgate.net/publication/230732835_A_Stochastic_Variational_Framework_for_Fitting_and_Diagnosing_Generalized_Linear_Mixed_Models","abstraction":"In stochastic variational inference, the variational Bayes objective function is optimized using stochastic gradient approximation, where gradients computed on small random subsets of data are used to approximate the true gradient over the whole data set. This enables complex models to be fit to large data sets as data can be processed in mini-batches. In this article, we extend stochastic variational inference for conjugate-exponential models to nonconjugate models and present a stochastic nonconjugate variational message passing algorithm for fitting generalized linear mixed models that is scalable to large data sets. In addition, we show that diagnostics for prior-likelihood conflict, which are useful for Bayesian model criticism, can be obtained from nonconjugate variational message passing automatically, as an alternative to simulation-based Markov chain Monte Carlo methods. Finally, we demonstrate that for moderate-sized data sets, convergence can be accelerated by using the stochastic version of nonconjugate variational message passing in the initial stage of optimization before switching to the standard version."},{"id":431,"title":"A Bayesian Non-Parametric Approach for Mapping Dynamic Quantitative Traits.","url":"https://www.researchgate.net/publication/239073675_A_Bayesian_Non-Parametric_Approach_for_Mapping_Dynamic_Quantitative_Traits","abstraction":"In biology, many quantitative traits are dynamic in nature. They can often be described by some smooth functions or curves. A joint analysis of all the repeated measurements of the dynamic traits by functional quantitative trait loci (QTL) mapping methods has the benefits to (1) understand the genetic control of the whole dynamic process of the quantitative traits, and (2) improve the statistical power to detect QTLs. One crucial issue in functional QTL mapping is how to correctly describe the smoothness of trajectories of functional valued traits. We develop an efficient Bayesian non-parametric multiple-loci procedure for mapping dynamic traits. The method uses the Bayesian P-splines with (non-parametric) B-spline bases to specify the functional form of a QTL trajectory, and a random walk prior to automatically determine its degree of smoothness. An efficient deterministic variational Bayes algorithm is used to implement both (1) search an optimal subset of QTLs among large marker panels, and (2) estimate the genetic effects of the selected QTLs changing over time. Our method can be fast even on some large scale data sets. The advantages of our method are illustrated on both simulated and real data sets."},{"id":432,"title":"Stochastic variational inference for large-scale discrete choice models using adaptive batch sizes","url":"https://www.researchgate.net/publication/262569028_Stochastic_variational_inference_for_large-scale_discrete_choice_models_using_adaptive_batch_sizes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Discrete choice models describe the choices made by decision makers among\n <br> alternatives and play an important role in transportation planning, marketing\n <br> research and other applications. The mixed multinomial logit (MMNL) model is a\n <br> popular discrete choice model that captures heterogeneity in the preferences of\n <br> decision makers through random coefficients. While Markov chain Monte Carlo\n <br> methods provide the Bayesian analogue to classical procedures for estimating\n <br> MMNL models, computations can be prohibitively expensive for large datasets.\n <br> Approximate inference can be obtained using variational methods at a lower\n <br> computational cost with competitive accuracy. In this paper, we develop\n <br> variational methods for estimating MMNL models that allow random coefficients\n <br> to be correlated in the posterior and can be extended to large-scale datasets.\n <br> We explore three alternatives: (1) Laplace variational inference, (2)\n <br> nonconjugate variational message passing and (3) stochastic linear regression,\n <br> and compare their performances using real and simulated data. To accelerate\n <br> convergence for large datasets, we develop stochastic variational inference for\n <br> MMNL models using each of the above three alternatives. Stochastic variational\n <br> inference allows data to be processed in minibatches by optimizing global\n <br> variational parameters using stochastic gradient approximation. A novel\n <br> strategy for increasing minibatch sizes adaptively within stochastic\n <br> variational inference is proposed.\n</div> \n<p></p>"},{"id":433,"title":"Variational inferences for partially linear additive models with variable selection","url":"https://www.researchgate.net/publication/264624370_Variational_inferences_for_partially_linear_additive_models_with_variable_selection","abstraction":"This article develops a mean field variational Bayes approximation algorithm for posterior inferences of the recently proposed partially linear additive models with simultaneous and automatic variable selection and linear/nonlinear component identification abilities. To solve the problem induced by some complicated expectation evaluations, we proposed two approximations based on Monte Carlo method and Laplace approximation respectively. With high accuracy, the algorithm we derived is much more computationally efficient than the existing Markov Chain Monte Carlo (MCMC) method. The simulation examples are used to demonstrate the performance of our new algorithm versus MCMC. The proposed approach is further illustrated on a real dataset."},{"id":434,"title":"Bayesian approaches to Gaussian modeling","url":"https://www.researchgate.net/publication/3192912_Bayesian_approaches_to_Gaussian_modeling","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A Bayesian-based methodology is presented which automatically\n <br> penalizes overcomplex models being fitted to unknown data. We show that,\n <br> with a Gaussian mixture model, the approach is able to select an\n <br> “optimal” number of components in the model and so partition\n <br> data sets. The performance of the Bayesian method is compared to other\n <br> methods of optimal model selection and found to give good results. The\n <br> methods are tested on synthetic and real data sets\n</div> \n<p></p>"},{"id":435,"title":"A Unifying Review of Linear Gaussian Models","url":"https://www.researchgate.net/publication/13338029_A_Unifying_Review_of_Linear_Gaussian_Models","abstraction":"Factor analysis, principal component analysis, mixtures of gaussian clusters, vector quantization, Kalman filter models, and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model. This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity. Through the use of other nonlinearities, we show how independent component analysis is also a variation of the same basic generative model. We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term. We introduce a new model for static data, known as sensible principal component analysis, as well as a novel concept of spatially adaptive observation noise. We also review some of the literature involving global and local mixtures of the basic models and provide pseudocode for inference and learning for all the basic models."},{"id":436,"title":"Mixtures of Probabilistic Principal Component Analyzers","url":"https://www.researchgate.net/publication/13338034_Mixtures_of_Probabilistic_Principal_Component_Analyzers","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Principal component analysis (PCA) is one of the most popular techniques for processing, compressing, and visualizing data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Therefore, previous attempts to formulate mixture models for PCA have been ad hoc to some extent. In this article, PCA is formulated within a maximum likelihood framework, based on a specific form of gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analyzers, whose parameters can be determined using an expectation-maximization algorithm. We discuss the advantages of this model in the context of clustering, density modeling, and local dimensionality reduction, and we demonstrate its application to image compression and handwritten digit recognition.\n</div> \n<p></p>"},{"id":437,"title":"SMEM algorithm for mixture models. Neural Comput","url":"https://www.researchgate.net/publication/221620379_SMEM_algorithm_for_mixture_models_Neural_Comput","abstraction":"We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models, local maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations, we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split- and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems."},{"id":438,"title":"Assessing Relevance Determination Methods Using DELVE","url":"https://www.researchgate.net/publication/2342884_Assessing_Relevance_Determination_Methods_Using_DELVE","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n . Empirically assessing the predictive performance of learning methods is an essential component of research in machine learning. The DELVE environment was developed to support such assessments. It provides a collection of datasets, a standard approach to conducting experiments with these datasets, and software for the statistical analysis of experimental results. In this paper, DELVE is used to assess the performance of neural network methods when the inputs available to the network have varying degrees of relevance. The results confirm that the Bayesian method of \"Automatic Relevance Determination\" (ARD) is often (but not always) helpful, and show that a variation on \"early stopping\" inspired by ARD is also beneficial. The experiments also reveal some other interesting characteristics of the methods tested. This example illustrates the essential role of empirical testing, and shows the strengths and weaknesses of the DELVE environment. 1 Introduction This paper has two purposes: To ...\n</div> \n<p></p>"},{"id":439,"title":"Adaptive Mixtures of Factor Analyzers","url":"https://www.researchgate.net/publication/280034147_Adaptive_Mixtures_of_Factor_Analyzers","abstraction":"A mixture of factor analyzers is a semi-parametric density estimator that generalizes the well-known mixtures of Gaussians model by allowing each Gaussian in the mixture to be represented in a different lower-dimensional manifold. This paper presents a robust and parsimonious model selection algorithm for training a mixture of factor analyzers, carrying out simultaneous clustering and locally linear, globally nonlinear dimensionality reduction. Permitting different number of factors per mixture component, the algorithm adapts the model complexity to the data complexity. We compare the proposed algorithm with related automatic model selection algorithms on a number of benchmarks. The results indicate the effectiveness of this fast and robust approach in clustering, manifold learning and class-conditional modeling."},{"id":440,"title":"Variational Inference of Kalman Filter and Its Application in Wireless Sensor Networks","url":"https://www.researchgate.net/publication/286277948_Variational_Inference_of_Kalman_Filter_and_Its_Application_in_Wireless_Sensor_Networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n An improved Kalman filter algorithm by using variational inference (VIKF) is proposed. With variational method, the joint posterior distribution of the states is approximately decomposed into several relatively independent posterior distributions. To avoid the difficulty of high-dimensional integrals, these independent posterior distributions are solved by using Kullback-Leibler divergence. The variational inference of Kalman filter includes two steps, the predict step and the update step, and an iterative process is included in the update step to get the optimized solutions of the posterior distribution. To verify the effectiveness of the proposed algorithm, VIKF is applied to the state estimation of discrete linear state space and the tracking problems in wireless sensor networks. Simulation results show that the variational approximation is effective and reliable for the linear state space, especially for the case with time-varying non-Gaussian noise.\n</div> \n<p></p>"},{"id":441,"title":"Variational Semi-blind Sparse Deconvolution with Orthogonal Kernel Bases and its Application to MRFM","url":"https://www.researchgate.net/publication/258817506_Variational_Semi-blind_Sparse_Deconvolution_with_Orthogonal_Kernel_Bases_and_its_Application_to_MRFM","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present a variational Bayesian method of joint image reconstruction\n <br> and point spread function (PSF) estimation when the PSF of the imaging\n <br> device is only partially known. To solve this semi-blind deconvolution\n <br> problem, prior distributions are specified for the PSF and the 3D image.\n <br> Joint image reconstruction and PSF estimation is then performed within a\n <br> Bayesian framework, using a variational algorithm to estimate the\n <br> posterior distribution. The image prior distribution imposes an explicit\n <br> atomic measure that corresponds to image sparsity. Importantly, the\n <br> proposed Bayesian deconvolution algorithm does not require hand tuning.\n <br> Simulation results clearly demonstrate that the semi-blind deconvolution\n <br> algorithm compares favorably with previous Markov chain Monte Carlo\n <br> (MCMC) version of myopic sparse reconstruction. It significantly\n <br> outperforms mismatched non-blind algorithms that rely on the assumption\n <br> of the perfect knowledge of the PSF. The algorithm is illustrated on\n <br> real data from magnetic resonance force microscopy (MRFM).\n</div> \n<p></p>"},{"id":442,"title":"Variational MCMC","url":"https://www.researchgate.net/publication/234108809_Variational_MCMC","abstraction":"We propose a new class of learning algorithms that combines variational approximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithms that use the variational approximation as proposal distribution can perform poorly because this approximation tends to underestimate the true variance and other features of the data. We solve this problem by introducing more sophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMC kernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH) kernel with a variational approximation as proposaldistribution. The MH kernel allows one to locate regions of high probability efficiently. The Metropolis kernel allows us to explore the vicinity of these regions. This algorithm outperforms variationalapproximations because it yields slightly better estimates of the mean and considerably better estimates of higher moments, such as covariances. It also outperforms standard MCMC algorithms because it locates theregions of high probability quickly, thus speeding up convergence. We demonstrate this algorithm on the problem of Bayesian parameter estimation for logistic (sigmoid) belief networks."},{"id":443,"title":"Regression Density Estimation With Variational Methods and Stochastic Approximation","url":"https://www.researchgate.net/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Regression density estimation is the problem of flexibly estimating a response distribution as a function of covariates. An important approach to regression density estimation uses finite mixture models and our article considers flexible mixtures of heteroscedastic regression (MHR) models where the response distribution is a normal mixture, with the component means, variances and mixture weights all varying as a function of covariates. Our article develops fast variational approximation methods for inference. Our motivation is that alternative computationally intensive MCMC methods for fitting mixture models are difficult to apply when it is desired to fit models repeatedly in exploratory analysis and model choice. Our article makes three contributions. First, a variational approximation for MHR models is described where the variational lower bound is in closed form. Second, the basic approximation can be improved by using stochastic approximation methods to perturb the initial solution to attain higher accuracy. Third, the advantages of our approach for model choice and evaluation compared to MCMC based approaches are illustrated. These advantages are particularly compelling for time series data where repeated refitting for one step ahead prediction in model choice and diagnostics and in rolling window computations is very common. Supplemental materials for the article are available online.\n</div> \n<p></p>"},{"id":444,"title":"Flexible Modeling of Latent Task Structures in Multitask Learning","url":"https://www.researchgate.net/publication/228095688_Flexible_Modeling_of_Latent_Task_Structures_in_Multitask_Learning","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Multitask learning algorithms are typically designed assuming some fixed, a\n <br> priori known latent structure shared by all the tasks. However, it is usually\n <br> unclear what type of latent task structure is the most appropriate for a given\n <br> multitask learning problem. Ideally, the \"right\" latent task structure should\n <br> be learned in a data-driven manner. We present a flexible, nonparametric\n <br> Bayesian model that posits a mixture of factor analyzers structure on the\n <br> tasks. The nonparametric aspect makes the model expressive enough to subsume\n <br> many existing models of latent task structures (e.g, mean-regularized tasks,\n <br> clustered tasks, low-rank or linear/non-linear subspace assumption on tasks,\n <br> etc.). Moreover, it can also learn more general task structures, addressing the\n <br> shortcomings of such models. We present a variational inference algorithm for\n <br> our model. Experimental results on synthetic and real-world datasets, on both\n <br> regression and classification problems, demonstrate the effectiveness of the\n <br> proposed method.\n</div> \n<p></p>"},{"id":445,"title":"On smoothing and inference for topic models","url":"https://www.researchgate.net/publication/228884043_On_smoothing_and_inference_for_topic_models","abstraction":"Latent Dirichlet analysis, or topic modeling, is a flexible latent variable framework for model-ing high-dimensional sparse count data. Various learning algorithms have been developed in re-cent years, including collapsed Gibbs sampling, variational inference, and maximum a posteriori estimation, and this variety motivates the need for careful empirical comparisons. In this paper, we highlight the close connections between these approaches. We find that the main differences are attributable to the amount of smoothing applied to the counts. When the hyperparameters are op-timized, the differences in performance among the algorithms diminish significantly. The ability of these algorithms to achieve solutions of com-parable accuracy gives us the freedom to select computationally efficient approaches. Using the insights gained from this comparative study, we show how accurate topic models can be learned in several seconds on text corpora with thousands of documents."},{"id":446,"title":"Hierarchical Semantic Processing Architecture for Smart Sensors in Surveillance Networks","url":"https://www.researchgate.net/publication/235991987_Hierarchical_Semantic_Processing_Architecture_for_Smart_Sensors_in_Surveillance_Networks","abstraction":"Data acquisition by multi-domain data acquisition provides means for environment perception usable for detecting unusual and possibly dangerous situations. When being automated, this approach can simplify surveillance tasks required in, for example, airports or other security sensitive infrastructures. This paper describes a novel architecture for surveillance networks based on combining multimodal sensor information. Compared to previous methodologies using only video information, the proposed approach also uses audio data thus increasing its ability to obtain valuable information about the sensed environment. A hierarchical processing architecture for observation and surveillance systems is proposed, which reco-gnizes a set of pre-defined behaviors and learns about normal behaviors. Deviations from “normality” are reported in a way understandable even for staff without special training. The processing architecture, including the physical sensor nodes, is called SENSE (smart embedded network of sensing entities). Parts of this work have been published previously; the main enhancements of this paper compared to previous publications are detailed descriptions of the layers 1 and 4, “pre-processing including plausibility checks” and “parameter inference”. In the other layers, details not necessary for a general understanding of the approach have been omitted."},{"id":447,"title":"Asynchronous Distributed Learning of Topic Models.","url":"https://www.researchgate.net/publication/221619032_Asynchronous_Distributed_Learning_of_Topic_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning al- gorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors indepen- dently perform Gibbs sampling on their local data and communicate their infor- mation in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard L DA and HDP samplers, but with significant improvements in computation time and me mory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processor s. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced.\n</div> \n<p></p>"},{"id":448,"title":"A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis","url":"https://www.researchgate.net/publication/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Unsupervised feature learning algorithms based on convolutional formulations\n <br> of independent components analysis (ICA) have been demonstrated to yield\n <br> state-of-the-art results in several action recognition benchmarks. However,\n <br> existing approaches do not allow for the number of latent components (features)\n <br> to be automatically inferred from the data in an unsupervised manner. This is a\n <br> significant disadvantage of the state-of-the-art, as it results in considerable\n <br> burden imposed on researchers and practitioners, who must resort to tedious\n <br> cross-validation procedures to obtain the optimal number of latent features. To\n <br> resolve these issues, in this paper we introduce a convolutional nonparametric\n <br> Bayesian sparse ICA architecture for overcomplete feature learning from\n <br> high-dimensional data. Our method utilizes an Indian buffet process prior to\n <br> facilitate inference of the appropriate number of latent features under a\n <br> hybrid variational inference algorithm, scalable to massive datasets. As we\n <br> show, our model can be naturally used to obtain deep unsupervised hierarchical\n <br> feature extractors, by greedily stacking successive model layers, similar to\n <br> existing approaches. In addition, inference for this model is completely\n <br> heuristics-free; thus, it obviates the need of tedious parameter tuning, which\n <br> is a major challenge most deep learning approaches are faced with. We evaluate\n <br> our method on several action recognition benchmarks, and exhibit its advantages\n <br> over the state-of-the-art.\n</div> \n<p></p>"},{"id":449,"title":"Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation","url":"https://www.researchgate.net/publication/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In the internet era there has been an explosion in the amount of digital text\n <br> information available, leading to difficulties of scale for traditional\n <br> inference algorithms for topic models. Recent advances in stochastic\n <br> variational inference algorithms for latent Dirichlet allocation (LDA) have\n <br> made it feasible to learn topic models on large-scale corpora, but these\n <br> methods do not currently take full advantage of the collapsed representation of\n <br> the model. We propose a stochastic algorithm for collapsed variational Bayesian\n <br> inference for LDA, which is simpler and more efficient than the state of the\n <br> art method. We show connections between collapsed variational Bayesian\n <br> inference and MAP estimation for LDA, and leverage these connections to prove\n <br> convergence properties of the proposed algorithm. In experiments on large-scale\n <br> text corpora, the algorithm was found to converge faster and often to a better\n <br> solution than the previous method. Human-subject experiments also demonstrated\n <br> that the method can learn coherent topics in seconds on small corpora,\n <br> facilitating the use of topic models in interactive document analysis software.\n</div> \n<p></p>"},{"id":450,"title":"Online Belief Propagation for Topic Modeling","url":"https://www.researchgate.net/publication/232063267_Online_Belief_Propagation_for_Topic_Modeling","abstraction":"The batch latent Dirichlet allocation (LDA) algorithms play important roles in probabilistic topic modeling, but they are not suitable for processing big data streams due to high time and space compleixty. Online LDA algorithms can not only extract topics from big data streams with constant memory requirements, but also detect topic shifts as the data stream flows. In this paper, we present a novel and easy-to-implement online belief propagation (OBP) algorithm that infers the topic distribution from the previously unseen documents incrementally within the stochastic approximation framework. We discuss intrinsic relations between OBP and online expectation-maximization (OEM) algorithms, and show that OBP can converge to the local stationary point of the LDA's likelihood function. Extensive empirical studies confirm that OBP significantly reduces training time and memory usage while achieves a much lower predictive perplexity when compared with current state-of-the-art online LDA algorithms. Due to its ease of use, fast speed and low memory usage, OBP is a strong candidate for becoming the standard online LDA algorithm."},{"id":451,"title":"Supervised dimension reduction with topic models","url":"https://www.researchgate.net/publication/268435721_Supervised_dimension_reduction_with_topic_models","abstraction":"We consider supervised dimension reduction (SDR) for problems with discrete variables. Existing methods are computationally expensive, and often do not take the local structure of data into consideration when searching for a low-dimensional space. In this paper, we propose a novel framework for SDR which is (1) general and flexible so that it can be easily adapted to various unsupervised topic models, (2) able to inherit scalability of unsupervised topic models, and (3) can exploit well label information and local structure of data when searching for a new space. Extensive experiments with adaptations to three models demonstrate that our framework can yield scalable and qualitative methods for SDR. One of those adaptations can perform better than the state-of-the-art method for SDR while enjoying significantly faster speed."},{"id":452,"title":"Interactive Topic Modeling.","url":"https://www.researchgate.net/publication/220874072_Interactive_Topic_Modeling","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Topic models are a useful and ubiquitous tool for understanding large corpora. However, topic models are not perfect, and for many users in computational social science, digital humanities, and information studies—who are not machine learning experts—existing models and frameworks are often a “take it or leave it” proposition. This paper presents a mechanism for giving users a voice by encoding users’ feedback to topic models as correlations between words into a topic model. This framework, interactive topic modeling (itm), allows untrained users to encode their feedback easily and iteratively into the topic models. Because latency in interactive systems is crucial, we develop more efficient inference algorithms for tree-based topic models. We validate the framework both with simulated and real users.\n</div> \n<p></p>"},{"id":453,"title":"FastEx: Fast Clustering with Exponential Families","url":"https://www.researchgate.net/publication/230786935_FastEx_Fast_Clustering_with_Exponential_Families","abstraction":"Clustering is a key component in any data analysis toolbox. Despite its importance, scalable algorithms often eschew rich statistical models in favor of simpler descriptions such as k-means clustering. In this paper we present a sampler, capable of estimating mixtures of exponential families. At its heart lies a novel proposal distribution using random projections to achieve high throughput in generating proposals, which is crucial for clustering models with large numbers of clusters."},{"id":454,"title":"Scalable Probabilistic Entity-Topic Modeling","url":"https://www.researchgate.net/publication/256327429_Scalable_Probabilistic_Entity-Topic_Modeling","abstraction":"We present an LDA approach to entity disambiguation. Each topic is associated with a Wikipedia article and topics generate either content words or entity mentions. Training such models is challenging because of the topic and vocabulary size, both in the millions. We tackle these problems using a novel distributed inference and representation framework based on a parallel Gibbs sampler guided by the Wikipedia link graph, and pipelines of MapReduce allowing fast and memory-frugal processing of large datasets. We report state-of-the-art performance on a public dataset."},{"id":455,"title":"Trawling in the Sea of the Great Unread: Sub-corpus topic modeling and Humanities research","url":"https://www.researchgate.net/publication/259125814_Trawling_in_the_Sea_of_the_Great_Unread_Sub-corpus_topic_modeling_and_Humanities_research","abstraction":"Given a small, well-understood corpus that is of interest to a Humanities scholar, we propose sub-corpus topic modeling (STM) as a tool for discovering meaningful passages in a larger collection of less well-understood texts. STM allows Humanities scholars to discover unknown passages from the vast sea of works that Moretti calls the “great unread” and to significantly increase the researcher's ability to discuss aspects of influence and the development of intellectual movements across a broader swath of the literary landscape. In this article, we test three typical Humanities research problems: in the first, a researcher wants to find text passages that exhibit similarities to a collection of influential non literary texts from a single author (here, Darwin); in the second, a researcher wants to discover literary passages related to a well understood corpus of literary texts (here, emblematic texts from the Modern Breakthrough); and in the third, a researcher hopes to understand the influence that a particular domain (here, folklore) has had on the realm of literature over a series of decades. We explore these research challenges with three experiments."},{"id":456,"title":"A Bayesian Nonparametric Approach to Reliability","url":"https://www.researchgate.net/publication/38358679_A_Bayesian_Nonparametric_Approach_to_Reliability","abstraction":"It is suggested that problems in a reliability context may be handled by a Bayesian nonparametric approach. A stochastic process is defined whose sample paths may be assumed to be increasing hazard rates by properly choosing the parameter functions of the process. The posterior distribution of the hazard rates is derived for both exact and censored data. Bayes estimates of hazard rates and $\\operatorname{cdf's}$ are found under squared error type loss functions. Some simulation is done and estimates graphed to better understand the estimators. Finally, estimates of the hazard rate from some data in a paper by Kaplan and Meier are constructed."},{"id":457,"title":"On Inconsistent Bayes Estimates of Location","url":"https://www.researchgate.net/publication/38359983_On_Inconsistent_Bayes_Estimates_of_Location","abstraction":"In some relatively natural settings, Bayes estimates of location are shown to be inconsistent."},{"id":458,"title":"Bayes Estimation from a Markov Renewal Process","url":"https://www.researchgate.net/publication/38359396_Bayes_Estimation_from_a_Markov_Renewal_Process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A procedure for Bayes nonparametric estimation from a Markov renewal process is developed. It is based on a conjugate class of a priori distributions on the parameter space of semi-Markov transition distributions. The class is characterized by a Dirichlet family of distributions for random Markov matrices and a Beta family of Levy processes for random cumulative hazard functions. The main result is the derivation of the posterior law from an observation of the Markov renewal process over a period of time.\n</div> \n<p></p>"},{"id":459,"title":"Nonparametric Estimation of a Survivorship Function with Doubly Censored Data","url":"https://www.researchgate.net/publication/265329375_Nonparametric_Estimation_of_a_Survivorship_Function_with_Doubly_Censored_Data","abstraction":"A simple iterative procedure is proposed for obtaining estimates of a response time distribution when some of the data are censored on the left and some on the right. The procedure is based on the product-limit method of Kaplan and Meier [15], and it also uses the idea of self-consistency due to Efron [8]. Under fairly general assumptions, the method is shown to yield unique consistent maximum likelihood estimators. Asymptotic expressions for their variances and covariances are derived and an extension to the case of arbitrary censoring is suggested."},{"id":460,"title":"A Note on a Paper by Ferguson and Phadia","url":"https://www.researchgate.net/publication/38358755_A_Note_on_a_Paper_by_Ferguson_and_Phadia","abstraction":"Ferguson and Phadia have recently discussed the nonparametric Bayesian estimation of a distribution function from a right-censored random sample using process priors that are neutral to the right. The more general problem of estimating the baseline distribution function with right censored data from the proportional hazards model has been studied by Kalbfleisch who uses the more restrictive class of gamma process priors. This note shows that, with a simple modification, the analysis of Ferguson and Phadia can be extended to deal with the proportional hazards situation with constant covariates. Extensions to time dependent covariates and other regression models are also considered."},{"id":461,"title":"Prior Distributions on Spaces of Probability Measures","url":"https://www.researchgate.net/publication/38357812_Prior_Distributions_on_Spaces_of_Probability_Measures","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Methods of generating prior distributions on spaces of probability measures for use in Bayesian nonparametric inference are reviewed with special emphasis on the Dirichlet processes, the tailfree processes, and processes neutral to the right. Some applications are given.\n</div> \n<p></p>"},{"id":462,"title":"Bayesian Nonparametric Estimation Based on Censored Data","url":"https://www.researchgate.net/publication/38358395_Bayesian_Nonparametric_Estimation_Based_on_Censored_Data","abstraction":"Let $X_1, \\cdots, X_n$ be a random sample from an unknown $\\operatorname{cdf} F$, let $y_1, \\cdots, y_n$ be known real constants, and let $Z_i = \\min(X_i, y_i), i = 1, \\cdots, n$. It is required to estimate $F$ on the basis of the observations $Z_1, \\cdots, Z_n$, when the loss is squared error. We find a Bayes estimate of $F$ when the prior distribution of $F$ is a process neutral to the right. This generalizes results of Susarla and Van Ryzin who use a Dirichlet process prior. Two types of censoring are introduced--the inclusive and exclusive types--and the class of maximum likelihood estimates which thus generalize the product limit estimate of Kaplan and Meier is exhibited. The modal estimate of $F$ for a Dirichlet process prior is found and related to work of Ramsey. In closing, an example illustrating the techniques is given."},{"id":463,"title":"Scaled subordinators and generalizations of the Indian buffet process","url":"https://www.researchgate.net/publication/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We study random families of subsets of $\\mathbb{N}$ that are similar to\n <br> exchangeable random partitions, but do not require constituent sets to be\n <br> disjoint: Each element of ${\\mathbb{N}}$ may be contained in multiple subsets.\n <br> One class of such objects, known as Indian buffet processes, has become a\n <br> popular tool in machine learning. Based on an equivalence between Indian buffet\n <br> and scale-invariant Poisson processes, we identify a random scaling variable\n <br> whose role is similar to that played in exchangeable partition models by the\n <br> total mass of a random measure. Analogous to the construction of exchangeable\n <br> partitions from normalized subordinators, random families of sets can be\n <br> constructed from randomly scaled subordinators. Coupling to a heavy-tailed\n <br> scaling variable induces a power law on the number of sets containing the first\n <br> $n$ elements. Several examples, with properties desirable in applications, are\n <br> derived explicitly. A relationship to exchangeable partitions is made precise\n <br> as a correspondence between scaled subordinators and Poisson-Kingman measures,\n <br> generalizing a result of Arratia, Barbour and Tavare on scale-invariant\n <br> processes.\n</div> \n<p></p>"},{"id":464,"title":"Markov survival processes and proportional-hazards regression","url":"https://www.researchgate.net/publication/268689765_Markov_survival_processes_and_proportional-hazards_regression","abstraction":"We explore the concept of a consistent exchangeable survival process - a joint distribution of survival times in which the risk set evolves as a continuous-time Markov chain with homogeneous transition rates. We show a correspondence with the de Finetti approach of constructing an exchangeable survival process by generating iid survival times conditional on a completely independent hazard measure. We describe several specific processes, showing how the number of blocks of tied failure times grows asymptotically with the number of individuals in each case. We end by applying these methods to data, showing how they can be easily extended to handle censoring and inhomogeneity among patients."},{"id":465,"title":"Beta-Negative Binomial Process and Exchangeable Random Partitions for Mixed-Membership Modeling","url":"https://www.researchgate.net/publication/267570378_Beta-Negative_Binomial_Process_and_Exchangeable_Random_Partitions_for_Mixed-Membership_Modeling","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The beta-negative binomial process (BNBP), an integer-valued stochastic\n <br> process, is employed to partition a count vector into a latent random count\n <br> matrix. As the marginal probability distribution of the BNBP that governs the\n <br> exchangeable random partitions of grouped data has not yet been developed,\n <br> current inference for the BNBP has to truncate the number of atoms of the beta\n <br> process. This paper introduces an exchangeable partition probability function\n <br> to explicitly describe how the BNBP clusters the data points of each group into\n <br> a random number of exchangeable partitions, which are shared across all the\n <br> groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a\n <br> novel nonparametric Bayesian topic model that is distinct from existing ones,\n <br> with simple implementation, fast convergence, good mixing, and state-of-the-art\n <br> predictive performance.\n</div> \n<p></p>"},{"id":466,"title":"Learning the Structure of Probabilistic Graphical Models with an Extended Cascading Indian Buffet Process","url":"https://www.researchgate.net/publication/261404006_Learning_the_Structure_of_Probabilistic_Graphical_Models_with_an_Extended_Cascading_Indian_Buffet_Process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents an extension of the cascading Indian buffet process (CIBP) intended to learning arbitrary directed acyclic graph structures as opposed to the CIBP, which is limited to purely layered structures. The extended cascading Indian buffet process (eCIBP) essentially consists in adding an extra sampling step to the CIBP to generate connections between non-consecutive layers. In the context of graphical model structure learning, the proposed approach allows learning structures having an unbounded number of hidden random variables and automatically selecting the model complexity. We evaluated the extended process on multivariate density estimation and structure identification tasks by measuring the structure complexity and predictive performance. The results suggest the extension leads to extracting simpler graphs without scarifying predictive precision. Copyright © 2014, Association for the Advancement of Artificial Intelligence.\n</div> \n<p></p>"},{"id":467,"title":"A Bayesian Nonparametric Goodness of Fit Test for Right Censored Data Based on Approximate Samples from the Beta-Stacy Process","url":"https://www.researchgate.net/publication/260146403_A_Bayesian_Nonparametric_Goodness_of_Fit_Test_for_Right_Censored_Data_Based_on_Approximate_Samples_from_the_Beta-Stacy_Process","abstraction":"In recent years, Bayesian nonparametric statistics has received extraordinary attention. The beta-Stacy process, a generalization of the Dirichlet process, is a fundamental tool in studying Bayesian nonparametric statistics. In this article, we derive a simple, yet efficient, way to simulate the beta-Stacy process. We compare the efficiency of the new approximation to several other well-known approximations, and we demonstrate a significant improvement. Using the Kolmogorov distance and samples from the beta-Stacy process, a Bayesian nonparametric goodness of fit test is proposed. The proposed test is very general in the sense that it can be applied to censored and non-censored observations. Some illustrative examples are included. 41: 466–487; 2013 © 2013 Statistical Society of Canada Résumé Au cours des dernières années, la statistique non paramétrique bayésienne a reçu une attention considérable. Le processus bêta-Stacy, une généralisation du processus de Dirichlet, est un outil fondamental dans l’étude de la statistique non paramétrique bayésienne. Dans cet article, les auteurs obtiennent une façon simple et efficace de simuler le processus bêta-Stacy. Ils comparent l'efficacité de la nouvelle approximation à plusieurs autres approximations bien connues et démontrent une amélioration notable. Ils proposent un test d'adéquation non paramétrique bayésien basé sur la distance de Kolmogorov et sur des échantillons provenant du processus bêta-Stacy. Ce test est très général puisqu'il s'applique autant à des observations censurées que non censurées. Des exemples illustrent la méthode proposée. La revue canadienne de statistique 41: 466–487; 2013 © 2013 Société statistique du Canada"},{"id":468,"title":"Bayesian Nonparametric Dictionary Learning for Compressed Sensing MRI","url":"https://www.researchgate.net/publication/235438734_Bayesian_Nonparametric_Dictionary_Learning_for_Compressed_Sensing_MRI","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We develop a Bayesian nonparametric model for reconstructing magnetic\n <br> resonance images (MRI) from highly undersampled k-space data. Our model uses\n <br> the beta process as a nonparametric prior for dictionary learning, in which an\n <br> image patch is a sparse combination of dictionary elements. The size of the\n <br> dictionary and the patch-specific sparsity pattern is inferred from the data,\n <br> in addition to all dictionary learning variables. Dictionary learning is\n <br> performed as part of the image reconstruction process, and so is tailored to\n <br> the MRI being considered. In addition, we investigate a total variation penalty\n <br> term in combination with the dictionary learning model. We derive a stochastic\n <br> optimization algorithm based on Markov Chain Monte Carlo (MCMC) sampling for\n <br> the Bayesian model, and use the alternating direction method of multipliers\n <br> (ADMM) for efficiently performing total variation minimization. We present\n <br> empirical results on several MRI, which show that the proposed regularization\n <br> framework can improve reconstruction accuracy over other methods.\n</div> \n<p></p>"},{"id":469,"title":"Predictive construction of priors in Bayesian nonparametrics","url":"https://www.researchgate.net/publication/254212192_Predictive_construction_of_priors_in_Bayesian_nonparametrics","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The characterization of models and priors through a predictive approach is a fundamental problem in Bayesian statistics. In the last decades, it has received renewed interest, as the basis of important developments in Bayesian nonparametrics and in machine learning. In this paper, we review classical and recent work based on the predictive approach in these areas. Our focus is on the predictive construction of priors for Bayesian nonparametric inference, for exchangeable and partially exchangeable sequences. Some results are revisited to shed light on theoretical connections among them.\n</div> \n<p></p>"},{"id":470,"title":"On exact sampling of the first passage event of Levy process with infinite Levy measure and bounded variation","url":"https://www.researchgate.net/publication/229065999_On_exact_sampling_of_the_first_passage_event_of_Levy_process_withinfinite_Levy_measure_and_bounded_variation","abstraction":"We present an exact sampling method for the first passage event of a Levy process. The idea is to embed the process into another one whose first passage event can be sampled exactly, and then recover the part belonging to the former from the latter. The method is based on several distributional properties that appear to be new. We obtain general procedures to sample the first passage event of a subordinator across a regular non-increasing boundary, and that of a process with infinite Levy measure, bounded variation, and suitable drift across a constant level or interval. We give examples of application to a rather wide variety of Levy measures."},{"id":471,"title":"Levy Measure Decompositions for the Beta and Gamma Processes","url":"https://www.researchgate.net/publication/227716415_Levy_Measure_Decompositions_for_the_Beta_and_Gamma_Processes","abstraction":"We develop new representations for the Levy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on Levy processes, as these are of increasing importance in machine learning."},{"id":472,"title":"Developing Population Codes By Minimizing Description Length","url":"https://www.researchgate.net/publication/2641493_Developing_Population_Codes_By_Minimizing_Description_Length","abstraction":"The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs. Most existing unsupervised ..."},{"id":473,"title":"Bayesian density estimation by mixtures of normal distributions. Recent Adv Stat","url":"https://www.researchgate.net/publication/245581848_Bayesian_density_estimation_by_mixtures_of_normal_distributions_Recent_Adv_Stat","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This chapter discusses Bayesian density estimation by mixtures of normal distributions and discusses the estimation of an arbitrary density f(x) on the real line. This density is modeled as a mixture of a countable number of normal distributions. Using such mixtures, any distribution on the real line can be approximated to within any preassigned accuracy in the Levy metric and any density on the real line can be approximated similarly in the L1 norm. Thus, the problem can be considered nonparametric. Asymptotic theory for kernel estimators involves the problems of letting the window size tend to zero at some rate as the sample size tends to infinity.\n</div> \n<p></p>"},{"id":474,"title":"Hierarchical Priors and Mixture Models, With Application in Regression and Density Estimation","url":"https://www.researchgate.net/publication/2251788_Hierarchical_Priors_and_Mixture_Models_With_Application_in_Regression_and_Density_Estimation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n this paper to exhibit a general framework for hierarchical linear modelling and density estimation, to show how posterior computations via Markov chain simulations can be routinely applied, and to provide illustrations in each context. Section 2 provides a rather general theoretical setting and summarises key features of multivariate data models with hierarchical mixture priors. Section 3 discusses Markov chain simulation methods in these models, with special emphasis on models centred around traditional normal structures. Section 4 concerns an application in hierarchical regression, highlighting the use of mixture priors for robustness and sensitivity analysis, and Section 5 develops an application to multivariate density estimation. y\n</div> \n<p></p>"},{"id":475,"title":"Parametric Mixture Models for Multi-Labeled Text.","url":"https://www.researchgate.net/publication/221618091_Parametric_Mixture_Models_for_Multi-Labeled_Text","abstraction":"We propose probabilistic generative models, called parametric mix- ture models (PMMs), for multiclass, multi-labeled text categoriza- tion problem. Conventionally, the binary classication approach has been employed, in which whether or not text belongs to a cat- egory is judged by the binary classier for every category. In con- trast, our approach can simultaneously detect multiple categories of text using PMMs. We derive ecien t learning and prediction algo- rithms for PMMs. We also empirically show that our method could signican tly outperform the conventional binary methods when ap- plied to multi-labeled text categorization using real World Wide Web pages."},{"id":476,"title":"A semiparametric Bayesian model for randomised block designs","url":"https://www.researchgate.net/publication/243662369_A_semiparametric_Bayesian_model_for_randomised_block_designs","abstraction":"A model is proposed for a Bayesian semiparametric analysis of randomised block experiments. The model is a hierarchical model in which a Dirichlet process is inserted at the middle stage for the distribution of the block effects. This model allows an arbitrary distribution of block effects, and it results in effective estimates of treatment contrasts, block effects and the distribution of block effects. An effective computational strategy is presented for describing the posterior distribution."},{"id":477,"title":"Strategies and Principles of Distributed Machine Learning on Big Data","url":"https://www.researchgate.net/publication/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The rise of Big Data has led to new demands for Machine Learning (ML) systems\n <br> to learn complex models with millions to billions of parameters, that promise\n <br> adequate capacity to digest massive datasets and offer powerful predictive\n <br> analytics thereupon. In order to run ML algorithms at such scales, on a\n <br> distributed cluster with 10s to 1000s of machines, it is often the case that\n <br> significant engineering efforts are required --- and one might fairly ask if\n <br> such engineering truly falls within the domain of ML research or not. Taking\n <br> the view that Big ML systems can benefit greatly from ML-rooted statistical and\n <br> algorithmic insights --- and that ML researchers should therefore not shy away\n <br> from such systems design --- we discuss a series of principles and strategies\n <br> distilled from our recent efforts on industrial-scale ML solutions. These\n <br> principles and strategies span a continuum from application, to engineering,\n <br> and to theoretical research and development of Big ML systems and\n <br> architectures, with the goal of understanding how to make them efficient,\n <br> generally-applicable, and supported with convergence and scaling guarantees.\n <br> They concern four key questions which traditionally receive little attention in\n <br> ML research: How to distribute an ML program over a cluster? How to bridge ML\n <br> computation with inter-machine communication? How to perform such\n <br> communication? What should be communicated between machines? By exposing\n <br> underlying statistical and algorithmic characteristics unique to ML programs\n <br> but not typically seen in traditional computer programs, and by dissecting\n <br> successful cases to reveal how we have harnessed these principles to design and\n <br> develop both high-performance distributed ML software as well as\n <br> general-purpose ML frameworks, we present opportunities for ML researchers and\n <br> practitioners to further shape and grow the area that lies between ML and\n <br> systems.\n</div> \n<p></p>"},{"id":478,"title":"Streaming, Distributed Variational Inference for Bayesian Nonparametrics","url":"https://www.researchgate.net/publication/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics","abstraction":"This paper presents a methodology for creating streaming, distributed inference algorithms for Bayesian nonparametric (BNP) models. In the proposed framework, processing nodes receive a sequence of data minibatches, compute a variational posterior for each, and make asynchronous streaming updates to a central model. In contrast to previous algorithms, the proposed framework is truly streaming, distributed, asynchronous, learning-rate-free, and truncation-free. The key challenge in developing the framework, arising from the fact that BNP models do not impose an inherent ordering on their components, is finding the correspondence between minibatch and central BNP posterior components before performing each update. To address this, the paper develops a combinatorial optimization problem over component correspondences, and provides an efficient solution technique. The paper concludes with an application of the methodology to the DP mixture model, with experimental results demonstrating its practical scalability and performance."},{"id":479,"title":"Bayesian Fuzzy Clustering","url":"https://www.researchgate.net/publication/282603739_Bayesian_Fuzzy_Clustering","abstraction":"We present a Bayesian probabilistic model and inference algorithm for fuzzy clustering that provides expanded capabilities over the traditional Fuzzy C-Means approach. Additionally, we extend the Bayesian Fuzzy Clustering model to handle a variable number of clusters and present a particle filter inference technique to estimate the model parameters including the number of clusters. We show results on synthetic and real data and compare with other approaches."},{"id":480,"title":"Generalized Independent Component Analysis Over Finite Alphabets","url":"https://www.researchgate.net/publication/281144567_Generalized_Independent_Component_Analysis_Over_Finite_Alphabets","abstraction":"Independent component analysis (ICA) is a statistical method for transforming an observable multidimensional random vector into components that are as statistically independent as possible from each other.Usually the ICA framework assumes a model according to which the observations are generated (such as a linear transformation with additive noise). ICA over finite fields is a special case of ICA in which both the observations and the independent components are over a finite alphabet. In this work we consider a generalization of this framework in which an observation vector is decomposed to its independent components (as much as possible) with no prior assumption on the way it was generated. This generalization is also known as Barlow's minimal redundancy representation problem and is considered an open problem. We propose several theorems and show that this NP hard problem can be accurately solved with a branch and bound search tree algorithm, or tightly approximated with a series of linear problems. Our contribution provides the first efficient and constructive set of solutions to Barlow's problem.The minimal redundancy representation (also known as factorial code) has many applications, mainly in the fields of Neural Networks and Deep Learning. The Binary ICA (BICA) is also shown to have applications in several domains including medical diagnosis, multi-cluster assignment, network tomography and internet resource management. In this work we show this formulation further applies to multiple disciplines in source coding such as predictive coding, distributed source coding and coding of large alphabet sources."},{"id":481,"title":"Dependent Indian Buffet Process-based Sparse Nonparametric Nonnegative Matrix Factorization","url":"https://www.researchgate.net/publication/280062180_Dependent_Indian_Buffet_Process-based_Sparse_Nonparametric_Nonnegative_Matrix_Factorization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into two\n <br> optimized nonnegative matrices appropriate for the intended applications. The\n <br> method has been widely used for unsupervised learning tasks, including\n <br> recommender systems (rating matrix of users by items) and document clustering\n <br> (weighting matrix of papers by keywords). However, traditional NMF methods\n <br> typically assume the number of latent factors (i.e., dimensionality of the\n <br> loading matrices) to be fixed. This assumption makes them inflexible for many\n <br> applications. In this paper, we propose a nonparametric NMF framework to\n <br> mitigate this issue by using dependent Indian Buffet Processes (dIBP). In a\n <br> nutshell, we apply a correlation function for the generation of two stick\n <br> weights associated with each pair of columns of loading matrices, while still\n <br> maintaining their respective marginal distribution specified by IBP. As a\n <br> consequence, the generation of two loading matrices will be column-wise\n <br> (indirectly) correlated. Under this same framework, two classes of correlation\n <br> function are proposed (1) using Bivariate beta distribution and (2) using\n <br> Copula function. Both methods allow us to adopt our work for various\n <br> applications by flexibly choosing an appropriate parameter settings. Compared\n <br> with the other state-of-the art approaches in this area, such as using Gaussian\n <br> Process (GP)-based dIBP, our work is seen to be much more flexible in terms of\n <br> allowing the two corresponding binary matrix columns to have greater variations\n <br> in their non-zero entries. Our experiments on the real-world and synthetic\n <br> datasets show that three proposed models perform well on the document\n <br> clustering task comparing standard NMF without predefining the dimension for\n <br> the factor matrices, and the Bivariate beta distribution-based and Copula-based\n <br> models have better flexibility than the GP-based model.\n</div> \n<p></p>"},{"id":482,"title":"Indian Buffet process for model selection in convolved multiple-output Gaussian processes","url":"https://www.researchgate.net/publication/274012431_Indian_Buffet_process_for_model_selection_in_convolved_multiple-output_Gaussian_processes","abstraction":"Multi-output Gaussian processes have received increasing attention during the last few years as a natural mechanism to extend the powerful flexibility of Gaussian processes to the setup of multiple output variables. The key point here is the ability to design kernel functions that allow exploiting the correlations between the outputs while fulfilling the positive definiteness requisite for the covariance function. Alternatives to construct these covariance functions are the linear model of coregionalization and process convolutions. Each of these methods demand the specification of the number of latent Gaussian process used to build the covariance function for the outputs. We propose in this paper, the use of an Indian Buffet process as a way to perform model selection over the number of latent Gaussian processes. This type of model is particularly important in the context of latent force models, where the latent forces are associated to physical quantities like protein profiles or latent forces in mechanical systems. We use variational inference to estimate posterior distributions over the variables involved, and show examples of the model performance over artificial data, a motion capture dataset, and a gene expression dataset."},{"id":483,"title":"Adaptive Learning in Changing Environments.","url":"https://www.researchgate.net/publication/221165555_Adaptive_Learning_in_Changing_Environments","abstraction":"An adaptive on-line algorithm extending the learning of learningidea is proposed and theoretically motivated. Relying only on gradientflow information it can be applied to learning continuousfunctions or distributions, even when no explicit loss function is givenand the Hessian is not available. Its efficiency is demonstratedfor a non-stationary blind separation task of acoustic signals.1 IntroductionNeural networks provide powerful tools to capture the structure in data by..."},{"id":484,"title":"Fast-convergence filtered regressor algorithms for blind equalisation","url":"https://www.researchgate.net/publication/3377007_Fast-convergence_filtered_regressor_algorithms_for_blind_equalisation","abstraction":"The authors present a simple extension of the standard Bussgang blind equalisation algorithms that significantly improves their convergence properties. The technique uses the inverse channel estimate to filter the regressor signal. The modified algorithms provide quasi-Newton convergence in the vicinity of a local minimum of the chosen cost function with only a modest increase in the overall computational complexity of the system. An example of the technique as applied to the constant-modulus algorithm indicates its superior convergence behaviour"},{"id":485,"title":"Adaptive Online Learning Algorithms for Blind Separation: Maximum Entropy and Minimum Mutual Information","url":"https://www.researchgate.net/publication/242919207_Adaptive_Online_Learning_Algorithms_for_Blind_Separation_Maximum_Entropy_and_Minimum_Mutual_Information","abstraction":"There are two major approaches for blind separation: maximum entropy (ME) and minimum mutual information (MMI). Both can be implemented by the stochastic gradient descent method for obtaining the demixing matrix. The MI is the contrast function for blind separation; the entropy is not. To justify the ME, the relation between ME and MMI is first elucidated by calculating the first derivative of the entropy and proving that the mean subtraction is necessary in applying the ME and at the solution points determined by the MI, the ME will not update the demixing matrix in the directions of increasing the cross-talking.Second, the natural gradient instead of the ordinary gradient is introduced to obtain efficient algorithms, because the parameter space is a Riemannian space consisting of matrices. The mutual information is calculated by applying the Gram-Charlier expansion to approximate probability density functions of the outputs.Finally, we propose an efficient learning algorithm that incorporates with an adaptive method of estimating the unknown cumulants. It is shown by computer simulation that the convergence of the stochastic descent algorithms is improved by using the natural gradient and the adaptively estimated cumulants."},{"id":486,"title":"Learning processes in neural networks. Phys Rev A","url":"https://www.researchgate.net/publication/13383386_Learning_processes_in_neural_networks_Phys_Rev_A","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We study the learning dynamics of neural networks from a general point of view. The environment from which the network learns is defined as a set of input stimuli. At discrete points in time, one of these stimuli is presented and an incremental learning step takes place. If the time between learning steps is drawn from a Poisson distribution, the dynamics of an ensemble of learning processes is described by a continuous-time master equation. A learning algorithm that enables a neural network to adapt to a changing environment must have a nonzero learning parameter. This constant adaptability, however, goes at cost of fluctuations in the plasticities, such as synapses and thresholds. The ensemble description allows us to study the asymptotic behavior of the plasticities for a large class of neural networks. For small learning parameters, we derive an expression for the size of the fluctuations in an unchanging environment. In a changing environment, there is a trade-off between adaptability and accuracy (i.e., size of the fluctuations). We use the networks of Grossberg [J. Stat. Phys. 48, 105 (1969)] and Oja [J. Math. Biol. 15, 267 (1982)] as simple examples to analyze and simulate the performance of neural networks in a changing environment. In some cases an optimal learning parameter can be calculated.\n</div> \n<p></p>"},{"id":487,"title":"Unsupervised Learning by Examples: On-line Versus Off-line","url":"https://www.researchgate.net/publication/2815942_Unsupervised_Learning_by_Examples_On-line_Versus_Off-line","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n . We study both on-line and off-line learning in the following unsupervised learning scheme: p patterns are sampled independently from a distribution on the N-sphere with a single symmetry breaking orientation. Exact results are obtained in the limit p !1 and N !1 with finite ratio p=N . One finds that for smooth pattern distributions, the asymptotic behavior of the optimal off-line and on-line learning are identical, and saturate the Cramer-Rao inequality from statistics. For discontinuous pattern distributions on the other hand, the optimal online algorithm needs (at least) twice as many examples asymptotically to reach the optimal off-line performance. 1 Introduction Over the last decade, networks that can learn by examples have been investigated theoretically by applying a formalism similar to that of statistical mechanics [1]-[8]. Typically, the network is trained off-line by minimization of a cost function which incorporates the information about the whole training set...\n</div> \n<p></p>"},{"id":488,"title":"Supervised Dimensionality Reduction via Distance Correlation Maximization","url":"https://www.researchgate.net/publication/289406962_Supervised_Dimensionality_Reduction_via_Distance_Correlation_Maximization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In our work, we propose a novel formulation for supervised dimensionality\n <br> reduction based on a nonlinear dependency criterion called Statistical Distance\n <br> Correlation, Szekely et. al. (2007). We propose an objective which is free of\n <br> distributional assumptions on regression variables and regression model\n <br> assumptions. Our proposed formulation is based on learning a low-dimensional\n <br> feature representation $\\mathbf{z}$, which maximizes the squared sum of\n <br> Distance Correlations between low dimensional features $\\mathbf{z}$ and\n <br> response $y$, and also between features $\\mathbf{z}$ and covariates\n <br> $\\mathbf{x}$. We propose a novel algorithm to optimize our proposed objective\n <br> using the Generalized Minimization Maximizaiton method of \\Parizi et. al.\n <br> (2015). We show superior empirical results on multiple datasets proving the\n <br> effectiveness of our proposed approach over several relevant state-of-the-art\n <br> supervised dimensionality reduction methods.\n</div> \n<p></p>"},{"id":489,"title":"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)","url":"https://www.researchgate.net/publication/284579051_Fast_and_Accurate_Deep_Network_Learning_by_Exponential_Linear_Units_ELUs","abstraction":"We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parameterized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently, dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers (&gt;= 5). Using ELUs, we obtained the best published single-crop result on CIFAR-100 and CIFAR-10. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with similar classification performance, obtaining less than 10% classification error for a single crop, single model network."},{"id":490,"title":"Data-Dependent Path Normalization in Neural Networks","url":"https://www.researchgate.net/publication/284476101_Data-Dependent_Path_Normalization_in_Neural_Networks","abstraction":"We propose a unified framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions. Through this framework we investigate issue of invariance of the optimization, data dependence and the connection with natural gradients."},{"id":491,"title":"Symmetry-invariant optimization in deep networks","url":"https://www.researchgate.net/publication/283531387_Symmetry-invariant_optimization_in_deep_networks","abstraction":"Recent works have highlighted scale invariance or symmetry that is present in the weight space of a typical deep network and the adverse effect that it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that these and other commonly used deep networks, such as those which use a max-pooling and sub-sampling layer, possess more complex forms of symmetry arising from scaling based reparameterization of the network weights. We then propose two symmetry-invariant gradient based weight updates for stochastic gradient descent based learning. Our empirical evidence based on the MNIST dataset shows that these updates improve the test performance without sacrificing the computational efficiency of the weight updates. We also show the results of training with one of the proposed weight updates on an image segmentation problem."},{"id":492,"title":"Understanding symmetries in deep networks","url":"https://www.researchgate.net/publication/283532009_Understanding_symmetries_in_deep_networks","abstraction":"Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates."},{"id":493,"title":"Turning Statistical Physics Models Into Materials Design Engines","url":"https://www.researchgate.net/publication/283043539_Turning_Statistical_Physics_Models_Into_Materials_Design_Engines","abstraction":"Despite the success statistical physics has enjoyed at predicting the properties of materials for given parameters, the inverse problem, identifying which material parameters produce given, desired properties, is only beginning to be addressed. Recently, several methods have emerged across disciplines that draw upon optimization and simulation to create computer programs that tailor material responses to specified behaviors. However, so far the methods developed either involve black-box techniques, in which the optimizer operates without explicit knowledge of the material's configuration space, or they require carefully tuned algorithms with applicability limited to a narrow subclass of materials. Here we introduce a formalism that can generate optimizers automatically by extending statistical mechanics into the realm of design. The strength of this new approach lies in its capability to transform statistical models that describe materials into optimizers to tailor them. By comparing against standard black-box optimization methods, we demonstrate how optimizers generated by this formalism can be faster and more effective, while remaining straightforward to implement. The scope of our approach includes new possibilities for solving a variety of complex optimization and design problems concerning materials both in and out of equilibrium."},{"id":494,"title":"Hessian-Free Optimization For Learning Deep Multidimensional Recurrent Neural Networks","url":"https://www.researchgate.net/publication/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Multidimensional recurrent neural network (MDRNN) has shown a remarkable\n <br> performance in speech and handwriting recognition. The performance of MDRNN is\n <br> improved by further increasing its depth, and the difficulty of learning the\n <br> deeper network is overcome by Hessian-free (HF) optimization. Considering that\n <br> connectionist temporal classification (CTC) is utilized as an objective of\n <br> learning MDRNN for sequence labelling, the non-convexity of CTC poses a problem\n <br> to apply HF to the network. As a solution to this, a convex approximation of\n <br> CTC is formulated and its relationship with the EM algorithm and the Fisher\n <br> information matrix is discussed. MDRNN up to the depth of 15 layers is\n <br> successfully trained using HF, resulting in improved performance for sequence\n <br> labelling.\n</div> \n<p></p>"},{"id":495,"title":"On Accelerated Methods in Optimization","url":"https://www.researchgate.net/publication/281768579_On_Accelerated_Methods_in_Optimization","abstraction":"In convex optimization, there is an {\\em acceleration} phenomenon in which we can boost the convergence rate of certain gradient-based algorithms. We can observe this phenomenon in Nesterov's accelerated gradient descent, accelerated mirror descent, and accelerated cubic-regularized Newton's method, among others. In this paper, we show that the family of higher-order gradient methods in discrete time (generalizing gradient descent) corresponds to a family of first-order rescaled gradient flows in continuous time. On the other hand, the family of {\\em accelerated} higher-order gradient methods (generalizing accelerated mirror descent) corresponds to a family of second-order differential equations in continuous time, each of which is the Euler-Lagrange equation of a family of Lagrangian functionals. We also study the exponential variant of the Nesterov Lagrangian, which corresponds to a generalization of Nesterov's restart scheme and achieves a linear rate of convergence in discrete time. Finally, we show that the family of Lagrangians is closed under time dilation (an orbit under the action of speeding up time), which demonstrates the universality of this Lagrangian view of acceleration in optimization."},{"id":496,"title":"G-protein genomic association with normal variation in gray matter density","url":"https://www.researchgate.net/publication/280869374_G-protein_genomic_association_with_normal_variation_in_gray_matter_density","abstraction":"While detecting genetic variations underlying brain structures helps reveal mechanisms of neural disorders, high data dimensionality poses a major challenge for imaging genomic association studies. In this work, we present the application of a recently proposed approach, parallel independent component analysis with reference (pICA-R), to investigate genomic factors potentially regulating gray matter variation in a healthy population. This approach simultaneously assesses many variables for an aggregate effect and helps to elicit particular features in the data. We applied pICA-R to analyze gray matter density (GMD) images (274,131 voxels) in conjunction with single nucleotide polymorphism (SNP) data (666,019 markers) collected from 1,256 healthy individuals of the Brain Imaging Genetics (BIG) study. Guided by a genetic reference derived from the gene GNA14, pICA-R identified a significant SNP-GMD association (r = -0.16, P = 2.34 × 10(-8) ), implying that subjects with specific genotypes have lower localized GMD. The identified components were then projected to an independent dataset from the Mind Clinical Imaging Consortium (MCIC) including 89 healthy individuals, and the obtained loadings again yielded a significant SNP-GMD association (r = -0.25, P = 0.02). The imaging component reflected GMD variations in frontal, precuneus, and cingulate regions. The SNP component was enriched in genes with neuronal functions, including synaptic plasticity, axon guidance, molecular signal transduction via PKA and CREB, highlighting the GRM1, PRKCH, GNA12, and CAMK2B genes. Collectively, our findings suggest that GNA12 and GNA14 play a key role in the genetic architecture underlying normal GMD variation in frontal and parietal regions. Hum Brain Mapp, 2015. © 2015 Wiley Periodicals, Inc. © 2015 Wiley Periodicals, Inc."},{"id":497,"title":"Approximated Newton Algorithm for the Ising Model Inference Speeds Up Convergence, Performs Optimally and Avoids Over-fitting","url":"https://www.researchgate.net/publication/280104564_Approximated_Newton_Algorithm_for_the_Ising_Model_Inference_Speeds_Up_Convergence_Performs_Optimally_and_Avoids_Over-fitting","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Inverse problems consist in inferring parameters of model distributions that\n <br> are able to fit properly chosen features of experimental data-sets. The Inverse\n <br> Ising problem specifically consists of searching for the maximal entropy\n <br> distribution reproducing frequencies and correlations of a binary data-set. In\n <br> order to solve this task, we propose an algorithm that takes advantage of the\n <br> provided by the data knowledge of the log-likelihood function around the\n <br> solution. We show that the present algorithm is faster than standard gradient\n <br> ascent methods. Moreover, by looking at the algorithm convergence as a\n <br> stochastic process, we properly define over-fitting and we show how the present\n <br> algorithm avoids it by construction.\n</div> \n<p></p>"},{"id":498,"title":"The Existence of Probability Measures with Given Marginals","url":"https://www.researchgate.net/publication/38365762_The_Existence_of_Probability_Measures_with_Given_Marginals","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n First an integral representation of a continuous linear functional dominated by a support function in integral form is given (Theorem 1). From this the theorem of Blackwell-Stein-Sherman-Cartier [2], [20], [4], is deduced as well as a result on capacities alternating of order 2 in the sense of Choquet [5], which includes Satz 4.3 of [23] and a result of Kellerer [10], [12], under somewhat stronger assumptions. Next (Theorem 7), the existence of probability distributions with given marginals is studied under typically weaker assumptions, than those which are required by the use of Theorem 1. As applications we derive necessary and sufficient conditions for a sequence of probability measures to be the sequence of distributions of a martingale (Theorem 8), an upper semi-martingale (Theorem 9) or of partial sums of independent random variables (Theorem 10). Moreover an alternative definition of Levy-Prokhorov's distance between probability measures in a complete separable metric space is obtained (corollary of Theorem 11). Section 6 can be read independently of the former sections.\n</div> \n<p></p>"},{"id":499,"title":"Canonical row-column-exchangeable arrays","url":"https://www.researchgate.net/publication/23630041_Canonical_row-column-exchangeable_arrays","abstraction":"Consider a standard row-column-exchangeable array X = (Xij : i,j &gt;= 1), i.e., Xij = f(a, [xi]i, [eta]j, [lambda]ij) is a function of i.i.d. random variables. It is shown that there is a canonical version of X, X', such that X', and [alpha]', [xi]'1, [xi]'2,..., [eta]'1, [eta]'2,..., are conditionally independent given [intersection]n &gt;= 1 [sigma](X'ij : max(i,j) &gt;= n). This result is quite a bit simpler to prove than the analogous result for the original array X, which is due to Aldous."},{"id":500,"title":"Random Partitions in Population Genetics","url":"https://www.researchgate.net/publication/253255469_Random_Partitions_in_Population_Genetics","abstraction":"This paper is concerned with models for the genetic variation of a sample of gametes from a large population. The need for consistency between different sample sizes limits the mathematical possibilities to what are here called 'partition structures'. Distinctive among them is the structure described by the Ewens sampling formula, which is shown to enjoy a characteristic property of non-interference between the different alleles. This characterization explains the robustness of the Ewens formula when neither selection nor recurrent mutation is significant, although different structures arise from selective and 'charge-state' models."},{"id":501,"title":"A Dirichlet Process Mixture Based Name Origin Clustering and Alignment Model for Transliteration","url":"https://www.researchgate.net/publication/282477931_A_Dirichlet_Process_Mixture_Based_Name_Origin_Clustering_and_Alignment_Model_for_Transliteration","abstraction":"In machine transliteration, it is common that the transliterated names in the target language come from multiple language origins. A conventional maximum likelihood based single model can not deal with this issue very well and often suffers from overfitting. In this paper, we exploit a coupled Dirichlet process mixture model (cDPMM) to address overfitting and names multiorigin cluster issues simultaneously in the transliteration sequence alignment step over the name pairs. After the alignment step, the cDPMM clusters name pairs into many groups according to their origin information automatically. In the decoding step, in order to use the learned origin information sufficiently, we use a cluster combination method (CCM) to build clustering-specific transliteration models by combining small clusters into large ones based on the perplexities of name language and transliteration model, which makes sure each origin cluster has enough data for training a transliteration model. On the three different Western-Chinese multiorigin names corpora, the cDPMM outperforms two state-of-the-art baseline models in terms of both the top-1 accuracy and mean F -score, and furthermore the CCM significantly improves the cDPMM."},{"id":502,"title":"Perturbed datasets methods for hypothesis testing and structure of corresponding confidence sets","url":"https://www.researchgate.net/publication/267928518_Perturbed_datasets_methods_for_hypothesis_testing_and_structure_of_corresponding_confidence_sets","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Hypothesis testing methods that do not rely on exact distribution assumptions have been emerging lately. The method of sign-perturbed sums (SPS) is capable of characterizing confidence regions with exact confidence levels for linear regression and linear dynamical systems parameter estimation problems if the noise distribution is symmetric. This paper describes a general family of hypothesis testing methods that have an exact user chosen confidence level based on finite sample count and without relying on an assumed noise distribution. It is shown that the SPS method belongs to this family and we provide another hypothesis test for the case where the symmetry assumption is replaced with exchangeability. In the case of linear regression problems it is shown that the confidence regions are connected, bounded and possibly non-convex sets in both cases. To highlight the importance of understanding the structure of confidence regions corresponding to such hypothesis tests it is shown that confidence sets for linear dynamical systems parameter estimates generated using the SPS method can have non-connected parts, which have far reaching consequences.\n</div> \n<p></p>"},{"id":503,"title":"Infinite-degree-corrected stochastic block model","url":"https://www.researchgate.net/publication/266944998_Infinite-degree-corrected_stochastic_block_model","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In stochastic block models, which are among the most prominent statistical models for cluster analysis of complex networks, clusters are defined as groups of nodes with statistically similar link probabilities within and between groups. A recent extension by Karrer and Newman [Karrer and Newman, Phys. Rev. E 83, 016107 (2011)] incorporates a node degree correction to model degree heterogeneity within each group. Although this demonstrably leads to better performance on several networks, it is not obvious whether modeling node degree is always appropriate or necessary. We formulate the degree corrected stochastic block model as a nonparametric Bayesian model, incorporating a parameter to control the amount of degree correction that can then be inferred from data. Additionally, our formulation yields principled ways of inferring the number of groups as well as predicting missing links in the network that can be used to quantify the model's predictive performance. On synthetic data we demonstrate that including the degree correction yields better performance on both recovering the true group structure and predicting missing links when degree heterogeneity is present, whereas performance is on par for data with no degree heterogeneity within clusters. On seven real networks (with no ground truth group structure available) we show that predictive performance is about equal whether or not degree correction is included; however, for some networks significantly fewer clusters are discovered when correcting for degree, indicating that the data can be more compactly explained by clusters of heterogenous degree nodes.\n</div> \n<p></p>"},{"id":504,"title":"Risk-Sensitive Mean Field Games","url":"https://www.researchgate.net/publication/232143618_Risk-Sensitive_Mean_Field_Games","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we study a class of risk-sensitive mean-field stochastic\n <br> differential games. We show that under appropriate regularity conditions, the\n <br> mean-field value of the stochastic differential game with exponentiated\n <br> integral cost functional coincides with the value function described by a\n <br> Hamilton-Jacobi-Bellman (HJB) equation with an additional quadratic term. We\n <br> provide an explicit solution of the mean-field best response when the\n <br> instantaneous cost functions are log-quadratic and the state dynamics are\n <br> affine in the control. An equivalent mean-field risk-neutral problem is\n <br> formulated and the corresponding mean-field equilibria are characterized in\n <br> terms of backward-forward macroscopic McKean-Vlasov equations,\n <br> Fokker-Planck-Kolmogorov equations, and HJB equations. We provide numerical\n <br> examples on the mean field behavior to illustrate both linear and McKean-Vlasov\n <br> dynamics.\n</div> \n<p></p>"},{"id":505,"title":"Dynamic Chinese Restaurant Game: Theory and Application to Cognitive Radio Networks","url":"https://www.researchgate.net/publication/261768274_Dynamic_Chinese_Restaurant_Game_Theory_and_Application_to_Cognitive_Radio_Networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Users in a social network are usually confronted with decision making under uncertain network state. While there are some works in the social learning literature on how to construct belief on an uncertain network state, few study has been made on integrating learning with decision making for the scenario where users are uncertain about the network state and their decisions influence with each other. Moreover, the population in a social network can be dynamic since users may arrive at or leave the network at any time, which makes the problem even more challenging. In this paper, we propose a Dynamic Chinese Restaurant Game to study how a user in a dynamic social network learns the uncertain network state and make optimal decision by taking into account not only the immediate utility but also subsequent users' negative influence. We introduce a Bayesian learning based method for users to learn the network state, and propose a Multi-dimensional Markov Decision Process based approach for users to achieve the optimal decisions. Finally, we apply the Dynamic Chinese Restaurant Game to cognitive radio networks and demonstrate from simulations to verify the effectiveness and efficiency of the proposed scheme.\n</div> \n<p></p>"},{"id":506,"title":"Record-dependent measures on the symmetric groups: Record-Dependent Measures","url":"https://www.researchgate.net/publication/221664737_Record-dependent_measures_on_the_symmetric_groups_Record-Dependent_Measures","abstraction":"Probability measure P_n on the symmetric group S_n is said to be record-dependent if P_n(s) depends only on the set of records of permutation s. A sequence P=(P_n) of consistent record-dependent measures determines a random order on the set of positive integers. In this paper we describe the extreme elements of the convex set of such P. This problem turns out to be related to the study of asymptotic behavior of permutation-valued growth processes, to random extensions of partial orders, and to the measures on the Young-Fibonacci lattice."},{"id":507,"title":"Scaling symmetry, renormalization, and time series modeling: The case of financial assets dynamics","url":"https://www.researchgate.net/publication/260003974_Scaling_symmetry_renormalization_and_time_series_modeling_The_case_of_financial_assets_dynamics","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present and discuss a stochastic model of financial assets dynamics based on the idea of an inverse renormalization group strategy. With this strategy we construct the multivariate distributions of elementary returns based on the scaling with time of the probability density of their aggregates. In its simplest version the model is the product of an endogenous autoregressive component and a random rescaling factor designed to embody also exogenous influences. Mathematical properties like increments' stationarity and ergodicity can be proven. Thanks to the relatively low number of parameters, model calibration can be conveniently based on a method of moments, as exemplified in the case of historical data of the S&amp;P500 index. The calibrated model accounts very well for many stylized facts, like volatility clustering, power-law decay of the volatility autocorrelation function, and multiscaling with time of the aggregated return distribution. In agreement with empirical evidence in finance, the dynamics is not invariant under time reversal, and, with suitable generalizations, skewness of the return distribution and leverage effects can be included. The analytical tractability of the model opens interesting perspectives for applications, for instance, in terms of obtaining closed formulas for derivative pricing. Further important features are the possibility of making contact, in certain limits, with autoregressive models widely used in finance and the possibility of partially resolving the long- and short-memory components of the volatility, with consistent results when applied to historical series.\n</div> \n<p></p>"},{"id":508,"title":"Indian Buffet Game With Negative Network Externality and Non-Bayesian Social Learning","url":"https://www.researchgate.net/publication/256487685_Indian_Buffet_Game_With_Negative_Network_Externality_and_Non-Bayesian_Social_Learning","abstraction":"How users in a dynamic system perform learning and make decision become more and more important in numerous research fields. Although there are some works in the social learning literatures regarding how to construct belief on an uncertain system state, few study has been conducted on incorporating social learning with decision making. Moreover, users may have multiple concurrent decisions on different objects/resources and their decisions usually negatively influence each other's utility, which makes the problem even more challenging. In this paper, we propose an Indian Buffet Game to study how users in a dynamic system learn the uncertain system state and make multiple concurrent decisions by not only considering the current myopic utility, but also taking into account the influence of subsequent users' decisions. We analyze the proposed Indian Buffet Game under two different scenarios: customers request multiple dishes without budget constraint and with budget constraint. For both cases, we design recursive best response algorithms to find the subgame perfect Nash equilibrium for customers and characterize special properties of the Nash equilibrium profile under homogeneous setting. Moreover, we introduce a non-Bayesian social learning algorithm for customers to learn the system state, and theoretically prove its convergence. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed algorithms."},{"id":509,"title":"On the Amount of Dependence in the Prime Factorization of a Uniform Random Integer","url":"https://www.researchgate.net/publication/236627542_On_the_Amount_of_Dependence_in_the_Prime_Factorization_of_a_Uniform_Random_Integer","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n How much dependence is there in the prime factorization of a random integer\n <br> distributed uniformly from 1 to n? How much dependence is there in the\n <br> decomposition into cycles of a random permutation of n points? What is the\n <br> relation between the Poisson-Dirichlet process and the scale invariant Poisson\n <br> process? These three questions have essentially the same answers, with respect\n <br> to total variation distance, considering only small components, and with\n <br> respect to a Wasserstein distance, considering all components. The Wasserstein\n <br> distance is the expected number of changes -- insertions and deletions --\n <br> needed to change the dependent system into an independent system.\n <br> In particular we show that for primes, roughly speaking, 2+o(1) changes are\n <br> necessary and sufficient to convert a uniformly distributed random integer from\n <br> 1 to n into a random integer prod_{p leq n} p^{Z_p} in which the multiplicity\n <br> Z_p of the factor p is geometrically distributed, with all Z_p independent. The\n <br> changes are, with probability tending to 1, one deletion, together with a\n <br> random number of insertions, having expectation 1+o(1).\n <br> The crucial tool for showing that 2+epsilon suffices is a coupling of the\n <br> infinite independent model of prime multiplicities, with the scale invariant\n <br> Poisson process on (0,infty). A corollary of this construction is the first\n <br> metric bound on the distance to the Poisson-Dirichlet in Billingsley's 1972\n <br> weak convergence result. Our bound takes the form: there are couplings in which\n <br> E sum |log P_i(n) - (log n) V_i | = O(\\log \\log n), where P_i denotes the\n <br> i-th largest prime factor and V_i denotes the i-th component of the\n <br> Poisson-Dirichlet process. It is reasonable to conjecture that O(1) is\n <br> achievable.\n</div> \n<p></p>"},{"id":510,"title":"Markov Chains for Exploring Posterior Distributions","url":"https://www.researchgate.net/publication/38357609_Markov_Chains_for_Exploring_Posterior_Distributions","abstraction":"Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation."},{"id":511,"title":"Nonparametric hierarchical Bayes via sequential imputation","url":"https://www.researchgate.net/publication/38348723_Nonparametric_hierarchical_Bayes_via_sequential_imputation","abstraction":"We consider the empirical Bayes estimation of a distribution using binary data via the Dirichlet process. Let $\\mathscr{D}(\\alpha)$ denote a Dirichlet process with $\\alpha$ being a finite measure on Instead of having direct samples from an unknown random distribution F from $\\mathscr{D}(\\alpha)$, we assume that only indirect binomial data are observable. This paper presents a new interpretation of Lo's formula, and thereby relates the predictive density of the observations based on a Dirichlet process model to likelihoods of much simpler models. As a consequence, the log-likelihood surface, as well as the maximum likelihood estimate of $c = \\alpha([0, 1])$, is found when the shape of $\\alpha$ a is assumed known, together with a formula for the Fisher information evaluated at the estimate. The sequential imputation method of Kong, Liu and Wong is recommended for overcoming computational difficulties commonly encountered in this area. The related approximation formulas are provided. An analysis of the tack data of Beckett and Diaconis, which motivated this study, is supplemented to illustrate our methods."},{"id":512,"title":"A Bayesian Analysis of Ordinal Data Using Mixtures","url":"https://www.researchgate.net/publication/2469179_A_Bayesian_Analysis_of_Ordinal_Data_Using_Mixtures","abstraction":"this paper, we discuss a Bayesian method for the analysis of ordinal data. It is assumed that the ordinal data arise from an underlying latent variable which is related to a set of covariates in a generalized linear model sense. The link function which associates the class probabilities to the set of covariates is estimated as a finite mixture of probit links, thereby introducing flexibility in the choice of the link function. The Bayesian model provides an easy-to-implement simulation environment and therefore avoids pitfalls of the expensive and numerically unstable maximum likelihood analysis. The class probabilities are estimated by considering the predictive distribution of a future observation given the set of covariates. Computations are performed using Gibbs sampling. The method is illustrated by using both simulated and real data sets."},{"id":513,"title":"Bayesian Curve Fitting Using Multivariate Normal Mixtures","url":"https://www.researchgate.net/publication/2419985_Bayesian_Curve_Fitting_Using_Multivariate_Normal_Mixtures","abstraction":"Problems of regression smoothing and curve fitting are addressed via predictive inference in a flexible class of mixture models. Multidimensional density estimation using Dirichlet mixture models provides the theoretical basis for semi-parametric regression methods in which fitted regression functions may be deduced as means of conditional predictive distributions. These Bayesian regression functions have features similar to generalised kernel regression estimates, but the formal analysis addresses problems of multivariate smoothing, parameter estimation, and the assessment of uncertainties about regression functions naturally. Computations are based on multidimensional versions of existing Markov chain simulation analysis of univariate Dirichlet mixture models."},{"id":514,"title":"Nonparametric Bayesian Bioassay Including Ordered Polytomous Response","url":"https://www.researchgate.net/publication/31102905_Nonparametric_Bayesian_Bioassay_Including_Ordered_Polytomous_Response","abstraction":"Previous attempts at implementing fully Bayesian nonparametric bioassay have enjoyed limited success due to computational difficulties. We show here how this problem may be generally handled using a sampling based approach to develop desired marginal posterior distributions and their features. A useful extension is presented which treats the case of ordered polytomous response. Illustrative examples are provided."},{"id":515,"title":"Bayesian Nonparametric Inference for the Power Likelihood","url":"https://www.researchgate.net/publication/259580158_Bayesian_Nonparametric_Inference_for_the_Power_Likelihood","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The aim in this article is to provide a means to undertake Bayesian inference for mixture models when the likelihood function is raised to a power between 0 and 1. The main purpose for doing this is to guarantee a strongly consistent model and hence, make it possible to compare the consistent posterior with the correct posterior, looking for signs of discrepancy. This will be explained in detail in the article. Another purpose would be for simulated annealing algorithms. In particular, for the widely used mixture of Dirichlet process model, it is far from obvious how to undertake inference via Markov chain Monte Carlo methods when the likelihood is raised to a power other than 1. In this article, we demonstrate how posterior sampling can be carried out when using a power likelihood. Matlab code to implement the algorithm is available as supplementary material.\n</div> \n<p></p>"},{"id":516,"title":"Variational Bayes inference and Dirichlet process priors","url":"https://www.researchgate.net/publication/256846096_Variational_Bayes_inference_and_Dirichlet_process_priors","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper shows how the variational Bayes method provides a computational\n <br> efficient technique in the context of hierarchical modelling using Dirichlet\n <br> process priors, in particular without requiring conjugate prior assumption. It\n <br> shows, using the so called parameter separation parameterization, a simple\n <br> criterion under which the variational method works well. Based on this\n <br> framework, its provides a full variational solution for the Dirichlet process.\n <br> The numerical results show that the method is very computationally efficient\n <br> when compared to MCMC. Finally, we propose an empirical method to estimate the\n <br> truncation level for the truncated Dirichlet process.\n</div> \n<p></p>"},{"id":517,"title":"Effect on Prediction when Modeling Covariates in Bayesian Nonparametric Models","url":"https://www.researchgate.net/publication/236921802_Effect_on_Prediction_when_Modeling_Covariates_in_Bayesian_Nonparametric_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In biomedical research, it is often of interest to characterize biologic processes giving rise to observations and to make predictions of future observations. Bayesian nonparametric methods provide a means for carrying out Bayesian inference making as few assumptions about restrictive parametric models as possible. There are several proposals in the literature for extending Bayesian nonparametric models to include dependence on covariates. Limited attention, however, has been directed to the following two aspects. In this article, we examine the effect on fitting and predictive performance of incorporating covariates in a class of Bayesian nonparametric models by one of two primary ways: either in the weights or in the locations of a discrete random probability measure. We show that different strategies for incorporating continuous covariates in Bayesian nonparametric models can result in big differences when used for prediction, even though they lead to otherwise similar posterior inferences. When one needs the predictive density, as in optimal design, and this density is a mixture, it is better to make the weights depend on the covariates. We demonstrate these points via a simulated data example and in an application in which one wants to determine the optimal dose of an anticancer drug used in pediatric oncology.\n</div> \n<p></p>"},{"id":518,"title":"A Sequential Algorithm for Fast Fitting of Dirichlet Process Mixture Models","url":"https://www.researchgate.net/publication/234113987_A_Sequential_Algorithm_for_Fast_Fitting_of_Dirichlet_Process_Mixture_Models","abstraction":"In this article we propose an improvement on the sequential updating and greedy search (SUGS) algorithm Wang and Dunson for fast fitting of Dirichlet process mixture models. The SUGS algorithm provides a means for very fast approximate Bayesian inference for mixture data which is particularly of use when data sets are so large that many standard Markov chain Monte Carlo (MCMC) algorithms cannot be applied efficiently, or take a prohibitively long time to converge. In particular, these ideas are used to initially interrogate the data, and to refine models such that one can potentially apply exact data analysis later on. SUGS relies upon sequentially allocating data to clusters and proceeding with an update of the posterior on the subsequent allocations and parameters which assumes this allocation is correct. Our modification softens this approach, by providing a probability distribution over allocations, with a similar computational cost; this approach has an interpretation as a variational Bayes procedure and hence we term it variational SUGS (VSUGS). It is shown in simulated examples that VSUGS can out-perform, in terms of density estimation and classification, the original SUGS algorithm in many scenarios. In addition, we present a data analysis for flow cytometry data, and SNP data via a three-class dirichlet process mixture model illustrating the apparent improvement over SUGS."},{"id":519,"title":"Bayesian Clustering of Animal Abundance Trends For Inference and Dimension Reduction","url":"https://www.researchgate.net/publication/239731075_Bayesian_Clustering_of_Animal_Abundance_Trends_For_Inference_and_Dimension_Reduction","abstraction":"We consider a model-based clustering approach to examining abundance trends in a metapopulation. When examining trends for an animal population with management goals in mind one is often interested in those segments of the population that behave similarly to one another with respect to abundance. Our proposed trend analysis in- corporates a clustering method that is an extension of the classic Chinese Restaurant Process, and the associated Dirichlet process prior, which allows for inclusion of dis- tance covariates between sites. This approach has two main benefits: (1) nonparametric spatial association of trends and (2) reduced dimension of the spatio-temporal trend process. We present a transdimensional Gibbs sampler for making Bayesian inference that is efficient in the sense that all of the full conditionals can be directly sampled from save one. To demonstrate the proposed method we examine long term trends in northern fur seal pup production at 19 rookeries in the Pribilof Islands, Alaska. There was strong evidence that clustering of similar year-to-year deviation from linear trends was associ- ated with whether rookeries were located on the same island. Clustering of local linear trends did not seem to be strongly associated with any of the distance covariates. In the fur seal trends analysis an overwhelming proportion of the MCMC iterations produced a 73–79 % reduction in the dimension of the spatio-temporal trend process, depending on the number of cluster groups."},{"id":520,"title":"Bayesian Semiparametric Regression Models to Characterize Molecular Evolution.","url":"https://www.researchgate.net/publication/232737223_Bayesian_Semiparametric_Regression_Models_to_Characterize_Molecular_Evolution","abstraction":"Background Statistical models and methods that associate changes in the physicochemical properties of amino acids with natural selection at the molecular level typically do not take into account the correlations between such properties. We propose a Bayesian hierarchical regression model with a generalization of the Dirichlet process prior on the distribution of the regression coefficients that describes the relationship between the changes in amino acid distances and natural selection in protein-coding DNA sequence alignments.  Results The Bayesian semiparametric approach is illustrated with simulated data and the abalone lysin sperm data. Our method identifies groups of properties which, for this particular dataset, have a similar effect on evolution. The model also provides nonparametric site-specific estimates for the strength of conservation of these properties.  Conclusions The model described here is distinguished by its ability to handle a large number of amino acid properties simultaneously, while taking into account that such data can be correlated. The multi-level clustering ability of the model allows for appealing interpretations of the results in terms of properties that are roughly equivalent from the standpoint of molecular evolution."},{"id":521,"title":"A nonparametric Bayesian approach for clustering bisulfate-based DNA methylation profiles","url":"https://www.researchgate.net/publication/257870618_A_nonparametric_Bayesian_approach_for_clustering_bisulfate-based_DNA_methylation_profiles","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n DNA methylation occurs in the context of a CpG dinucleotide. It is an important epigenetic modification, which can be inherited through cell division. The two major types of methylation include hypomethylation and hypermethylation. Unique methylation patterns have been shown to exist in diseases including various types of cancer. DNA methylation analysis promises to become a powerful tool in cancer diagnosis, treatment and prognostication. Large-scale methylation arrays are now available for studying methylation genome-wide. The Illumina methylation platform simultaneously measures cytosine methylation at more than 1500 CpG sites associated with over 800 cancer-related genes. Cluster analysis is often used to identify DNA methylation subgroups for prognosis and diagnosis. However, due to the unique non-Gaussian characteristics, traditional clustering methods may not be appropriate for DNA and methylation data, and the determination of optimal cluster number is still problematic. A Dirichlet process beta mixture model (DPBMM) is proposed that models the DNA methylation expressions as an infinite number of beta mixture distribution. The model allows automatic learning of the relevant parameters such as the cluster mixing proportion, the parameters of beta distribution for each cluster, and especially the number of potential clusters. Since the model is high dimensional and analytically intractable, we proposed a Gibbs sampling \"no-gaps\" solution for computing the posterior distributions, hence the estimates of the parameters. The proposed algorithm was tested on simulated data as well as methylation data from 55 Glioblastoma multiform (GBM) brain tissue samples. To reduce the computational burden due to the high data dimensionality, a dimension reduction method is adopted. The two GBM clusters yielded by DPBMM are based on data of different number of loci (P-value &lt; 0.1), while hierarchical clustering cannot yield statistically significant clusters.\n</div> \n<p></p>"},{"id":522,"title":"Online versus Offline Learning from Random Examples: GeneralResults","url":"https://www.researchgate.net/publication/13227200_Online_versus_Offline_Learning_from_Random_Examples_GeneralResults","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n I propose a general model of on-line learning from random examples which, when applied to a smooth realizable stochastic rule, yields the same asymptotic generalization error rate as optimal batch algorithms. The approach is based on an iterative Gaussian approximation to the posterior Gibbs distribution of rule parameters.\n</div> \n<p></p>"},{"id":523,"title":"A Bayesian Committee Machine","url":"https://www.researchgate.net/publication/12218265_A_Bayesian_Committee_Machine","abstraction":"The Bayesian committee machine (BCM) is a novel approach to combining estimators that were trained on different data sets. Although the BCM can be applied to the combination of any kind of estimators, the main foci are gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data. Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator. The BCM also provides a new solution for on-line learning with potential applications to data mining. We apply the BCM to systems with fixed basis functions and discuss its relationship to gaussian process regression. Finally, we show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input-dependent combination of estimators."},{"id":524,"title":"RKHS-based functional analysis for exact incremental learning","url":"https://www.researchgate.net/publication/220549594_RKHS-based_functional_analysis_for_exact_incremental_learning","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We investigate the problem of incremental learning in artificial neural networks by viewing it as a sequential function approximation problem. A framework for discussing the generalization ability of a trained network in the original function space using tools of functional analysis based on reproducing kernel Hilbert spaces (RKHS) is introduced. Using this framework, we devise a method of carrying out optimal incremental learning with respect to the entire set of training data by employing the results derived at the previous stage of learning and incorporating the newly available training data effectively. Most importantly, the incrementally learned function has the same (optimal) generalization ability as would have been achieved by using batch learning on the entire set of training data, hence, referred to as exact learning. This ensures that both the learning operator and the learned function can be computed using an online incremental scheme. Finally, we also provide a simplified closed-form relationship between the learned functions before and after the incorporation of new data for various optimization criteria, opening avenues for work into selection of optimal training set. We also show that learning under this kind of framework is inherently well suited for applying novel model selection strategies and introducing bias and a priori knowledge in a more systematic way. Moreover, it provides a useful hint in performing kernel-based approximations, of which the regularization and SVM networks are special cases, in an online setting.\n</div> \n<p></p>"},{"id":525,"title":"Bayesian classification with Gaussian processes","url":"https://www.researchgate.net/publication/3192933_Bayesian_classification_with_Gaussian_processes","abstraction":"We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the probability of class one given x is estimated by ?(y(x)), where ?(y)=1/(1+e\n<sup>-y</sup>). A Gaussian process prior is placed on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation. The method is generalized to multiclass problems (m&gt;2) using the softmax function. We demonstrate the effectiveness of the method on a number of datasets"},{"id":526,"title":"Real-time model learning using Incremental Sparse Spectrum Gaussian Process Regression","url":"https://www.researchgate.net/publication/230869907_Real-time_model_learning_using_Incremental_Sparse_Spectrum_Gaussian_Process_Regression","abstraction":"Novel applications in unstructured and non-stationary human environments require robots that learn from experience and adapt autonomously to changing conditions. Predictive models therefore not only need to be accurate, but should also be updated incrementally in real-time and require minimal human intervention. Incremental Sparse Spectrum Gaussian Process Regression is an algorithm that is targeted specifically for use in this context. Rather than developing a novel algorithm from the ground up, the method is based on the thoroughly studied Gaussian Process Regression algorithm, therefore ensuring a solid theoretical foundation. Non-linearity and a bounded update complexity are achieved simultaneously by means of a finite dimensional random feature mapping that approximates a kernel function. As a result, the computational cost for each update remains constant over time. Finally, algorithmic simplicity and support for automated hyperparameter optimization ensures convenience when employed in practice. Empirical validation on a number of synthetic and real-life learning problems confirms that the performance of Incremental Sparse Spectrum Gaussian Process Regression is superior with respect to the popular Locally Weighted Projection Regression, while computational requirements are found to be significantly lower. The method is therefore particularly suited for learning with real-time constraints or when computational resources are limited."},{"id":527,"title":"Input Space Versus Feature Space in Kernel-Based Methods","url":"https://www.researchgate.net/publication/220279290_Input_Space_Versus_Feature_Space_in_Kernel-Based_Methods","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector (SV) kernel functions. We first discuss the geometry of feature space. In particular, we review what is known about the shape of the image of input space under the feature space map, and how this influences the capacity of SV methods. Following this, we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel, using the example of the class of inhomogeneous polynomial kernels, which are often used in SV pattern recognition. We then discuss the connection between feature space and input space by dealing with the question of how one can, given some vector in feature space, find a preimage (exact or approximate) in input space. We describe algorithms to tackle this issue, and show their utility in two applications of kernel methods. First, we use it to reduce the computational complexity of SV decision functions; second, we combine it with the Kernel PCA algorithm, thereby constructing a nonlinear statistical denoising technique which is shown to perform well on real-world data.\n</div> \n<p></p>"},{"id":528,"title":"Gaussian Processes for Regression","url":"https://www.researchgate.net/publication/2320571_Gaussian_Processes_for_Regression","abstraction":"The Bayesian analysis of neural networks is difficult because a simple prior over weights implies a complex prior distribution over functions. In this paper we investigate the use of Gaussian process priors over functions, which permit the predictive Bayesian analysis for fixed values of hyperparameters to be carried out exactly using matrix operations. Two methods, using optimization and averaging (via Hybrid Monte Carlo) over hyperparameters have been tested on a number of challenging problems and have produced excellent results. 1 INTRODUCTION In the Bayesian approach to neural networks a prior distribution over the weights induces a prior distribution over functions. This prior is combined with a noise model, which specifies the probability of observing the targets t given function values y, to yield a posterior over functions which can then be used for predictions. For neural networks the prior over functions has a complex form which means that implementations must eithe..."},{"id":529,"title":"A Resource-Allocating Network for Function Interpolation","url":"https://www.researchgate.net/publication/243689579_A_Resource-Allocating_Network_for_Function_Interpolation","abstraction":"We have created a network that allocates a new computational unit whenever an unusual pattern is presented to the network. This network forms compact representations, yet learns easily and rapidly. The network can be used at any time in the learning process and the learning patterns do not have to be repeated. The units in this network respond to only a local region of the space of input values.The network learns by allocating new units and adjusting the parameters of existing units. If the network performs poorly on a presented pattern, then a new unit is allocated that corrects the response to the presented pattern. If the network performs well on a presented pattern, then the network parameters are updated using standard LMS gradient descent.We have obtained good results with our resource-allocating network (RAN). For predicting the Mackey-Glass chaotic time series, RAN learns much faster than do those using backpropagation networks and uses a comparable number of synapses."},{"id":530,"title":"Sparse representation based on projection method in online least squares support vector machines","url":"https://www.researchgate.net/publication/225425447_Sparse_representation_based_on_projection_method_in_online_least_squares_support_vector_machines","abstraction":"A sparse approximation algorithm based on projection is presented in this paper in order to overcome the limitation of the non-sparsity of least squares support vector machines (LS-SVM). The new inputs are projected into the subspace spanned by previous basis vectors (BV) and those inputs whose squared distance from the subspace is higher than a threshold are added in the BV set, while others are rejected. This consequently results in the sparse approximation. In addition, a recursive approach to deleting an exiting vector in the BV set is proposed. Then the online LS-SVM, sparse approximation and BV removal are combined to produce the sparse online LS-SVM algorithm that can control the size of memory irrespective of the processed data size. The suggested algorithm is applied in the online modeling of a pH neutralizing process and the isomerization plant of a refinery, respectively. The detailed comparison of computing time and precision is also given between the suggested algorithm and the nonsparse one. The results show that the proposed algorithm greatly improves the sparsity just with little cost of precision."},{"id":531,"title":"Structured neural network modelling of multi-valued functions for wind retrieval from scatterometer measurements","url":"https://www.researchgate.net/publication/222695635_Structured_neural_network_modelling_of_multi-valued_functions_for_wind_retrieval_from_scatterometer_measurements","abstraction":"A conventional neural network approach to regression problems approximates the conditional mean of the output vector. For mappings which are multi-valued this approach breaks down, since the average of two solutions is not necessarily a valid solution. In this article mixture density networks, a principled method for modelling conditional probability density functions, are applied to retrieving Cartesian wind vector components from satellite scatterometer data. A hybrid mixture density network is implemented to incorporate prior knowledge of the predominantly bimodal function branches. An advantage of a fully probabilistic model is that more sophisticated and principled methods can be used to resolve ambiguities."},{"id":532,"title":"Gaussian Process based Subsumption of a Parasitic Control Component","url":"https://www.researchgate.net/publication/277324885_Gaussian_Process_based_Subsumption_of_a_Parasitic_Control_Component","abstraction":"Many existing control architectures assume that the main control system being designed is the only controller that governs a system's actuators. However, with the increasing availability of off-the shelf controls packages, the number of internal unadjustable control systems is increasing. Some of these control systems may behave in parasitic way by enforcing a rigid set of behaviors that could disrupt a desired system behavior. We present a control architecture that can subsume parasitic control behavior through iteratively shaping the main control command with an intelligent feed-forward term. Our architecture requires very little prior knowledge about the subsystem whose behavior is to be subsumed, rather it relies on online learned sparsified predictive Gaussian Process (GP) models. We provide rigorous quantifiable bounds relating the sparsification of the GP to the accuracy in estimating and subsuming the parasitic subsystem. The presented subsumption architecture is realized using a variant of D-Type iterative learning control (ILC) and is validated through a series of flight tests on a Parrot AR Drone 2.0 quadrotor where the quadrotor's sonar based altitude control loop's behavior of maintaining a fixed altitude over ground surfaces is subsumed through a main controller via a feed-forward term."},{"id":533,"title":"Gaussian Process Models with Parallelization and GPU acceleration","url":"https://www.researchgate.net/publication/267157313_Gaussian_Process_Models_with_Parallelization_and_GPU_acceleration","abstraction":"In this work, we present an extension of Gaussian process (GP) models with sophisticated parallelization and GPU acceleration. The parallelization scheme arises naturally from the modular computational structure w.r.t. datapoints in the sparse Gaussian process formulation. Additionally, the computational bottleneck is implemented with GPU acceleration for further speed up. Combining both techniques allows applying Gaussian process models to millions of datapoints. The efficiency of our algorithm is demonstrated with a synthetic dataset. Its source code has been integrated into our popular software library GPy."},{"id":534,"title":"Analyzing Sparse Dictionaries for Online Learning With Kernels","url":"https://www.researchgate.net/publication/265967011_Analyzing_Sparse_Dictionaries_for_Online_Learning_With_Kernels","abstraction":"Many signal processing and machine learning methods share essentially the same linear-in-the-parameter model, with as many parameters as available samples as in kernel-based machines. Sparse approximation is essential in many disciplines, with new challenges emerging in online learning with kernels. To this end, several sparsity measures have been proposed in the literature to quantify sparse dictionaries and constructing relevant ones, the most prolific ones being the distance, the approximation, the coherence and the Babel measures. In this paper, we analyze sparse dictionaries based on these measures. By conducting an eigenvalue analysis, we show that these sparsity measures share many properties, including the linear independence condition and inducing a well-posed optimization problem. Furthermore, we prove that there exists a quasi-isometry between the parameter (i.e., dual) space and the dictionary's induced feature space."},{"id":535,"title":"Approximation Errors of Online Sparsification Criteria","url":"https://www.researchgate.net/publication/265967012_Approximation_Errors_of_Online_Sparsification_Criteria","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Many machine learning frameworks, such as resource-allocating networks,\n <br> kernel-based methods, Gaussian processes, and radial-basis-function networks,\n <br> require a sparsification scheme in order to address the online learning\n <br> paradigm. For this purpose, several online sparsification criteria have been\n <br> proposed to restrict the model definition on a subset of samples. The most\n <br> known criterion is the (linear) approximation criterion, which discards any\n <br> sample that can be well represented by the already contributing samples, an\n <br> operation with excessive computational complexity. Several computationally\n <br> efficient sparsification criteria have been introduced in the literature, such\n <br> as the distance, the coherence and the Babel criteria. In this paper, we\n <br> provide a framework that connects these sparsification criteria to the issue of\n <br> approximating samples, by deriving theoretical bounds on the approximation\n <br> errors. Moreover, we investigate the error of approximating any feature, by\n <br> proposing upper-bounds on the approximation error for each of the\n <br> aforementioned sparsification criteria. Two classes of features are described\n <br> in detail, the empirical mean and the principal axes in the kernel principal\n <br> component analysis.\n</div> \n<p></p>"},{"id":536,"title":"Spatio-Temporal Learning With the Online Finite and Infinite Echo-State Gaussian Processes","url":"https://www.researchgate.net/publication/263124732_Spatio-Temporal_Learning_With_the_Online_Finite_and_Infinite_Echo-State_Gaussian_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Successful biological systems adapt to change. In this paper, we are principally concerned with adaptive systems that operate in environments where data arrives sequentially and is multivariate in nature, for example, sensory streams in robotic systems. We contribute two reservoir inspired methods: 1) the online echostate Gaussian process (OESGP) and 2) its infinite variant, the online infinite echostate Gaussian process (OIESGP) Both algorithms are iterative fixed-budget methods that learn from noisy time series. In particular, the OESGP combines the echo-state network with Bayesian online learning for Gaussian processes. Extending this to infinite reservoirs yields the OIESGP, which uses a novel recursive kernel with automatic relevance determination that enables spatial and temporal feature weighting. When fused with stochastic natural gradient descent, the kernel hyperparameters are iteratively adapted to better model the target system. Furthermore, insights into the underlying system can be gleamed from inspection of the resulting hyperparameters. Experiments on noisy benchmark problems (one-step prediction and system identification) demonstrate that our methods yield high accuracies relative to state-of-the-art methods, and standard kernels with sliding windows, particularly on problems with irrelevant dimensions. In addition, we describe two case studies in robotic learning-by-demonstration involving the Nao humanoid robot and the Assistive Robot Transport for Youngsters (ARTY) smart wheelchair.\n</div> \n<p></p>"},{"id":537,"title":"Incrementally Learning Objects by Touch: Online Discriminative and Generative Models for Tactile-Based Recognition","url":"https://www.researchgate.net/publication/262560295_Incrementally_Learning_Objects_by_Touch_Online_Discriminative_and_Generative_Models_for_Tactile-Based_Recognition","abstraction":"Human beings not only possess the remarkable ability to distinguish objects through tactile feedback but are further able to improve upon recognition competence through experience. In this work, we explore tactile-based object recognition with learners capable of incremental learning. Using the sparse online infinite Echo-State Gaussian process (OIESGP), we propose and compare two novel discriminative and generative tactile learners that produce probability distributions over objects during object grasping/palpation. To enable iterative improvement, our online methods incorporate training samples as they become available. We also describe incremental unsupervised learning mechanisms, based on novelty scores and extreme value theory, when teacher labels are not available. We present experimental results for both supervised and unsupervised learning tasks using the iCub humanoid, with tactile sensors on its five-fingered anthropomorphic hand, and 10 different object classes. Our classifiers perform comparably to state-of-the-art methods (C4.5 and SVM classifiers) and findings indicate that tactile signals are highly relevant for making accurate object classifications. We also show that accurate “early” classifications are possible using only 20-30% of the grasp sequence. For unsupervised learning, our methods generate high quality clusterings relative to the widely-used sequential k-means and selforganising map (SOM), and we present analyses into the differences between the approaches."},{"id":538,"title":"Nonparametric Kullback-Leibler Stochastic Control","url":"https://www.researchgate.net/publication/261700410_Nonparametric_Kullback-Leibler_Stochastic_Control","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present two nonparametric approaches to Kullback-Leibler (KL) control, or\n <br> linearly-solvable Markov decision problem (LMDP) based on Gaussian processes\n <br> (GP) and Nystr\\\"{o}m approximation. Compared to recently developed parametric\n <br> methods, the proposed frameworks feature accurate function approximation and\n <br> efficient on-line operations. Theoretically, we derive the mathematical\n <br> connection of KL control based on dynamic programming with earlier work in\n <br> control theory which relies on information theoretic dualities for case of\n <br> infinite time horizon problems. Algorithmically, we give analytical expressions\n <br> to optimal control policies in nonparametric forms, and propose on-line update\n <br> schemes with budgeted computational costs. Numerical results demonstrate the\n <br> effectiveness and usefulness of the proposed frameworks.\n</div> \n<p></p>"},{"id":539,"title":"Indirect robot model learning for tracking control","url":"https://www.researchgate.net/publication/263593840_Indirect_robot_model_learning_for_tracking_control","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Learning task-space tracking control on redundant robot manipulators is an important but difficult problem. A main difficulty is the non-uniqueness of the solution: a task-space trajectory has multiple joint-space trajectories associated, therefore averaging over non-convex solution space needs to be done if treated as a regression problem. A second class of difficulties arise for those robots when the physical model is either too complex or even not available. In this situation machine learning methods may be a suitable alternative to classical approaches. We propose a learning framework for tracking control that is applicable for underactuated or non-rigid robots where an analytical physical model of the robot is unavailable. The proposed framework builds on the insight that tracking problems are well defined in the joint task- and joint-space coordinates and consequently predictions can be obtained via local optimization. Physical experiments show that state-of-the art accuracy can be achieved in both online and offline tracking control learning. Furthermore, we show that the presented method is capable of controlling underactuated robot architectures as well.\n</div> \n<p></p>"},{"id":540,"title":"Efficient Probabilistic Classification Vector Machine With Incremental Basis Function Selection","url":"https://www.researchgate.net/publication/262148785_Efficient_Probabilistic_Classification_Vector_Machine_With_Incremental_Basis_Function_Selection","abstraction":"Probabilistic classification vector machine (PCVM) is a sparse learning approach aiming to address the stability problems of relevance vector machine for classification problems. Because PCVM is based on the expectation maximization algorithm, it suffers from sensitivity to initialization, convergence to local minima, and the limitation of Bayesian estimation making only point estimates. Another disadvantage is that PCVM was not efficient for large data sets. To address these problems, this paper proposes an efficient PCVM (EPCVM) by sequentially adding or deleting basis functions according to the marginal likelihood maximization for efficient training. Because of the truncated prior used in EPCVM, two approximation techniques, i.e., Laplace approximation and expectation propagation (EP), have been used to implement EPCVM to obtain full Bayesian solutions. We have verified Laplace approximation and EP with a hybrid Monte Carlo approach. The generalization performance and computational effectiveness of EPCVM are extensively evaluated. Theoretical discussions using Rademacher complexity reveal the relationship between the sparsity and the generalization bound of EPCVM."},{"id":541,"title":"Dynamic retrospective filtering of physiological noise in BOLD fMRI: DRIFTER","url":"https://www.researchgate.net/publication/221781468_Dynamic_retrospective_filtering_of_physiological_noise_in_BOLD_fMRI_DRIFTER","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this article we introduce the DRIFTER algorithm, which is a new model based Bayesian method for retrospective elimination of physiological noise from functional magnetic resonance imaging (fMRI) data. In the method, we first estimate the frequency trajectories of the physiological signals with the interacting multiple models (IMM) filter algorithm. The frequency trajectories can be estimated from external reference signals, or if the temporal resolution is high enough, from the fMRI data. The estimated frequency trajectories are then used in a state space model in combination of a Kalman filter (KF) and Rauch-Tung-Striebel (RTS) smoother, which separates the signal into an activation related cleaned signal, physiological noise, and white measurement noise components. Using experimental data, we show that the method outperforms the RETROICOR algorithm if the shape and amplitude of the physiological signals change over time.\n</div> \n<p></p>"},{"id":542,"title":"Nonlinear Smoothing Theory","url":"https://www.researchgate.net/publication/3479471_Nonlinear_Smoothing_Theory","abstraction":"Differential equations are developed for the smoothing density function and for the smoothed expectation of an arbitrary function of the state. The exact equations developed herein are difficult to solve except in trivially simple cases. Approximations to these equations are developed for the smoothed expectation of the state and the smoothing covariance matrix. For linear systems these equations are shown to reduce to previously derived results. An iterative technique is suggested for even greater accuracy in approximations for severely nonlinear systems."},{"id":543,"title":"The two-filter formula for smoothing and an implementation of the Gaussian-sum smoother","url":"https://www.researchgate.net/publication/24052253_The_two-filter_formula_for_smoothing_and_an_implementation_of_the_Gaussian-sum_smoother","abstraction":"In this paper we examine Australian data on national and regional employment numbers, focusing in particular on whether there have been common national and regional changes in the volatility of employment. A subsidiary objective is to assess whether the results derived from traditional growth rate models are sustained when alternative filtering methods are used. In particular, we compare the results of the growth rate models with those obtained from Hodrick-Prescott models. Using frequency filtering methods in conjunction with autoregressive modeling, we show that there is considerable diversity in the regional pattern of change and that it would be wrong to suppose that results derived from the aggregate employment series are generally applicable across the regions. The results suggest that the so-called great moderation may have been less extensive than aggregate macro studies suggest."},{"id":544,"title":"Marginalized particle filters for Bayesian estimation of Gaussian noise parameters","url":"https://www.researchgate.net/publication/224218695_Marginalized_particle_filters_for_Bayesian_estimation_of_Gaussian_noise_parameters","abstraction":"The particle filter provides a general solution to the nonlinear filtering problem with arbitrarily accuracy. However, the curse of dimensionality prevents its application in cases where the state dimensionality is high. Further, estimation of stationary parameters is a known challenge in a particle filter framework. We suggest a marginalization approach for the case of unknown noise distribution parameters that avoid both aforementioned problem. First, the standard approach of augmenting the state vector with sensor offsets and scale factors is avoided, so the state dimension is not increased. Second, the mean and covariance of both process and measurement noises are represented with parametric distributions, whose statistics are updated adaptively and analytically using the concept of conjugate prior distributions. The resulting marginalized particle filter is applied to and illustrated with a standard example from literature."},{"id":545,"title":"Square-Root Algorithms for the Continuous-Time Linear Least-Square Estimation Problem","url":"https://www.researchgate.net/publication/3028272_Square-Root_Algorithms_for_the_Continuous-Time_Linear_Least-Square_Estimation_Problem","abstraction":"We present a simple differential equation for the triangular square root of the state error variance of the continuous-time Kalman filter. Unlike earlier methods of Andrews, and Tapley and Choe, this algorithm does not explicitly involve any antisymmetric matrix in the differential equation for the square roots. The role of antisymmetric matrices is clarified: it is shown that they are just the generators of the orthogonal transformations that connect the various square roots; in the constant model case, a similar set of antisymmetric matrices appears inside the Chandrasekhar-type equations for the square roots of the derivative of the error variance. Several square-root algorithms for the smoothing problem are also presented and are related to some well-known smoothing approaches."},{"id":546,"title":"Continuous-time and continuous-discrete-time unscented Rauch-Tung-Striebel smoothers","url":"https://www.researchgate.net/publication/220226884_Continuous-time_and_continuous-discrete-time_unscented_Rauch-Tung-Striebel_smoothers","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article considers the application of the unscented transformation to approximate fixed-interval optimal smoothing of continuous-time non-linear stochastic dynamic systems. The proposed methodology can be applied to systems, where the dynamics can be modeled with non-linear stochastic differential equations and the noise corrupted measurements are obtained continuously or at discrete times. The smoothing algorithm is based on computing the continuous-time limit of the recently proposed unscented Rauch–Tung–Striebel smoother, which is an approximate optimal smoothing algorithm for discrete-time stochastic dynamic systems.\n</div> \n<p></p>"},{"id":547,"title":"CATS benchmark time series prediction by Kalman smoother with cross-validated noise density","url":"https://www.researchgate.net/publication/222682280_CATS_benchmark_time_series_prediction_by_Kalman_smoother_with_cross-validated_noise_density","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article presents the winning solution to the CATS time series prediction competition. The solution is based on classical optimal linear estimation theory. The proposed method models the long and short term dynamics of the time series as stochastic linear models. The computation is based on a Kalman smoother, in which the noise densities are estimated by cross-validation. In time series prediction the Kalman smoother is applied three times in different stages of the method.\n</div> \n<p></p>"},{"id":548,"title":"Discrete-time fixed-lag smoothing algorithms","url":"https://www.researchgate.net/publication/223746525_Discrete-time_fixed-lag_smoothing_algorithms","abstraction":"Kalman filtering results are applied to yield alternative computationally stable fixed-lag smoothing algorithms including reduced order and minimal order fixed-lag smoothers. The reduced order smoothing algorithms are new and clearly have advantages over the more familiar algorithms. The properties of such fixed-lag smoothers are also studied.RésuméLes résultats de filtrage de Kalman sont appliqués pour donner des algorithemes alternatifs de nivellement à délai fixe et de calcul stable, y compris des niveleurs à délai fixe d'ordre réduit et d'ordre minimal. Les algorithmes de nivellement d'ordre réduit sont nouveaux et ont des avantages distincts sur les algorithmes plus familiers. Les propriétés de tels niveleurs à délai fixe sont également étudiées.ZusammenfassungResultate der Kalman-Filterung wurden angewandt, um alternative berechenbare stabile Glättungs-algorithmen mit gegebener Verzögerung einschließlich Glättungseinrichtungen reduzierter und minimaler Ordnung mit gegebener Verzögerung zu liefern.Die Glättungsalgorithmen reduzierter Ordung sind neu und haben gegenüber den bekannteren klare Vorzüge. Die Eigenschaften solcher Glättungseinrichtungen mit gegebener Verzögerung werden ebenfalls studiert.???????Pe?y???a?? ?????po?a??? ?a???a?a ?p??e????c? ??? ?o?y?e??? a???ep?a?????? yc?o?????? ? ????c?e???? a??op???o? c??a???a??? c ???c?po?a???? ?a?a????a??e?, ?????a? c??a???a?e?? c ???c?po?a???? ?a?a????a??e? co?pa?e??o?o ? ?????a???o?o ?op???a. A??op???? c??a???a??? co?pa?e??o?o ?op???a ??????c? ?o???? ? o?e????o ??e?? ?pe??y?ec??a ?epe? ?o?ee ??a?o???? a??op???a??. Pacc?a?p??a??c? ?a??e c?o?c??a c??a???a?e?e? c ???c?po?a???? ?a?a????a??e?."},{"id":549,"title":"Application of Girsanov Theorem to Particle Filtering of Discretely Observed Continuous-Time Non-Linear Systems","url":"https://www.researchgate.net/publication/1889902_Application_of_Girsanov_Theorem_to_Particle_Filtering_of_Discretely_Observed_Continuous-Time_Non-Linear_Systems","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article considers the application of particle filtering to continuous-discrete optimal filtering problems, where the system model is a stochastic differential equation, and noisy measurements of the system are obtained at discrete instances of time. It is shown how the Girsanov theorem can be used for evaluating the likelihood ratios needed in importance sampling. It is also shown how the methodology can be applied to a class of models, where the driving noise process is lower in the dimensionality than the state and thus the laws of state and noise are not absolutely continuous. Rao-Blackwellization of conditionally Gaussian models and unknown static parameter models is also considered.\n</div> \n<p></p>"},{"id":550,"title":"Expectation propagation and generalised EP methods for inference in switching linear dynamical systems","url":"https://www.researchgate.net/publication/254897362_Expectation_propagation_and_generalised_EP_methods_for_inference_in_switching_linear_dynamical_systems","abstraction":"Many real-world problems can be described by models that extend the classical linear Gaussian dynamical system with (unobserved) discrete regime indicators. In such extended models the discrete indicators dictate what transition and observation model the process follows at a particular time. The problems of tracking and estimation in models with manoeuvring targets [1], multiple targets [25], non-Gaussian disturbances [15], unknown model parameters [9], failing sensors [20] and different trends [8] are all examples of problems that have been formulated in a conditionally Gaussian state space model framework. Since the extended model is so general it has been invented and re-invented many times in multiple fields, and is known by many different names, such as switching linear dynamical system, conditionally Gaussian state space model, switching Kalman filter model and hybrid model. Although the extended model has a lot of expressive power, it is notorious for the fact that exact estimation of posteriors is intractable. In general, exact filtered, smoothed or predicted posteriors have a complexity exponential in the number of observations. Even when only marginals on the indicator variables are required the problem remains NP-hard [19]. In this chapter we introduce a deterministic approximation scheme that is particularly suited to find smoothed one and two time slice posteriors. It can be seen as a symmetric backward pass and iteration scheme for previously proposed assumed density filtering approaches [9]. The chapter is organised as follows. In Section 7.2 we present the general model; variants where only the transition or only the observation model switch, or where states or observations are multi-or univariate can be treated as special cases."},{"id":551,"title":"On the (non-)convergence of particle filters with Gaussian importance distributions**This work was supported by the Academy of Finland projects 266940 and 273475.","url":"https://www.researchgate.net/publication/288179574_On_the_non-convergence_of_particle_filters_with_Gaussian_importance_distributionsThis_work_was_supported_by_the_Academy_of_Finland_projects_266940_and_273475","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider convergence properties of particle filters with Gaussian importance distributions for certain time-varying Poisson regression models. We analyze both the classical bounded-importance-weight condition and a more recent moment condition. We show that Gaussian importance distributions based on Laplace approximations or non-linear Kalman filters lead to particle filters that are not guaranteed to converge. We also suggest avoiding the problem by a certain split-Gaussian modification that naturally arises from ensuring bounded weights. Although in this paper we concentrate on the time-varying Poisson regression model, we argue that our findings have implications in more general particle filtering problems.\n</div> \n<p></p>"},{"id":552,"title":"Nonlinear Filtering with Population Codes","url":"https://www.researchgate.net/publication/288059815_Nonlinear_Filtering_with_Population_Codes","abstraction":"In this paper we develop a method for the nonlinear filtering of dynamic population codes with unknown latent dynamics. This method is based on a parameterized function which updates beliefs while keeping the dimension of the belief space constant. In order to optimize this function, we construct a graphical model representation of the belief-latent state dynamics, and show that a certain marginalization of the graphical model has the form of a conditional exponential family harmonium. This marginalization allows us to maximize the likelihood of observations, and thereby optimize the parameterized function, by the method of contrastive divergence minimization. By defining the parameterized function as a multilayer perceptron, we show how the stochastic contrastive divergence gradient may be computed with backpropagation. We conclude by demonstrating the effectiveness of this approach on a filtering problem based on a stochastic pendulum."},{"id":553,"title":"Sequential estimation of intrinsic activity and synaptic input in single neurons by particle filtering with optimal importance density","url":"https://www.researchgate.net/publication/283761407_Sequential_estimation_of_intrinsic_activity_and_synaptic_input_in_single_neurons_by_particle_filtering_with_optimal_importance_density","abstraction":"This paper deals with the problem of inferring the signals and parameters that cause neural activity to occur. The ultimate challenge being to unveil brain's connectivity, here we focus on a microscopic vision of the problem, where single neurons (potentially connected to a network of peers) are at the core of our study. The sole observation available are noisy, sampled voltage traces obtained from intracellular recordings. We design algorithms and inference methods using the tools provided by stochastic filtering, that allow a probabilistic interpretation and treatment of the problem. Using particle filtering we are able to reconstruct traces of voltages and estimate the time course of auxiliary variables. By extending the algorithm, through PMCMC methodology, we are able to estimate hidden physiological parameters as well, like intrinsic conductances or reversal potentials. Last, but not least, the method is applied to estimate synaptic conductances arriving at a target cell, thus reconstructing the synaptic excitatory/inhibitory input traces. Notably, these estimations have a bound-achieving performance even in spiking regimes."},{"id":554,"title":"Derivation of the PHD and CPHD Filters Based on Direct Kullback–Leibler Divergence Minimization","url":"https://www.researchgate.net/publication/282404227_Derivation_of_the_PHD_and_CPHD_Filters_Based_on_Direct_Kullback-Leibler_Divergence_Minimization","abstraction":"In this paper, we provide novel derivations of the probability hypothesis density (PHD) and cardinalised PHD (CPHD) filters without using probability generating functionals or functional derivatives. We show that both the PHD and CPHD filters fit in the context of assumed density filtering and implicitly perform Kullback–Leibler divergence (KLD) minimizations after the prediction and update steps. We perform the KLD minimizations directly on the multitarget prediction and posterior densities."},{"id":555,"title":"A Systematization of the Unscented Kalman Filter Theory","url":"https://www.researchgate.net/publication/273485916_A_Systematization_of_the_Unscented_Kalman_Filter_Theory","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we propose a systematization of the (discrete-time) Unscented Kalman Filter (UKF) theory. We gather all available UKF variants in the literature, present corrections to theoretical inconsistencies, and provide a tool for the construction of new UKF's in a consistent way. This systematization is done, mainly, by revisiting the concepts of Sigma-Representation, Unscented Transformation (UT), Scaled Unscented Transformation (SUT), UKF, and Square-Root Unscented Kalman Filter (SRUKF). Inconsistencies are related to 1) matching the order of the transformed covariance and cross-covariance matrices of both the UT and the SUT; 2) multiple UKF definitions; 3) issue with some reduced sets of sigma points described in the literature; 4) the conservativeness of the SUT; 5) the scaling effect of the SUT on both its transformed covariance and cross-covariance matrices; and 6) possibly ill-conditioned results in SRUKF's. With the proposed systematization, the symmetric sets of sigma points in the literature are formally justified, and we are able to provide new consistent variations for UKF's, such as the Scaled SRUKF's and the UKF's composed by the minimum number of sigma points. Furthermore, our proposed SRUKF has improved computational properties when compared to state-of-the-art methods.\n</div> \n<p></p>"},{"id":556,"title":"Detecting Regime Shifts in Fish Stock Dynamics","url":"https://www.researchgate.net/publication/281316802_Detecting_Regime_Shifts_in_Fish_Stock_Dynamics","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Environmental factors such as water temperature, salinity, and the abundance of zooplankton can have major effects on certain fish stocks’ ability to produce juveniles and, thus, stock renewal ability. This variability in stock productivity manifests itself as different productivity regimes. Here, we detect productivity regime shifts by analyzing recruit-per-spawner time series with Bayesian online change point detection algorithm. The algorithm infers the time since the last regime shift (change in mean or variance or both) as well as the parameters of the data-generating process for the current regime sequentially. We demonstrate the algorithm’s performance using simulated recruitment data from an individual-based model and further apply the algorithm to stock assessment estimates for four Atlantic cod (Gadus morhua) stocks obtained from RAM legacy database. Our analysis shows that the algorithm performs well when the variability between the regimes is high enough compared with the variability within the regimes. The algorithm found several productivity regimes for all four cod stocks, and the findings suggest that the stocks are currently in low productivity regimes, which have started during the 1990s and 2000s. © 2015, National Research Council of Canada. All rights reserved.\n</div> \n<p></p>"},{"id":557,"title":"Pedestrian Localization in Moving Platforms Using Dead Reckoning, Particle Filtering and Map Matching","url":"https://www.researchgate.net/publication/275890740_Pedestrian_Localization_in_Moving_Platforms_Using_Dead_Reckoning_Particle_Filtering_and_Map_Matching","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Localization in global navigation satellite system denied environments using inertial sensors alone, or radio sensors alone or a combination of both are the currently active research topics. The current research works are primarily focused on static environments with earth fixed coordinate frames, having nonmoving maps. In this research work, we use micro electromechanical sensors based inertial sensors, band pass filtering, particle filtering, maps and map matching techniques for pedestrian localization with respect to on ground moving platforms such as train or bus. Since these platforms are moving, the maps of such platforms are moving maps with respect to earth centered, earth fixed coordinate frames. The techniques of this research work could further be extended and adapted to other moving platforms such as airplanes, boats and submarines.\n</div> \n<p></p>"},{"id":558,"title":"Nonlinear regression using smooth Bayesian estimation","url":"https://www.researchgate.net/publication/281202988_Nonlinear_regression_using_smooth_Bayesian_estimation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper proposes a new Bayesian strategy for the estimation of smooth parameters from nonlinear models. The observed signal is assumed to be corrupted by an independent and non identically (colored) Gaussian distribution. A prior enforcing a smooth temporal evolution of the model parameters is considered. The joint posterior distribution of the unknown parameter vector is then derived. A Gibbs sampler coupled with a Hamiltonian Monte Carlo algorithm is proposed which allows samples distributed according to the posterior of interest to be generated and to estimate the unknown\n <br> model parameters/hyperparameters. Simulations conducted with synthetic and real satellite altimetric data show the potential of the proposed Bayesian model and the corresponding estimation algorithm for nonlinear regression with smooth estimated parameters.\n</div> \n<p></p>"},{"id":559,"title":"Time-varying forecasts by variational approximation of sequential Bayesian inference","url":"https://www.researchgate.net/publication/273776881_Time-varying_forecasts_by_variational_approximation_of_sequential_Bayesian_inference","abstraction":"Abstract: Methods developed for making time-varying forecasts in economic and financial analysis include (a) equal-weighted moving-window (or rolling) regression, (b) time-weighted (e.g. exponentially weighted) regression, (c) the Kalman filter and (d) adaptive Kalman filters. This paper developed a new method based on variational approximation of sequential Bayesian inference (VASB). Concepts and notions of the sequential Bayesian analysis and the variational approximation of an intractable posterior are simple and straightforward. Our VASB algorithm is not complicated and is easy to code. For a regression on multiple time-series, the regression coefficients, standard errors, prediction and residual error are time-varying and are estimated jointly at every time step. For a single time-series (e.g. price returns of an asset), its mean and variance are time-varying and are predicted jointly at every time step. The VASB algorithm performs better than the rolling and time-weighted statistics or regressions and the Kalman filter in terms of higher predictive power and stronger robustness. Derivations of the VASB algorithm are presented in the appendices.  Keywords: Time-varying forecasts; Time-varying regression; Time-varying variance; State space modeling of time-series; Sequential Bayesian inference; Variational Bayes; Kalman filter; Bayesian filtering.  JEL Classification: C11, C22, C32, C53."},{"id":560,"title":"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo","url":"https://www.researchgate.net/publication/51956659_The_No-U-Turn_Sampler_Adaptively_Setting_Path_Lengths_in_HamiltonianMonte_Carlo","abstraction":"Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size {\\epsilon} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter {\\epsilon} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient \"turnkey\" sampling algorithms."},{"id":561,"title":"Approximate Bayesian Inference for Latent Gaussian Models by Using Integrated Nested Laplace Approximations","url":"https://www.researchgate.net/publication/46537493_Approximate_Bayesian_Inference_for_Latent_Gaussian_Models_by_Using_Integrated_Nested_Laplace_Approximations","abstraction":"Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, \"latent Gaussian models\", where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged. Copyright (c) 2009 Royal Statistical Society."},{"id":562,"title":"Accurate Approximations for Posterior Moments and Marginal Densities. — JASA, 81, 82-86","url":"https://www.researchgate.net/publication/243620229_Accurate_Approximations_for_Posterior_Moments_and_Marginal_Densities_-_JASA_81_82-86","abstraction":"This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal."},{"id":563,"title":"Inference From Iterative Simulation Using Multiple Sequences","url":"https://www.researchgate.net/publication/38363224_Inference_From_Iterative_Simulation_Using_Multiple_Sequences","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.\n</div> \n<p></p>"},{"id":564,"title":"Pseudo-Marginal Bayesian Inference for Gaussian Processes","url":"https://www.researchgate.net/publication/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The main challenges that arise when adopting Gaussian process priors in probabilistic modeling are how to carry out exact Bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. Using probit regression as an illustrative working example, this paper presents a general and effective methodology based on the pseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses both of these issues. The results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the Gaussian Process prior. This is particularly important as it offers a powerful tool to carry out full Bayesian inference of Gaussian Process based hierarchic statistical models in general. The results also demonstrate that Monte Carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions. Extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion.\n</div> \n<p></p>"},{"id":565,"title":"Scalable iterative methods for sampling from massive Gaussian random vectors","url":"https://www.researchgate.net/publication/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Sampling from Gaussian Markov random fields (GMRFs), that is multivariate\n <br> Gaussian ran- dom vectors that are parameterised by the inverse of their\n <br> covariance matrix, is a fundamental problem in computational statistics. In\n <br> this paper, we show how we can exploit arbitrarily accu- rate approximations to\n <br> a GMRF to speed up Krylov subspace sampling methods. We also show that these\n <br> methods can be used when computing the normalising constant of a large\n <br> multivariate Gaussian distribution, which is needed for both any\n <br> likelihood-based inference method. The method we derive is also applicable to\n <br> other structured Gaussian random vectors and, in particu- lar, we show that\n <br> when the precision matrix is a perturbation of a (block) circulant matrix, it\n <br> is still possible to derive O(n log n) sampling schemes.\n</div> \n<p></p>"},{"id":566,"title":"Kernelized Bayesian Matrix Factorization","url":"https://www.researchgate.net/publication/232906044_Kernelized_Bayesian_Matrix_Factorization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We extend kernelized matrix factorization with a fully Bayesian treatment and\n <br> with an ability to work with multiple side information sources expressed as\n <br> different kernels. Kernel functions have been introduced to matrix\n <br> factorization to integrate side information about the rows and columns (e.g.,\n <br> objects and users in recommender systems), which is necessary for making\n <br> out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite\n <br> graph inference, where the output matrix is binary, but extensions to more\n <br> general matrices are straightforward. We extend the state of the art in two key\n <br> aspects: (i) A fully conjugate probabilistic formulation of the kernelized\n <br> matrix factorization problem enables an efficient variational approximation,\n <br> whereas fully Bayesian treatments are not computationally feasible in the\n <br> earlier approaches. (ii) Multiple side information sources are included,\n <br> treated as different kernels in multiple kernel learning that additionally\n <br> reveals which side information sources are informative. Our method outperforms\n <br> alternatives in predicting drug-protein interactions on two data sets. We then\n <br> show that our framework can also be used for solving multilabel learning\n <br> problems by considering samples and labels as the two domains where matrix\n <br> factorization operates on. Our algorithm obtains the lowest Hamming loss values\n <br> on 10 out of 14 multilabel classification data sets compared to five\n <br> state-of-the-art multilabel learning algorithms.\n</div> \n<p></p>"},{"id":567,"title":"A case study of the widely applicable Bayesian information criterion and its optimality","url":"https://www.researchgate.net/publication/261637716_A_case_study_of_the_widely_applicable_Bayesian_information_criterion_and_its_optimality","abstraction":"In Bayesian statistics, the marginal likelihood (evidence) is one of the key factors that can be used as a measure of model goodness. However, for many practical model families it cannot be computed analytically. An alternative solution is to use some approximation method or time-consuming sampling method. The widely applicable Bayesian information criterion (WBIC) was developed recently to have a marginal likelihood approximation that works also with singular models. The central idea of the approximation is to select a single thermodynamic integration term (power posterior) with the (approximated) optimal temperature ?^?=1/log(n) , where n is the data size. We apply this new approximation to the analytically solvable Gaussian process regression case to show that the optimal temperature may depend also on data itself or other variables, such as the noise level. Moreover, we show that the steepness of a thermodynamic curve at the optimal temperature indicates the magnitude of the error that WBIC makes."},{"id":568,"title":"Bayesian Inference for Gaussian Process Classifiers with Annealing and Pseudo-Marginal MCMC","url":"https://www.researchgate.net/publication/286227230_Bayesian_Inference_for_Gaussian_Process_Classifiers_with_Annealing_and_Pseudo-Marginal_MCMC","abstraction":"Kernel methods have revolutionized the fields of pattern recognition and machine learning. Their success, however, critically depends on the choice of kernel parameters. Using Gaussian process (GP) classification as a working example, this paper focuses on Bayesian inference of covariance (kernel) parameters using Markov chain Monte Carlo (MCMC) methods. The motivation is that, compared to standard optimization of kernel parameters, they have been systematically demonstrated to be superior in quantifying uncertainty in predictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as a practical inference tool for GP models. In particular, it amounts in replacing the analytically intractable marginal likelihood by an unbiased estimate obtainable by approximate methods and importance sampling. After discussing the potential drawbacks in employing importance sampling, this paper proposes the application of annealed importance sampling. The results empirically demonstrate that compared to importance sampling, annealed importance sampling can reduce the variance of the estimate of the marginal likelihood exponentially in the number of data at a computational cost that scales only polynomially. The results on real data demonstrate that employing annealed importance sampling in the Pseudo-Marginal MCMC approach represents a step forward in the development of fully automated exact inference engines for GP models."},{"id":569,"title":"Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)","url":"https://www.researchgate.net/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In applications of Gaussian processes where quantification of uncertainty is\n <br> of primary interest, it is necessary to accurately characterize the posterior\n <br> distribution over covariance parameters. This paper proposes stochastic\n <br> gradient-based inference to draw samples from the posterior distribution over\n <br> covariance parameters with negligible bias and without the need to compute the\n <br> marginal likelihood. In Gaussian process regression, this has the enormous\n <br> advantage that stochastic gradients can be computed by solving linear systems\n <br> only. A novel unbiased linear systems solver based on parallelizable covariance\n <br> matrix-vector products is developed to accelerate the unbiased estimation of\n <br> gradients. The results demonstrate the possibility to enable scalable and exact\n <br> (in a Monte Carlo sense) quantification of uncertainty in Gaussian processes\n <br> without imposing any special structures on the covariance or reducing the\n <br> number of input vectors.\n</div> \n<p></p>"},{"id":570,"title":"MCMC for Variationally Sparse Gaussian Processes","url":"https://www.researchgate.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","abstraction":"Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper will be available shortly."},{"id":571,"title":"Health Informatics via Machine Learning for the Clinical Management of Patients","url":"https://www.researchgate.net/publication/281030543_Health_Informatics_via_Machine_Learning_for_the_Clinical_Management_of_Patients","abstraction":"To review how health informatics systems based on machine learning methods have impacted the clinical management of patients, by affecting clinical practice. We reviewed literature from 2010-2015 from databases such as Pubmed, IEEE xplore, and INSPEC, in which methods based on machine learning are likely to be reported. We bring together a broad body of literature, aiming to identify those leading examples of health informatics that have advanced the methodology of machine learning. While individual methods may have further examples that might be added, we have chosen some of the most representative, informative exemplars in each case. Our survey highlights that, while much research is taking place in this high-profile field, examples of those that affect the clinical management of patients are seldom found. We show that substantial progress is being made in terms of methodology, often by data scientists working in close collaboration with clinical groups. Health informatics systems based on machine learning are in their infancy and the translation of such systems into clinical management has yet to be performed at scale."},{"id":572,"title":"Deep Gaussian Processes","url":"https://www.researchgate.net/publication/232805135_Deep_Gaussian_Processes","abstraction":"In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GPLVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples."},{"id":573,"title":"LIBLINEAR: a library for large linear classification","url":"https://www.researchgate.net/publication/220320803_LIBLINEAR_a_library_for_large_linear_classification","abstraction":"LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regres- sion and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets."},{"id":574,"title":"Gaussian Processes for Big Data","url":"https://www.researchgate.net/publication/257069490_Gaussian_Processes_for_Big_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We introduce stochastic variational inference for Gaussian process models.\n <br> This enables the application of Gaussian process (GP) models to data sets\n <br> containing millions of data points. We show how GPs can be vari- ationally\n <br> decomposed to depend on a set of globally relevant inducing variables which\n <br> factorize the model in the necessary manner to perform variational inference.\n <br> Our ap- proach is readily extended to models with non-Gaussian likelihoods and\n <br> latent variable models based around Gaussian processes. We demonstrate the\n <br> approach on a simple toy problem and two real world data sets.\n</div> \n<p></p>"},{"id":575,"title":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data","url":"https://www.researchgate.net/publication/2869026_Gaussian_Process_Latent_Variable_Models_for_Visualisation_of_High_Dimensional_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we introduce a new underlying probabilistic model for principal component analysis (PCA). Our formulation interprets PCA as a particular Gaussian process prior on a mapping from a latent space to the observed data-space. We show that if the prior's covariance function constrains the mappings to be linear the model is equivalent to PCA, we then extend the model by considering less restrictive covariance functions which allow non-linear mappings. This more general Gaussian process latent variable model (GPLVM) is then evaluated as an approach to the visualisation of high dimensional data for three different data-sets.\n</div> \n<p></p>"},{"id":576,"title":"The Generalized FITC Approximation","url":"https://www.researchgate.net/publication/221618825_The_Generalized_FITC_Approximation","abstraction":"We present an efficient generalization of the sparse pseudo- input Gaussian pro- cess (SPGP) model developed by Snelson and Ghahramani (1), applying it to binary classification problems. By taking advantage of the S PGP prior covari- ance structure, we derive a numerically stable algorithm with O(NM2) training complexity—asymptotically the same as related sparse methods such as the in- formative vector machine (2), but which more faithfully represents the posterior. We present experimental results for several benchmark problems showing that in many cases this allows an exceptional degree of sparsity without compromis- ing accuracy. Following (1), we locate pseudo-inputs by gradient ascent on the marginal likelihood, but exhibit occasions when this is lik ely to fail, for which we suggest alternative solutions."},{"id":577,"title":"Approximations for Binary Gaussian Process Classification","url":"https://www.researchgate.net/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.\n</div> \n<p></p>"},{"id":578,"title":"Scikit-learn: Machine Learning in Python","url":"https://www.researchgate.net/publication/51969319_Scikit-learn_Machine_Learning_in_Python","abstraction":"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net."},{"id":579,"title":"Stochastic Expectation Propagation for Large Scale Gaussian Process Classification","url":"https://www.researchgate.net/publication/283762222_Stochastic_Expectation_Propagation_for_Large_Scale_Gaussian_Process_Classification","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A method for large scale Gaussian process classification has been recently\n <br> proposed based on expectation propagation (EP). Such a method allows Gaussian\n <br> process classifiers to be trained on very large datasets that were out of the\n <br> reach of previous deployments of EP and has been shown to be competitive with\n <br> related techniques based on stochastic variational inference. Nevertheless, the\n <br> memory resources required scale linearly with the dataset size, unlike in\n <br> variational methods. This is a severe limitation when the number of instances\n <br> is very large. Here we show that this problem is avoided when stochastic EP is\n <br> used to train the model.\n</div> \n<p></p>"},{"id":580,"title":"On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes","url":"https://www.researchgate.net/publication/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The variational framework for learning inducing variables Titsias (2009) has\n <br> had a large impact on the Gaussian process literature. The framework may be\n <br> interpreted as minimizing a rigorously defined Kullback-Leibler divergence\n <br> between the approximate and posterior processes. To our knowledge this\n <br> connection has thus far gone unremarked in the literature. Many of the\n <br> technical requirements for such a result were derived in the pioneering work of\n <br> Seeger (2003,2003b). In this work we give a relatively gentle and largely\n <br> self-contained explanation of the result. The result is important in\n <br> understanding the variational inducing framework and could lead to principled\n <br> novel generalizations.\n</div> \n<p></p>"},{"id":581,"title":"A Unifying View of Sparse Approximate Gaussian Process Regression","url":"https://www.researchgate.net/publication/41781406_A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.\n</div> \n<p></p>"},{"id":582,"title":"A Unifying View of Sparse Approximate Gaussian Process Regression","url":"https://www.researchgate.net/publication/41781406_A_Unifying_View_of_Sparse_Approximate_Gaussian_Process_Regression","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We provide a new unifying view, including all existing proper probabilistic sparse approximations for Gaussian process regression. Our approach relies on expressing the effective prior which the methods are using. This allows new insights to be gained, and highlights the relationship between existing methods. It also allows for a clear theoretically justified ranking of the closeness of the known approximations to the corresponding full GPs. Finally we point directly to designs of new better sparse approximations, combining the best of the existing strategies, within attractive computational constraints.\n</div> \n<p></p>"},{"id":583,"title":"Transductive and Inductive Methods for Approximate Gaussian Process Regression","url":"https://www.researchgate.net/publication/2561241_Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression","abstraction":"Gaussian process regression allows a simple analytical treatment of exact Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data. In this paper we compare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian committee machine. Furthermore we provide theoretical insight into some of our experimental results. We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels. On complex low noise data sets, the Bayesian committee machine achieves significantly better accuracy, yet at a higher computational cost."},{"id":584,"title":"A matching pursuit approach to sparse Gaussian process regression. Adv Neural Inf Process Syst","url":"https://www.researchgate.net/publication/221618449_A_matching_pursuit_approach_to_sparse_Gaussian_process_regression_Adv_Neural_Inf_Process_Syst","abstraction":"In this paper we propose a new basis selection criterion for building sparse GP regression models that provides promising gains in accuracy as well as efficiency over previous methods. Our algorithm is much faster than that of Smola and Bartlett, while, in generalization it greatly outper- forms the information gain approach proposed by Seeger et al, especially on the quality of predictive distributions."},{"id":585,"title":"Fast Forward Selection to Speed Up Sparse Gaussian Process Regression","url":"https://www.researchgate.net/publication/49459301_Fast_Forward_Selection_to_Speed_Up_Sparse_Gaussian_Process_Regression","abstraction":"We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the \"support\" patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically. We demonstrate the model selection capabilities of the algorithm in a range of experiments. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods."},{"id":586,"title":"Using the Nystroem Method to Speed Up Kernel Machines","url":"https://www.researchgate.net/publication/49459305_Using_the_Nystroem_Method_to_Speed_Up_Kernel_Machines","abstraction":"A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n ), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystr?m method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m &lt; n, and then..."},{"id":587,"title":"Sparse Bayesian Learning and the Relevance Vector Machine","url":"https://www.researchgate.net/publication/2413122_Sparse_Bayesian_Learning_and_the_Relevance_Vector_Machine","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classication tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the `relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art `support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while oering a number of additional advantages. These include the benets of probabilistic predictions, automatic estimation of `nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-`Mercer' kernels).\n</div> \n<p></p>"},{"id":588,"title":"The Nature Of Statistical Learning Theory","url":"https://www.researchgate.net/publication/5595969_The_Nature_Of_Statistical_Learning_Theory","abstraction":"First Page of the Article"},{"id":589,"title":"Efficient Multiscale Gaussian Process Regression using Hierarchical Clustering","url":"https://www.researchgate.net/publication/283659093_Efficient_Multiscale_Gaussian_Process_Regression_using_Hierarchical_Clustering","abstraction":"Standard Gaussian Process (GP) regression, a powerful machine learning tool, is computationally expensive when it is applied to large datasets, and potentially inaccurate when data points are sparsely distributed in a high-dimensional feature space. To address these challenges, a new multiscale, sparsified GP algorithm is formulated, with the goal of application to large scientific computing datasets. In this approach, the data is partitioned into clusters and the cluster centers are used to define a reduced training set, resulting in an improvement over standard GPs in terms of training and evaluation costs. Further, a hierarchical technique is used to adaptively map the local covariance representation to the underlying sparsity of the feature space, leading to improved prediction accuracy when the data distribution is highly non-uniform. A theoretical investigation of the computational complexity of the algorithm is presented. The efficacy of this method is then demonstrated on simple analytical functions and on data from a direct numerical simulation of turbulent combustion."},{"id":590,"title":"Real-Time Energy Prediction for a Milling Machine Tool Using Sparse Gaussian Process Regression","url":"https://www.researchgate.net/publication/286418563_Real-Time_Energy_Prediction_for_a_Milling_Machine_Tool_Using_Sparse_Gaussian_Process_Regression","abstraction":"This paper describes a real-time data collection framework and an adaptive machining learning method for constructing a real-time energy prediction model for a machine tool. To effectively establish the energy consumption pattern of a machine tool over time, the energy prediction model is continuously updated with new measurement data to account for time-varying effects of the machine tool, such as tool wear and machine tool deterioration. In this work, a real-time data collection and processing framework is developed to retrieve raw data from a milling machine tool and its sensors and convert them into relevant input features. The extracted input features are then used to construct the energy prediction model using Gaussian Process (GP) regression. To update the GP regression model with real-time streaming data, we investigate the use of sparse representation of the covariance matrix to reduce the computational and storage demands of the GP regression. We compare computational efficiency of sparse GP to that of full GP regression model and show the effectiveness of the sparse GP regression model for tracking the variation in the energy consumption pattern of the target machine. (Abstract)"},{"id":591,"title":"A multi-resolution approximation for massive spatial datasets","url":"https://www.researchgate.net/publication/280221189_A_multi-resolution_approximation_for_massive_spatial_datasets","abstraction":"Automated sensing instruments on satellites and aircraft have enabled the collection of massive amounts of high-resolution observations of spatial fields over large spatial regions. If these datasets can be efficiently exploited, they can provide new insights on a wide variety of issues. However, traditional spatial-statistical techniques such as kriging are not computationally feasible for big datasets. We propose a multi-resolution approximation (M-RA) of Gaussian processes observed at irregular locations in space. The M-RA process is specified as a linear combination of basis functions at multiple levels of spatial resolution, which can capture spatial structure from very fine to very large scales. The basis functions are automatically chosen to approximate a given covariance function, which can be nonstationary. All computations involving the M-RA, including parameter inference and prediction, are highly scalable for massive datasets. Crucially, the inference algorithms can also be parallelized to take full advantage of large distributed-memory computing environments. In comparisons using simulated data and a large satellite dataset, the M-RA outperforms a related state-of-the-art method."},{"id":592,"title":"Lie Algebra-Based Kinematic Prior for 3D Human Pose Tracking","url":"https://www.researchgate.net/publication/282929548_Lie_Algebra-Based_Kinematic_Prior_for_3D_Human_Pose_Tracking","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a novel kinematic prior for 3D human pose tracking that allows predicting the position in subsequent frames given the current position. We first define a Riemannian manifold that models the pose and extend it with its Lie algebra to also be able to represent the kinematics. We then learn a joint Gaussian mixture model of both the human pose and the kinematics on this manifold. Finally by conditioning the kinematics on the pose we are able to obtain a distribution of poses for subsequent frames that which can be used as a reliable prior in 3D human pose tracking. Our model scales well to large amounts of data and can be sampled at over 100,000 samples/second. We show it outperforms the widely used Gaussian diffusion model on the challenging Human3.6M dataset.\n</div> \n<p></p>"},{"id":593,"title":"Dynamic forward prediction for prosthetic hand control by integration of EMG, MMG and kinematic signals","url":"https://www.researchgate.net/publication/281432559_Dynamic_forward_prediction_for_prosthetic_hand_control_by_integration_of_EMG_MMG_and_kinematic_signals","abstraction":"We propose a new framework for extracting information from extrinsic muscles in the forearm that will allow a continuous, natural and intuitive control of a neuroprosthetic devices and robotic hands. This is achieved through a continuous mapping between muscle activity and joint angles rather than prior discretisation of hand gestures. We instructed 6 able-bodied subjects, to perform everyday object manipulation tasks. We recorded the Electromyographic (EMG) and Mechanomyographic (MMG) activities of 5 extrinsic muscles of the hand in their forearm, while simultaneously monitoring 11 joints of hand and fingers using a sensorised glove. We used these signals to train a Gaussian Process (GP) and a Vector AutoRegressive Moving Average model with Exogenous inputs (VARMAX) to learn the mapping from current muscle activity and current joint state to predict future hand configurations. We investigated the performances of both models across tasks, subjects and different joints for varying time-lags, finding that both models have good generalisation properties and high correlation even for time-lags in the order of hundreds of milliseconds. Our results suggest that regression is a very appealing tool for natural, intuitive and continuous control of robotic devices, with particular focus on prosthetic replacements where high dexterity is required for complex movements."},{"id":594,"title":"Gaussian Process Regression for accurate prediction of prosthetic limb movements from the natural kinematics of intact limbs","url":"https://www.researchgate.net/publication/281432557_Gaussian_Process_Regression_for_accurate_prediction_of_prosthetic_limb_movements_from_the_natural_kinematics_of_intact_limbs","abstraction":"We propose a Gaussian Process-based regression framework for continuous prediction of the state of missing limbs by exclusively decoding missing limb movements from intact limbs - we achieve this as we have measured the correlation structure and synergies of natural limb kinematics in daily life. Using the example of hand neuroprosthetic, we demonstrate how our model can use non-linear regression to infer the velocity of the flexion/extension joints of missing fingers by observing the intact joints using a data glove. We based our framework on hand joint velocity data, that we recorded with a sensorised glove from 7 able-bodied subjects performing everyday hand movements. We then simulate missing fingers by making our regressors predict the motion that a neuroprosthetic finger should execute based on the previously observed movements of intact fingers. Perhaps surprisingly, we achieve and R2 = 0.89 and an RMSE = 0.20°/s across all missing joints. Moreover, by performing one-subject-out cross validation, we can show that the prediction accuracy and precision has negligible significant loss of performance when tested on new subjects. This suggests that kinematic correlations in daily life can provide a powerful channel refining, if not driving, multi-source neuroprosthetic and Brain Computer Interface approaches."},{"id":595,"title":"Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)","url":"https://www.researchgate.net/publication/273157807_Kernel_Interpolation_for_Scalable_Structured_Gaussian_Processes_KISS-GP","abstraction":"We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling."},{"id":596,"title":"Optimal Scaling for Various Metropolis-Hastings Algorithms","url":"https://www.researchgate.net/publication/38326865_Optimal_Scaling_for_Various_Metropolis-Hastings_Algorithms","abstraction":"We review and extend results related to optimal scaling of Metropolis–Hastings algorithms. We present various theoretical results for the high-dimensional limit. We also present simulation studies which confirm the theoretical results in finite-dimensional contexts."},{"id":597,"title":"Optimal scaling of discrete approximations to Langevin diffusions. J R Stat Soc Ser B (Stat Methodol)","url":"https://www.researchgate.net/publication/227672958_Optimal_scaling_of_discrete_approximations_to_Langevin_diffusions_J_R_Stat_Soc_Ser_B_Stat_Methodol","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the optimal scaling problem for proposal distributions in Hastings–Metropolis algorithms derived from Langevin diffusions. We prove an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterized by its overall acceptance rate, independently of the target distribution. The asymptotically optimal acceptance rate is 0.574. We show that, as a function of dimension n, the complexity of the algorithm is O(n1/3), which compares favourably with the O(n) complexity of random walk Metropolis algorithms. We illustrate this comparison with some example simulations.\n</div> \n<p></p>"},{"id":598,"title":"Bayesian Prediction of Spatial Count Data Using Generalised Linear Mixed Models","url":"https://www.researchgate.net/publication/11304144_Bayesian_Prediction_of_Spatial_Count_Data_Using_Generalised_Linear_Mixed_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Introduction Site specic farming is aiming at targeting inputs of fertiliser, pesticide, and herbicide according to locally determined requirements. In connection with herbicide application on a eld, it is important to map the weed intensity so that the dose of herbicide applied at any location can be adjusted to the amount of weed present at the location. In a Danish project on precision farming (Olesen, 1997) one objective was to investigate whether observations of soil properties could be used for prediction of weed intensity. In practice the farmer or his advisor should then establish a relation between soil properties and weed occurrence from extensive observations collected one year and use this for prediction of the weed intensity in subsequent years where only a limited number of weed count observations would be 1 collected. Many soil properties are fairly constant over time so that observations of soil samples obtained the rst year can also be used in subseq\n</div> \n<p></p>"},{"id":599,"title":"Langevin-Type Models I: Diffusions with Given Stationary Distributions and their Discretizations*","url":"https://www.researchgate.net/publication/226046683_Langevin-Type_Models_I_Diffusions_with_Given_Stationary_Distributions_and_their_Discretizations","abstraction":"We describe algorithms for estimating a given measure known up to a constant of proportionality, based on a large class of diffusions (extending the Langevin model) for which is invariant. We show that under weak conditions one can choose from this class in such a way that the diffusions converge at exponential rate to , and one can even ensure that convergence is independent of the starting point of the algorithm. When convergence is less than exponential we show that it is often polynomial at verifiable rates. We then consider methods of discretizing the diffusion in time, and find methods which inherit the convergence rates of the continuous time process. These contrast with the behavior of the naive or Euler discretization, which can behave badly even in simple cases. Our results are described in detail in one dimension only, although extensions to higher dimensions are also briefly described."},{"id":600,"title":"Exponential Convergence of Langevin Distributions and Their Discrete Approximations","url":"https://www.researchgate.net/publication/38373539_Exponential_Convergence_of_Langevin_Distributions_and_Their_Discrete_Approximations","abstraction":"In this paper we consider a continuous-time method of approximating a given distribution [math] using the Langevin diffusion [math] . We find conditions under which this diffusion converges exponentially quickly to [math] or does not: in one dimension, these are essentially that for distributions with exponential tails of the form [math] , [math] , exponential convergence occurs if and only if [math] . We then consider conditions under which the discrete approximations to the diffusion converge. We first show that even when the diffusion itself converges, naive discretizations need not do so. We then consider a 'Metropolis-adjusted' version of the algorithm, and find conditions under which this also converges at an exponential rate: perhaps surprisingly, even the Metropolized version need not converge exponentially fast even if the diffusion does. We briefly discuss a truncated form of the algorithm which, in practice, should avoid the difficulties of the other forms."},{"id":601,"title":"Optimal scaling for the transient phase of Metropolis Hastings algorithms: The longtime behavior","url":"https://www.researchgate.net/publication/233967527_Optimal_scaling_for_the_transient_phase_of_Metropolis_Hastings_algorithms_The_longtime_behavior","abstraction":"We consider the Random Walk Metropolis algorithm on $\\R^n$ with Gaussian proposals, and when the target probability measure is the $n$-fold product of a one dimensional law. It is well-known (see Roberts et al. (1997))) that, in the limit $n \\to \\infty$, starting at equilibrium and for an appropriate scaling of the variance and of the timescale as a function of the dimension $n$, a diffusive limit is obtained for each component of the Markov chain. In Jourdain et al. (2012), we generalize this result when the initial distribution is not the target probability measure. The obtained diffusive limit is the solution to a stochastic differential equation nonlinear in the sense of McKean. In the present paper, we prove convergence to equilibrium for this equation. We discuss practical counterparts in order to optimize the variance of the proposal distribution to accelerate convergence to equilibrium. Our analysis confirms the interest of the constant acceptance rate strategy (with acceptance rate between 1/4 and 1/3) first suggested in Roberts et al. (1997). We also address scaling of the Metropolis-Adjusted Langevin Algorithm. When starting at equilibrium, a diffusive limit for an optimal scaling of the variance is obtained in Roberts and Rosenthal (1998). In the transient case, we obtain formally that the optimal variance scales very differently in $n$ depending on the sign of a moment of the distribution, which vanishes at equilibrium. This suggest that it is difficult to derive practical recommendations for MALA from such asymptotic results."},{"id":602,"title":"Optimal scaling for the transient phase of the random walk Metropolis algorithm: The mean-field limit","url":"https://www.researchgate.net/publication/232718818_Optimal_scaling_for_the_transient_phase_of_the_random_walk_Metropolis_algorithm_The_mean-field_limit","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the random walk Metropolis algorithm on $\\R^n$ with Gaussian\n <br> proposals, and when the target probability is the $n$-fold product of a one\n <br> dimensional law. In the limit $n \\to \\infty$, it is well-known that, when the\n <br> variance of the proposal scales inversely proportional to the dimension $n$\n <br> whereas time is accelerated by the factor $n$, a diffusive limit is obtained\n <br> for each component of the Markov chain if this chain starts at equilibrium.\n <br> This paper extends this result when the initial distribution is not the target\n <br> probability measure. Remarking that the interaction between the components of\n <br> the chain due to the common acceptance/rejection of the proposed moves is of\n <br> mean-field type, we obtain a propagation of chaos result under the same scaling\n <br> as in the stationary case. This proves that, in terms of the dimension $n$, the\n <br> same scaling holds for the transient phase of the Metropolis-Hastings algorithm\n <br> as near stationarity. The diffusive and mean-field limit of each component is a\n <br> nonlinear diffusion process in the sense of McKean. This opens the route to new\n <br> investigations of the optimal choice for the variance of the proposal\n <br> distribution in order to accelerate convergence to equilibrium.\n</div> \n<p></p>"},{"id":603,"title":"Riemannian Manifold Hamiltonian Monte Carlo","url":"https://www.researchgate.net/publication/45860846_Riemannian_Manifold_Hamiltonian_Monte_Carlo","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The paper proposes a Riemannian Manifold Hamiltonian Monte Carlo sampler to\n <br> resolve the shortcomings of existing Monte Carlo algorithms when sampling from\n <br> target densities that may be high dimensional and exhibit strong correlations.\n <br> The method provides a fully automated adaptation mechanism that circumvents the\n <br> costly pilot runs required to tune proposal densities for Metropolis-Hastings\n <br> or indeed Hybrid Monte Carlo and Metropolis Adjusted Langevin Algorithms. This\n <br> allows for highly efficient sampling even in very high dimensions where\n <br> different scalings may be required for the transient and stationary phases of\n <br> the Markov chain. The proposed method exploits the Riemannian structure of the\n <br> parameter space of statistical models and thus automatically adapts to the\n <br> local manifold structure at each step based on the metric tensor. A\n <br> semi-explicit second order symplectic integrator for non-separable Hamiltonians\n <br> is derived for simulating paths across this manifold which provides highly\n <br> efficient convergence and exploration of the target density. The performance of\n <br> the Riemannian Manifold Hamiltonian Monte Carlo method is assessed by\n <br> performing posterior inference on logistic regression models, log-Gaussian Cox\n <br> point processes, stochastic volatility models, and Bayesian estimation of\n <br> parameter posteriors of dynamical systems described by nonlinear differential\n <br> equations. Substantial improvements in the time normalised Effective Sample\n <br> Size are reported when compared to alternative sampling approaches. Matlab code\n <br> at \\url{http://www.dcs.gla.ac.uk/inference/rmhmc} allows replication of all\n <br> results.\n</div> \n<p></p>"},{"id":604,"title":"Optimal acceptance rates for Metropolis algorithms: Moving beyond 0.234","url":"https://www.researchgate.net/publication/222298844_Optimal_acceptance_rates_for_Metropolis_algorithms_Moving_beyond_0234","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Recent optimal scaling theory has produced a condition for the asymptotically optimal acceptance rate of Metropolis algorithms to be the well-known 0.234 when applied to certain multi-dimensional target distributions. These d-dimensional target distributions are formed of independent components, each of which is scaled according to its own function of d. We show that when the condition is not met the limiting process of the algorithm is altered, yielding an asymptotically optimal acceptance rate which might drastically differ from the usual 0.234. Specifically, we prove that as d?? the sequence of stochastic processes formed by say the i?th component of each Markov chain usually converges to a Langevin diffusion process with a new speed measure ?, except in particular cases where it converges to a one-dimensional Metropolis algorithm with acceptance rule ??. We also discuss the use of inhomogeneous proposals, which might prove to be essential in specific cases.\n</div> \n<p></p>"},{"id":605,"title":"Optimal scaling of Metropolis algorithms: is 0.234 as robust as believed?","url":"https://www.researchgate.net/publication/277289463_Optimal_scaling_of_Metropolis_algorithms_is_0234_as_robust_as_believed","abstraction":"The Metropolis algorithm with Gaussian proposal distribution is a popular sampling method; it is versatile and easy to implement. Optimal scaling theory aims to improve the speed of convergence of this algorithm to its stationary distribution by carefully selecting its tuning parameter. This paper is an overview of existing optimal scaling results and addresses in more depth the case of high-dimensional target distributions formed of independent, but not identically distributed components. It attempts to give an intuitive explanation as to when the previously-derived optimal acceptance rate of 0.234 is indeed optimal, and when it is unsuitable. In the latter case, it also explains how to find the correct asymptotically optimal acceptance rate, and why we sometimes have to turn to inhomogeneous proposal variances in order to obtain an efficient algorithm. This is all illustrated with a simple example."},{"id":606,"title":"Optimal Scaling of Mala for Nonlinear Regression","url":"https://www.researchgate.net/publication/2113732_Optimal_Scaling_of_Mala_for_Nonlinear_Regression","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We address the problem of simulating efficiently from the posterior distribution over the parameters of a particular class of nonlinear regression models using a Langevin-Metropolis sampler. It is shown that as the number N of parameters increases, the proposal variance must scale as N{-1/3} in order to converge to a diffusion. This generalizes previous results of Roberts and Rosenthal [J. R. Stat. Soc. Ser. B Stat. Methodol. 60 (1998) 255-268] for the i.i.d. case, showing the robustness of their analysis.\n</div> \n<p></p>"},{"id":607,"title":"Nearest-Neighbour Markov Point Processes and Random Sets","url":"https://www.researchgate.net/publication/265375218_Nearest-Neighbour_Markov_Point_Processes_and_Random_Sets","abstraction":"The Markov point processes introduced by B. D. Ripley and F. P. Kelly [J. Lond. Math. Soc., II. Ser. 15, 188-192 (1977; Zbl 0354.60037)] are generalized by replacing fixed-range spatial interactions by interactions between neighbouring particles, where the neighbourhood relation may depend on the point configuration. The corresponding Hammersley-Clifford characterization theorem is proved. The results are used to construct new models for point processes, multitype point processes and random processes of geometrical objects. Monte Carlo simulations of these models can be generated by running spatial birth- and-death processes."},{"id":608,"title":"Non?parametric Bayesian Estimation of a Spatial Poisson Intensity","url":"https://www.researchgate.net/publication/227686996_Non-parametric_Bayesian_Estimation_of_a_Spatial_Poisson_Intensity","abstraction":"A method introduced by Arjas &amp; Gasbarra (1994) and later modified by Arjas &amp; Heikkinen (1997) for the non-parametric Bayesian estimation of an intensity on the real line is generalized to cover spatial processes. The method is based on a model approximation where the approximating intensities have the structure of a piecewise constant function. Random step functions on the plane are generated using Voronoi tessellations of random point patterns. Smoothing between nearby intensity values is applied by means of a Markov random field prior in the spirit of Bayesian image analysis. The performance of the method is illustrated in examples with both real and simulated data."},{"id":609,"title":"On the Problem of Permissible Covariance and Variogram Models","url":"https://www.researchgate.net/publication/240489008_On_the_Problem_of_Permissible_Covariance_and_Variogram_Models","abstraction":"The covariance and variogram models (ordinary or generalized) are important statistical tools used in various estimation and simulation techniques which have been recently applied to diverse hydrologic problems. For example, the efficacy of kriging, a method for interpolating, filtering, or averaging spatial phenomena, depends, to a large extent, on the covariance or variogram model chosen. The aim of this article is to provide the users of these techniques with convenient criteria that may help them to judge whether a function which arises in a particular problem, and is not included among the known covariance or variogram models, is permissible as such a model. This is done by investigating the properties of the candidate model in both the space and frequency domains. In the present article this investigation covers stationary random functions as well as intrinsic random functions (i.e., nonstationary functions for which increments of some order are stationary). Then, based on the theoretical results obtained, a procedure is outlined and successfully applied to a number of candidate models. In order to give to this procedure a more practical context, we employ \"stereological\" equations that essentially transfer the investigations to one-dimensional space, together with approximations in terms of polygonal functions and Fourier-Bessel series expansions. There are many benefits and applications of such a procedure. Polygonal models can be fit arbitrarily closely to the data. Also, the approximation of a particular model in the frequency domain by a Fourier-Bessel series expansion can be very effective. This is shown by theory and by example."},{"id":610,"title":"Markov Properties of Cluster Processes","url":"https://www.researchgate.net/publication/2437916_Markov_Properties_of_Cluster_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We show that a Poisson cluster point process is a nearest-neighbour Markov point process [2] if the clusters have uniformly bounded diameter. It is typically not a finite-range Markov point process in the sense of Ripley and Kelly [12]. Furthermore, when the parent Poisson process is replaced by a Markov or nearest-neighbour Markov point process, the resulting cluster process is also nearest-neighbour Markov, provided all clusters are nonempty. In particular, the nearest-neighbour Markov property is preserved when points of the process are independently randomly translated, but not when they are randomly thinned. 1. Introduction Markov or Gibbs point processes [2, 9, 12, 13] form a large, flexible, and understandable class of point process models with many practical advantages (see e.g. [4, 10, 11] for surveys). In this paper we consider the relationship of these models to the basic point process operation of clustering. We ask whether cluster processes are Markov, and whether the Mar...\n</div> \n<p></p>"},{"id":611,"title":"Area-interaction point process","url":"https://www.researchgate.net/publication/226698248_Area-interaction_point_process","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We introduce a new Markov point process that exhibits a range of clustered, random, and ordered patterns according to the value of a scalar parameter. In contrast to pairwise interaction processes, this model has interaction terms of all orders. The likelihood is closely related to the empty space functionF, paralleling the relation between the Strauss process and Ripley'sK-function. We show that, in complete analogy with pairwise interaction processes, the pseudolikelihood equations for this model are a special case of the Takacs-Fiksel method, and our model is the limit of a sequence of auto-logistic lattice processes.\n</div> \n<p></p>"},{"id":612,"title":"Pseudolikelihood for Exponential Family Models of Spatial Point Processes","url":"https://www.researchgate.net/publication/38363003_Pseudolikelihood_for_Exponential_Family_Models_of_Spatial_Point_Processes","abstraction":"The pseudolikelihood for general spatial point processes including marked point processes is derived, and some of its properties are investigated. In particular, we prove consistency of the maximum pseudolikelihood estimates for Markov processes of finite range."},{"id":613,"title":"Geometric Convergence and Central Limit Theorems for Multidimensional Hastings and Metropolis Algorithms","url":"https://www.researchgate.net/publication/2349384_Geometric_Convergence_and_Central_Limit_Theorems_for_Multidimensional_Hastings_and_Metropolis_Algorithms","abstraction":"We develop results on geometric ergodicity of Markov chains and apply these and other recent results in Markov chain theory to multi-dimensional Hastings and Metropolis algorithms. For those based on random walk candidate distributions, we find sufficient conditions for moments and for moment generating functions to converge at a geometric rate to a prescribed distribution . By phrasing the conditions in terms of the curvature of the densities we show that the results apply to all distributions with positive density of the form (x) = h(x) exp(p(x)) where h and p are polynomials on IR d and p has an appropriate \"negativedefiniteness \" property. From these results we further develop central limit theorems for the Metropolis algorithm. Converse results, showing non-geometric convergence rates for chains where the rejection rate is not bounded from unity, are also given; these show that the negative-definiteness property is not redundant. Work supported in part by NSF Grant DMS-920568..."},{"id":614,"title":"Stochastic geometry models in high-level vision","url":"https://www.researchgate.net/publication/243615858_Stochastic_geometry_models_in_high-level_vision","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We survey the use of Markov models from stochastic geometry as priors in ‘high-level’ computer vision, in direct analogy with the use of discrete Markov random fields in ‘low-level’ vision. There are analogues of the Gibbs sampler, ICM and simulated annealing, and connections with existing methods in computer vision.\n</div> \n<p></p>"},{"id":615,"title":"The Intrinsic Random Functions and Their Applications","url":"https://www.researchgate.net/publication/230837892_The_Intrinsic_Random_Functions_and_Their_Applications","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The intrinsic random functions (IRF) are a particular case of the Guelfand generalized processes with stationary increments. They constitute a much wider class than the stationary RF, and are used in practical applications for representing non-stationary phenomena. The most important topics considered are: existence of a generalized covariance (GC) for which statistical inference is possible from a unique realization; theory of the best linear intrinsic estimator (BLIE) used for contouring and estimating problems; the turning bands method for simulating IRF; and the models with polynomial GC, for which statistical inference may be performed by automatic procedures.\n</div> \n<p></p>"},{"id":616,"title":"Exploitation by Informed Exploration between Isolated Operatives for Information-Theoretic Data Harvesting","url":"https://www.researchgate.net/publication/281846507_Exploitation_by_Informed_Exploration_between_Isolated_Operatives_for_Information-Theoretic_Data_Harvesting","abstraction":"We consider the problem of ferrying data between nodes of a sparsely distributed sensing network of Unattended Ground Sensors (UGS) with endurance-constrained Unmanned Aerial Systems (UAS). The sensing domain wherein the sparsely distributed UGS network is deployed is assumed to be highly nonstationary (time-varying) and noisy. This makes the data-ferrying problem very complicated as the expected value-of-information at a sensing location can rapidly change. To address this issue, we present a new data ferrying algorithm termed Exploitation by Informed Exploration between Isolated Operatives (EIEIO), and show that with several reasonable assumptions and a model on the predicted accumulation of value-of-information, the problem can be simplified to a mathematical linear program. To solve the linear program, the UAS learns to anticipate regions in the sensing domain that have the highest degree of change. The degree of change, is learned using a novel implementation of a Cox Process called the Cox-Gaussian Process (CGP). Our approach does not require a priori knowledge of the sensing domain model to arrive at an optimal UAS allocation strategy."},{"id":617,"title":"Modelling the oceanic habitats of two pelagic species using recreational fisheries data","url":"https://www.researchgate.net/publication/282073129_Modelling_the_oceanic_habitats_of_two_pelagic_species_using_recreational_fisheries_data","abstraction":"Defining the oceanic habitats of migratory marine species is important for both single species and ecosystem-based fisheries management, particularly when the distribution of these habitats vary temporally. This can be achieved using species distribution models that include physical environmental predictors. In the present study, species distribution models that describe the seasonal habitats of two pelagic fish (dolphinfish, Coryphaena hippurus and yellowtail kingfish, Seriola lalandi), are developed using 19 yr of presence-only data from a recreational angler-based catch-and-release fishing programme. A Poisson point process model within a generalized additive modelling framework was used to determine the species distributions off the east coast of Australia as a function of several oceanographic covariates. This modelling framework uses presence-only data to determine the intensity of fish (fish km?2), rather than a probability of fish presence. Sea surface temperature (SST), sea level anomaly, SST frontal index and eddy kinetic energy were significant environmental predictors for both dolphinfish and kingfish distributions. Models for both species indicate a greater fish intensity off the east Australian coast during summer and autumn in response to the regional oceanography, namely shelf incursions by the East Australian Current. This study provides a framework for using presence-only recreational fisheries data to create species distribution models that can contribute to the future dynamic spatial management of pelagic fisheries."},{"id":618,"title":"Multivariate product?shot?noise Cox point process models","url":"https://www.researchgate.net/publication/279308825_Multivariate_product-shot-noise_Cox_point_process_models","abstraction":"We introduce a new multivariate product-shot-noise Cox process which is useful for modeling multi-species spatial point patterns with clustering intra-specific interactions and neutral, negative, or positive inter-specific interactions. The auto- and cross-pair correlation functions of the process can be obtained in closed analytical forms and approximate simulation of the process is straightforward. We use the proposed process to model interactions within and among five tree species in the Barro Colorado Island plot. © 2015, The International Biometric Society."},{"id":619,"title":"Exact Bayesian inference in spatio-temporal Cox processes driven by multivariate Gaussian processes","url":"https://www.researchgate.net/publication/275588369_Exact_Bayesian_inference_in_spatio-temporal_Cox_processes_driven_by_multivariate_Gaussian_processes","abstraction":"In this paper we present a novel inference methodology to perform Bayesian inference for spatio-temporal Cox processes where the intensity function depends on a multivariate Gaussian process. Dynamic Gaussian processes are introduced to allow for evolution of the intensity function over discrete time. The novelty of the method lies on the fact that no discretisation error is involved despite the non-tractability of the likelihood function and infinite dimensionality of the problem. The method is based on a Markov chain Monte Carlo algorithm that samples from the joint posterior distribution of the parameters and latent variables of the model. A particular choice of the dominating measure to obtain the likelihood function is shown to be crucial to devise a valid MCMC. The models are defined in a general and flexible way but they are amenable to direct sampling from the relevant distributions, due to careful characterisation of its components. The models also allow for the inclusion of regression covariates and/or temporal components to explain the variability of the intensity function. These components may be subject to relevant interaction with space and/or time. Simulated examples illustrate the methodology, followed by concluding remarks."},{"id":620,"title":"Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods","url":"https://www.researchgate.net/publication/280224897_Fast_Kronecker_Inference_in_Gaussian_Processes_with_non-Gaussian_Likelihoods","abstraction":"Gaussian processes (GPs) are a flexible class of methods with state of the art performance on spatial statistics applications. However, GPs require O(n 3) computations and O(n 2) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance matrix for scalability, while allowing for expressive kernel learning (Wilson et al., 2014). However , fast Kronecker methods have been confined to Gaussian likelihoods. We propose new scal-able Kronecker methods for Gaussian processes with non-Gaussian likelihoods, using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning. Our approach has near linear scaling, requiring O(Dn D+1 D) operations and O(Dn 2 D) storage , for n training data-points on a dense D &gt; 1 dimensional grid. Moreover, we introduce a log Gaussian Cox process, with highly expressive kernels, for modelling spatiotemporal count processes, and apply it to a point pattern (n = 233,088) of a decade of crime events in Chicago. Using our model, we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts."},{"id":621,"title":"Optimality of Poisson processes intensity learning with Gaussian processes","url":"https://www.researchgate.net/publication/265787713_Optimality_of_Poisson_processes_intensity_learning_with_Gaussian_processes","abstraction":"In this paper we provide theoretical support for the so-called \"Sigmoidal Gaussian Cox Process\" approach to learning the intensity of an inhomogeneous Poisson process on a $d$-dimensional domain. This method was proposed by Adams, Murray and MacKay (ICML, 2009), who developed a tractable computational approach and showed in simulation and real data experiments that it can work quite satisfactorily. The results presented in the present paper provide theoretical underpinning of the method. In particular, we show how to tune the priors on the hyper parameters of the model in order for the procedure to automatically adapt to the degree of smoothness of the unknown intensity and to achieve optimal convergence rates."},{"id":622,"title":"A Bayesian hierarchical spatial point process model for multi-type neuroimaging meta-analysis","url":"https://www.researchgate.net/publication/268875953_A_Bayesian_hierarchical_spatial_point_process_model_for_multi-type_neuroimaging_meta-analysis","abstraction":"Neuroimaging meta-analysis is an important tool for finding consistent effects over studies that each usually have 20 or fewer subjects. Interest in meta-analysis in brain mapping is also driven by a recent focus on so-called \"reverse inference\": where as traditional \"forward inference\" identifies the regions of the brain involved in a task, a reverse inference identifies the cognitive processes that a task engages. Such reverse inferences, however, requires a set of meta-analysis, one for each possible cognitive domain. However, existing methods for neuroimaging meta-analysis have significant limitations. Commonly used methods for neuroimaging meta-analysis are not model based, do not provide interpretable parameter estimates, and only produce null hypothesis inferences; further, they are generally designed for a single group of studies and cannot produce reverse inferences. In this work we address these limitations by adopting a non-parametric Bayesian approach for meta analysis data from multiple classes or types of studies. In particular, foci from each type of study are modeled as a cluster process driven by a random intensity function that is modeled as a kernel convolution of a gamma random field. The type-specific gamma random fields are linked and modeled as a realization of a common gamma random field, shared by all types, that induces correlation between study types and mimics the behavior of a univariate mixed effects model. We illustrate our model on simulation studies and a meta analysis of five emotions from 219 studies and check model fit by a posterior predictive assessment. In addition, we implement reverse inference by using the model to predict study type from a newly presented study. We evaluate this predictive performance via leave-one-out cross validation that is efficiently implemented using importance sampling techniques."},{"id":623,"title":"A J?function for Inhomogeneous Spatio?temporal Point Processes","url":"https://www.researchgate.net/publication/258849829_A_J-function_for_Inhomogeneous_Spatio-temporal_Point_Processes","abstraction":"We propose a new summary statistic for inhomogeneous intensity-reweighted moment stationary spatio-temporal point processes. The statistic is defined through the n-point correlation functions of the point process and it generalises the J-function when stationarity is assumed. We show that our statistic can be represented in terms of the generating functional and that it is related to the inhomogeneous K-function. We further discuss its explicit form under some specific model assumptions and derive a ratio-unbiased estimator. We finally illustrate the use of our statistic on simulated data."},{"id":624,"title":"Marine reserve spillover: Modelling from multiple data sources","url":"https://www.researchgate.net/publication/258630581_Marine_reserve_spillover_Modelling_from_multiple_data_sources","abstraction":"The functional formof spillover,measured as a gradient of abundance of fish,may provide insight about processes that control the spatial distribution of fish inside and outside theMPA. In this study,we aimed to infer on spillover mechanism of Diplodus spp. (family Sparidae) from a Mediterranean MPA (Carry-le-Rouet, France) from visual censuses and artisanal fisheries data. From the existing literature, three potential functional forms of spillover such as a linear gradient, an exponential gradient and a logistic gradient are defined. Each functional form is included in a spatial generalized linear mixed model allowing accounting for spatial autocorrelation of data. We select between the different forms of gradients by using a Bayesian model selection procedure. In a first step, the functional form of the spillover for visual census and artisanal fishing data is assessed separately. For both sets of data, ourmodel selection favoured the negative exponentialmodel, evidencing a decrease of the spatial abundance of fish vanishing around 1000 m from the MPA border. We combined both datasets in a joint model by including an observability parameter. This parameter captures how the different sources of data quantify the underlying spatial distribution of the harvested species. This enabled us to demonstrate that the different sampling methods do not affect the estimation of the underlying spatial distribution of Diplodus spp. inside and outside the MPA. We show that data from different sources can be pooled through spatial generalized linear mixedmodel. Our findings allow to better understand the underlying mechanisms that control spillover of fish from MPA."},{"id":625,"title":"Bayesian Gaussian Process Latent Variable Model.","url":"https://www.researchgate.net/publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model","abstraction":"We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input vari- ables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maxi- mization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the di- mensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality re- duction problems, but the methodology is more general. For example, our algorithm is imme- diately applicable for training Gaussian process models in the presence of missing or uncertain inputs."},{"id":626,"title":"Variational Learning of Inducing Variables in Sparse Gaussian Processes","url":"https://www.researchgate.net/publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.\n</div> \n<p></p>"},{"id":627,"title":"MapReduce: Simplified Data Processing on Large Clusters","url":"https://www.researchgate.net/publication/220851866_MapReduce_Simplified_Data_Processing_on_Large_Clusters","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.\n</div> \n<p></p>"},{"id":628,"title":"Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models","url":"https://www.researchgate.net/publication/220320714_Probabilistic_Non-linear_Principal_Component_Analysis_with_Gaussian_Process_Latent_Variable_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Summarising a high dimensional data set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an over view of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be non- linearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GP-LVM). Through analysis of the GP-LVM objective function, we relate the model to popular spectral techniques such as kernel PCA and multidimensional scaling. We then review a practical algorithm for GP-LVMs in the context of large data sets and develop it to also handle discrete valued data and missing attributes. We demonstrate the model on a range of real-world and artificially generated data sets.\n</div> \n<p></p>"},{"id":629,"title":"Moller, M.F.: A Scaled Conjugate Gradient Algorithm For Fast Supervised Learning. Neural Networks 6, 525-533","url":"https://www.researchgate.net/publication/222482794_Moller_MF_A_Scaled_Conjugate_Gradient_Algorithm_For_Fast_Supervised_Learning_Neural_Networks_6_525-533","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A supervised learning algorithm (Scaled Conjugate Gradient, SCG) is introduced. The performance of SCG is benchmarked against that of the standard back propagation algorithm (BP) (Rumelhart, Hinton, &amp; Williams, 1986), the conjugate gradient algorithm with line search (CGL) (Johansson, Dowla, &amp; Goodman, 1990) and the one-step Broyden-Fletcher-Goldfarb-Shanno memoriless quasi-Newton algorithm (BFGS) (Battiti, 1990). SCG is fully-automated, includes no critical user-dependent parameters, and avoids a time consuming line search, which CGL and BFGS use in each iteration in order to determine an appropriate step size. Experiments show that SCG is considerably faster than BP, CGL, and BFGS.\n</div> \n<p></p>"},{"id":630,"title":"Parallel Markov chain Monte Carlo Simulation by Pre-Fetching","url":"https://www.researchgate.net/publication/261683938_Parallel_Markov_chain_Monte_Carlo_Simulation_by_Pre-Fetching","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In recent years, parallel processing has become widely available to researchers. It can be applied in an obvious way in the context of Monte Carlo simulation, but techniques for “parallelizing” Markov chain Monte Carlo (MCMC) algorithms are not so obvious, apart from the natural approach of generating multiple chains in parallel. Although generation of parallel chains is generally the easiest approach, in cases where burn-in is a serious problem, it is often desirable to use parallelization to speed up generation of a single chain. This article briefly discusses some existing methods for parallelization of MCMC algorithms, and proposes a new “pre-fetching” algorithm to parallelize generation of a single chain.\n</div> \n<p></p>"},{"id":631,"title":"Semi-described and semi-supervised learning with Gaussian processes","url":"https://www.researchgate.net/publication/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","abstraction":"Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as \"semi-described learning\". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks."},{"id":632,"title":"Semi-described and semi-supervised learning with Gaussian processes","url":"https://www.researchgate.net/publication/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as \" semi-described learning \". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regres-sion/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.\n</div> \n<p></p>"},{"id":633,"title":"Mondrian Forests for Large-Scale Regression when Uncertainty Matters","url":"https://www.researchgate.net/publication/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters","abstraction":"Decision forests and their variants deliver efficient state-of-the-art prediction performance, but many applications, such as probabilistic numerics, Bayesian optimization, uncertainty quantification, and planning, also demand a measure of the uncertainty associated with each prediction. Existing approaches to measuring the uncertainty of decision forest predictions are known to perform poorly, and so Gaussian processes and approximate variants are the standard tools in these application areas. With a goal of providing efficient state-of-the-art predictions together with estimates of uncertainty, we describe a regression framework using Mondrian forests, an approach to decision forests where the underlying random decision trees are modeled as i.i.d. Mondrian processes and efficient algorithms perform nonparametric Bayesian inference within each tree and ordinary model combination across trees. In our framework, the underlying nonparametric inference is a Gaussian diffusion over the tree, which results in a multivariate Gaussian calculation for inference in light of finite data that can be carried out efficiently using belief propagation. On a synthetic task designed to mimic a typical probabilistic numerical task, we demonstrate that Mondrian forest regression delivers far superior uncertainty quantification than existing decision forest methods with little-to-no cost in predictive performance. We then turn to a real-world probabilistic numerics benchmark modeling flight delay, where we compare Mondrian forests also to large-scale GP approximate methods. Our experiments demonstrate that Mondrian forests can deliver superior uncertainty assessments to GPs, as measured by negative predictive log density, with little-to-no loss in RMSE performance."},{"id":634,"title":"Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process Regression","url":"https://www.researchgate.net/publication/269417452_Hierarchical_Mixture-of-Experts_Model_for_Large-Scale_Gaussian_Process_Regression","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a practical and scalable Gaussian process model for large-scale\n <br> nonlinear probabilistic regression. Our mixture-of-experts model is\n <br> conceptually simple and hierarchically recombines computations for an overall\n <br> approximation of a full Gaussian process. Closed-form and distributed\n <br> computations allow for efficient and massive parallelisation while keeping the\n <br> memory consumption small. Given sufficient computing resources, our model can\n <br> handle arbitrarily large data sets, without explicit sparse approximations. We\n <br> provide strong experimental evidence that our model can be applied to large\n <br> data sets of sizes far beyond millions. Hence, our model has the potential to\n <br> lay the foundation for general large-scale Gaussian process research.\n</div> \n<p></p>"},{"id":635,"title":"Nested Variational Compression in Deep Gaussian Processes","url":"https://www.researchgate.net/publication/269116839_Nested_Variational_Compression_in_Deep_Gaussian_Processes","abstraction":"Deep Gaussian processes provide a flexible approach to probabilistic modelling of data using either supervised or unsupervised learning. For tractable inference approximations to the marginal likelihood of the model must be made. The original approach to approximate inference in these models used variational compression to allow for approximate variational marginalization of the hidden variables leading to a lower bound on the marginal likelihood of the model [Damianou and Lawrence, 2013]. In this paper we extend this idea with a nested variational compression. The resulting lower bound on the likelihood can be easily parallelized or adapted for stochastic variational inference."},{"id":636,"title":"Dimensionality reduction for survival data via the Gaussian process latent variable model","url":"https://www.researchgate.net/publication/262840808_Dimensionality_reduction_for_survival_data_via_the_Gaussian_process_latent_variable_model","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The analysis of high dimensional survival data is challenging, primarily due\n <br> to the problem of overfitting which occurs when spurious relationships are\n <br> inferred from data that subsequently fail to exist in test data. Here we\n <br> propose a novel method of extracting a low dimensional rep- resentation of\n <br> survival data by combining the popular Gaussian Process Latent Variable Model\n <br> (GPLVM) with a Weibull Proportional Hazards Model (WPHM). The model offers a\n <br> flexible non-linear probabilistic method of detecting and extracting any\n <br> intrinsic lower dimensional structure from high dimensional data. In addition\n <br> we can simultaneously combine information from multiple data sources. We\n <br> present results from several simulation studies that illustrate a reduction in\n <br> overfitting and an increase in predictive performance, as well as successful\n <br> detection of intrinsic dimensionality.\n</div> \n<p></p>"},{"id":637,"title":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data","url":"https://www.researchgate.net/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","abstraction":"Multivariate categorical data occur in many applications of machine learning. One of the main difficulties with these vectors of categorical variables is sparsity. The number of possible observations grows exponentially with vector length, but dataset diversity might be poor in comparison. Recent models have gained significant improvement in supervised tasks with this data. These models embed observations in a continuous space to capture similarities between them. Building on these ideas we propose a Bayesian model for the unsupervised task of distribution estimation of multivariate categorical data. We model vectors of categorical variables as generated from a non-linear transformation of a continuous latent space. Non-linearity captures multi-modality in the distribution. The continuous representation addresses sparsity. Our model ties together many existing models, linking the linear categorical latent Gaussian model, the Gaussian process latent variable model, and Gaussian process classification. We derive inference for our model based on recent developments in sampling based variational inference. We show empirically that the model outperforms its linear and discrete counterparts in imputation tasks of sparse data."},{"id":638,"title":"Dropout as a Bayesian Approximation: Appendix","url":"https://www.researchgate.net/publication/277959103_Dropout_as_a_Bayesian_Approximation_Appendix","abstraction":"We show that a multilayer perceptron (MLP) with arbitrary depth and nonlinearities, with dropout applied after every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation offers an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. This document is an appendix for the main paper \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" by Gal and Ghahramani, 2015."},{"id":639,"title":"Gaussian Processes for Ordinal Regression.","url":"https://www.researchgate.net/publication/220320873_Gaussian_Processes_for_Ordinal_Regression","abstraction":"We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach."},{"id":640,"title":"Choosing Multiple Parameters for Support Vector Machines","url":"https://www.researchgate.net/publication/2374270_Choosing_Multiple_Parameters_for_Support_Vector_Machines","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The problem of automatically tuning multiple parameters for pattern recognition Support Vector Machines (SVMs) is considered. This is done by minimizing some estimates of the generalization error of SVMs using a gradient descent algorithm over the set of parameters. Usual methods for choosing parameters, based on exhaustive search become intractable as soon as the number of parameters exceeds two. Some experimental results assess the feasibility of our approach for a large number of parameters (more than 100) and demonstrate an improvement of generalization performance.\n</div> \n<p></p>"},{"id":641,"title":"Probabilities for SV Machines","url":"https://www.researchgate.net/publication/233784966_Probabilities_for_SV_Machines","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.\n</div> \n<p></p>"},{"id":642,"title":"Comparison of Approximate Methods for Handling Hyperparameters","url":"https://www.researchgate.net/publication/2836338_Comparison_of_Approximate_Methods_for_Handling_Hyperparameters","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models which include unknown hyperparameters such as regularization constants and noise levels. In the 'evidence framework' the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters.\n</div> \n<p></p>"},{"id":643,"title":"Gaussian Processes for Classification: Mean-Field Algorithms","url":"https://www.researchgate.net/publication/40498233_Gaussian_Processes_for_Classification_Mean-Field_Algorithms","abstraction":"We derive a mean field algorithm for binary classification with Gaussian processes which is based on the TAP approach originally proposed in Statistical Physics of disordered systems. The theory also yields an approximate leave-one-out estimator for the generalization error which is computed with no extra computational cost. We show that from the TAP approach, it is possible to derive both a simpler `naive' mean field theory and support vector machines (SVM) as limiting cases. For both mean field algorithms and support vectors machines, simulation results for three small benchmark data sets are presented. They show 1. that one may get state of the art performance by using the leave-one-out estimator for model selection and 2. the built-in leave-one-out estimators are extremely precise when compared to the exact leave-one-out estimate. The latter result is a taken as a strong support for the internal consistency of the mean field approach."},{"id":644,"title":"Fast Sparse Gaussian Process Methods: The Informative Vector Machine","url":"https://www.researchgate.net/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Abstract We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d{sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory,requirements. The scaling of our method is at most O(n d,), and in large real-world classication experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signican tly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation.\n</div> \n<p></p>"},{"id":645,"title":"Variational Gaussian process classifiers. IEEE Trans Neural Netw","url":"https://www.researchgate.net/publication/3302847_Variational_Gaussian_process_classifiers_IEEE_Trans_Neural_Netw","abstraction":"Gaussian processes are a promising nonlinear regression tool, but it is not straightforward to solve classification problems with them. In the paper the variational methods of Jaakkola and Jordan (2000) are applied to Gaussian processes to produce an efficient Bayesian binary classifier."},{"id":646,"title":"Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and Sparse Approximations","url":"https://www.researchgate.net/publication/2893638_Bayesian_Gaussian_Process_Models_PAC-Bayesian_Generalisation_Error_Bounds_and_Sparse_Approximations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Non-parametric models and techniques enjoy a growing popularity in the field of machine learning, and among these Bayesian inference for Gaussian process (GP) models has recently received significant attention. We feel that GP priors should be part of the standard toolbox for constructing models relevant to machine learning in the same way as parametric linear models are, and the results in this thesis help to remove some obstacles on the way towards this goal.\n</div> \n<p></p>"},{"id":647,"title":"Probabilistic Inference Using Markov Chain Monte Carlo Methods","url":"https://www.researchgate.net/publication/2704064_Probabilistic_Inference_Using_Markov_Chain_Monte_Carlo_Methods","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difculties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The \"Metropolis algorithm\" has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of \"Gibbs sampling\" has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the \"hybrid Monte Carlo\" method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of \"simulated annealing\", and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, and present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilitic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.\n</div> \n<p></p>"},{"id":648,"title":"Probabilistic Programming with Gaussian Process Memoization","url":"https://www.researchgate.net/publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","abstraction":"Gaussian Processes (GPs) are widely used tools in statistics, machine learning, robotics, computer vision, and scientific computation. However, despite their popularity, they can be difficult to apply; all but the simplest classification or regression applications require specification and inference over complex covariance functions that do not admit simple analytical posteriors. This paper shows how to embed Gaussian processes in any higher-order probabilistic programming language, using an idiom based on memoization, and demonstrates its utility by implementing and extending classic and state-of-the-art GP applications. The interface to Gaussian processes, called gpmem, takes an arbitrary real-valued computational process as input and returns a statistical emulator that automatically improve as the original process is invoked and its input-output behavior is recorded. The flexibility of gpmem is illustrated via three applications: (i) robust GP regression with hierarchical hyper-parameter learning, (ii) discovering symbolic expressions from time-series data by fully Bayesian structure learning over kernels generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian optimization with automatic inference and action selection. All applications share a single 50-line Python library and require fewer than 20 lines of probabilistic code each."},{"id":649,"title":"Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo","url":"https://www.researchgate.net/publication/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo","abstraction":"We present a novel approach for fully non-stationary Gaussian process regression (GPR), where all three key parameters -- noise variance, signal variance and lengthscale -- can be simultaneously input-dependent. We develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. We propose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC), which conveniently extends the analytical gradient-based GPR learning by guiding the sampling with model gradients. We also learn the MAP solution from the posterior by gradient ascent. In experiments on several synthetic datasets and in modelling of temporal gene expression, the nonstationary GPR is shown to be necessary for modeling realistic input-dependent dynamics, while it performs comparably to conventional stationary or previous non-stationary GPR models otherwise."},{"id":650,"title":"Evaluation of different classification methods for the diagnosis of schizophrenia based on functional near-infrared spectroscopy","url":"https://www.researchgate.net/publication/270651847_Evaluation_of_different_classification_methods_for_the_diagnosis_of_schizophrenia_based_on_functional_near-infrared_spectroscopy","abstraction":"Based on near-infrared spectroscopy (NIRS), recent converging evidence has been observed that patients with schizophrenia exhibit abnormal functional activities in the prefrontal cortex during a verbal fluency task (VFT). Therefore, some studies have attempted to employ NIRS measurements to differentiate schizophrenia patients from healthy controls with different classification methods. However, no systematic evaluation was conducted to compare their respective classification performances on the same study population. In this study, we evaluated the classification performance of four classification methods (including linear discriminant analysis, k-nearest neighbors, Gaussian process classifier, and support vector machines) on an NIRS-aided schizophrenia diagnosis. We recruited a large sample of 120 schizophrenia patients and 120 healthy controls and measured the hemoglobin response in the prefrontal cortex during the VFT using a multichannel NIRS system. Features for classification were extracted from three types of NIRS data in each channel. We subsequently performed a principal component analysis (PCA) for feature selection prior to comparison of the different classification methods. We achieved a maximum accuracy of 85.83% and an overall mean accuracy of 83.37% using a PCA-based feature selection on oxygenated hemoglobin signals and support vector machine classifier. This is the first comprehensive evaluation of different classification methods for the diagnosis of schizophrenia based on different types of NIRS signals. Our results suggested that, using the appropriate classification method, NIRS has the potential capacity to be an effective objective biomarker for the diagnosis of schizophrenia. Copyright © 2014. Published by Elsevier B.V."},{"id":651,"title":"Mind the Nuisance: Gaussian Process Classification using Privileged Noise","url":"https://www.researchgate.net/publication/263582568_Mind_the_Nuisance_Gaussian_Process_Classification_using_Privileged_Noise","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The learning with privileged information setting has recently attracted a lot\n <br> of attention within the machine learning community, as it allows the\n <br> integration of additional knowledge into the training process of a classifier,\n <br> even when this comes in the form of a data modality that is not available at\n <br> test time. Here, we show that privileged information can naturally be treated\n <br> as noise in the latent function of a Gaussian Process classifier (GPC). That\n <br> is, in contrast to the standard GPC setting, the latent function is not just a\n <br> nuisance but a feature: it becomes a natural measure of confidence about the\n <br> training data by modulating the slope of the GPC sigmoid likelihood function.\n <br> Extensive experiments on public datasets show that the proposed GPC method\n <br> using privileged noise, called GPC+, improves over a standard GPC without\n <br> privileged knowledge, and also over the current state-of-the-art SVM-based\n <br> method, SVM+. Moreover, we show that advanced neural networks and deep learning\n <br> methods can be compressed as privileged information.\n</div> \n<p></p>"},{"id":652,"title":"Bayesian Inference for Gaussian Process Classifiers with Annealing and Exact-Approximate MCMC","url":"https://www.researchgate.net/publication/259010234_Bayesian_Inference_for_Gaussian_Process_Classifiers_with_Annealing_and_Exact-Approximate_MCMC","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Kernel methods have revolutionized the fields of pattern recognition and\n <br> machine learning. The importance of achieving a sound quantification of\n <br> uncertainty in predictions by characterizing the posterior distribution over\n <br> kernel parameters exactly has been demonstrated in several applications. This\n <br> paper focuses on Markov chain Monte Carlo (MCMC) based inference of covariance\n <br> (kernel) parameters for Gaussian process classifiers. Recently, the\n <br> exact-approximate MCMC approach has been proposed as a practical way to\n <br> efficiently infer covariance parameters in Gaussian process classifiers\n <br> exactly. In this approach, an unbiased estimate of the marginal likelihood\n <br> obtained by importance sampling replaces the actual marginal likelihood in the\n <br> Hastings ratio. This paper presents the application of annealed importance\n <br> sampling to obtain a low-variance unbiased estimate of the marginal likelihood.\n <br> This paper empirically demonstrates that annealed importance sampling reduces\n <br> the variance of the estimate of the marginal likelihood exponentially in the\n <br> number of data compared to importance sampling, while the computational cost\n <br> scales only polynomially. The results on real data demonstrate that employing\n <br> annealed importance sampling in the exact-approximate MCMC approach represents\n <br> a step forward in the development of fully automated exact inference engines\n <br> for Gaussian process classifiers.\n</div> \n<p></p>"},{"id":653,"title":"Data Integration for Classification Problems Employing Gaussian Process Priors (The full paper with accompanying material for the derivations for EP)","url":"https://www.researchgate.net/publication/257365689_Data_Integration_for_Classification_Problems_Employing_Gaussian_Process_Priors_The_full_paper_with_accompanying_material_for_the_derivations_for_EP","abstraction":"By adopting Gaussian process priors a fully Bayesian solution to the problem of integrating possibly heterogeneous data sets within a classification setting is presented. Approximate inference schemes employing Variational &amp; Expectation Propagation based methods are developed and rigorously assessed. We demonstrate our approach to integrating multiple data sets on a large scale protein fold prediction problem where we infer the optimal combinations of covariance functions and achieve state-of-the-art performance without resorting to any ad hoc parameter tuning and classifier combination."},{"id":654,"title":"Exact-Approximate Bayesian Inference for Gaussian Processes","url":"https://www.researchgate.net/publication/257299295_Exact-Approximate_Bayesian_Inference_for_Gaussian_Processes","abstraction":"Challenges arising when using Gaussian Process priors in probabilistic modeling are how to carry out exact Bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. Using probit regression as a working example, this paper presents a simple methodology based on Markov chain Monte Carlo and the pseudo-marginal approach that efficiently addresses both those questions. The results presented in this paper show improvements over previous approaches for sampling from the posterior distribution of the parameters of the covariance function of the Gaussian Process prior. This is particularly important as it offers a powerful tool to carry out the fully Bayesian treatment of Gaussian Process based statistical models. The results also demonstrate that Monte Carlo based integration of all model parameters is actually feasible providing a superior quantification of uncertainty in predictions. Comparison with respect to state-of-the-art probabilistic classifiers confirm this assertion. Finally, this paper demonstrates with an application on a financial time series that the proposed methodology can exploit sparsity in the inverse covariance of the Gaussian Process prior leading to a computationally efficient Markov chain Monte Carlo approach."},{"id":655,"title":"Population Monte Carlo","url":"https://www.researchgate.net/publication/2836794_Population_Monte_Carlo","abstraction":"This paper shows that importance sampling can be iterated to produce more accurate approximations to iid sampling from a target distribution, than sequential sampling from an MCMC algorithm. We first illustrate the adaptability of the joint scheme on a toy mixture example. As a more realistic example, we then reanalyse the ion channel model of Hodgson (1999), using an importance sampling scheme based on a hidden Markov representation. The degeneracy phenomenon that usually occurs in particle systems is studied on both examples"},{"id":656,"title":"Markov Chain Monte Carlo: Can We Trust the Third Significant Figure?","url":"https://www.researchgate.net/publication/2136938_Markov_Chain_Monte_Carlo_Can_We_Trust_the_Third_Significant_Figure","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Current reporting of results based on Markov chain Monte Carlo computations could be improved. In particular, a measure of the accuracy of the resulting estimates is rarely reported. Thus we have little ability to objectively assess the quality of the reported estimates. We address this issue in that we discuss why Monte Carlo standard errors are important, how they can be easily calculated in Markov chain Monte Carlo and how they can be used to decide when to stop the simulation. We compare their use to a popular alternative in the context of two examples. Comment: Published in at http://dx.doi.org/10.1214/08-STS257 the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org)\n</div> \n<p></p>"},{"id":657,"title":"Adaptive proposal distribution for random walk Metropolis algorithm. Comput Stat","url":"https://www.researchgate.net/publication/2762543_Adaptive_proposal_distribution_for_random_walk_Metropolis_algorithm_Comput_Stat","abstraction":"The choice of a suitable MCMC method and further the choice of a proposal distribution is known to be crucial for the convergence of the Markov chain. However, in many cases the choice of an effective proposal distribution is difficult. As a remedy we suggest a method called Adaptive Proposal (AP). Although the stationary distribution of the AP algorithm is slightly biased, it appears to provide an efficient tool for, e.g., reasonably low dimensional problems, as typically encountered in non-linear regression problems in natural sciences. As a realistic example we include a successful application of the AP algorithm in parameter estimation for the satellite instrument 'GOMOS'. In this paper we also present systematic performance criteria for comparing Adaptive Proposal algorithm with more traditional Metropolis algorithms."},{"id":658,"title":"An Adaptive Metropolis Algorithm","url":"https://www.researchgate.net/publication/38322292_An_Adaptive_Metropolis_Algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.\n</div> \n<p></p>"},{"id":659,"title":"Bayesian Calibration of Computer Models","url":"https://www.researchgate.net/publication/4772045_Bayesian_Calibration_of_Computer_Models","abstraction":"We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise."},{"id":660,"title":"Predicting Continuous Conflict Perception with Bayesian Gaussian Processes","url":"https://www.researchgate.net/publication/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Conflict is one of the most important phenomena of social life, but it is still largely neglected by the computing community. This work proposes an approach that detects common conversational social signals (loudness, overlapping speech, etc.) and predicts the conflict level perceived by human observers in continuous, non-categorical terms. The proposed regression approach is fully Bayesian and it adopts automatic relevance determination to identify the social signals that influence most the outcome of the prediction. The experiments are performed over the SSPNet Conflict Corpus, a publicly available collection of 1,430 clips extracted from televised political debates (roughly 12 hours of material for 138 subjects in total). The results show that it is possible to achieve a correlation close to 0.8 between actual and predicted conflict perception.\n</div> \n<p></p>"},{"id":661,"title":"On Block Updating in Markov Random Field Models for Disease Mapping","url":"https://www.researchgate.net/publication/4788953_On_Block_Updating_in_Markov_Random_Field_Models_for_Disease_Mapping","abstraction":"Gaussian Markov random field (GMRF) models are commonly used to model spatial correlation in disease mapping applications. For Bayesian inference by MCMC, so far mainly single-site updating algorithms have been considered. However, convergence and mixing properties of such algorithms can be extremely bad due to strong dependencies of parameters in the posterior distribution. In this paper, we propose various block sampling algorithms in order to improve the MCMC performance. The methodology is rather general, allows for non-standard full conditionals, and can be applied in a modular fashion in a large number of different scenarios. For illustration we consider three different models: two formulations for spatial modelling of a single disease (with and without additional unstructured parameters respectively), and one formulation for the joint analysis of two diseases. We apply the proposed algorithms to two datasets known from the literature. The results indicate that the largest benef..."},{"id":662,"title":"Beaumont MA. Estimation of population growth or decline in genetically monitored populations. Genetics 164: 1139-1160","url":"https://www.researchgate.net/publication/10652634_Beaumont_MA_Estimation_of_population_growth_or_decline_in_genetically_monitored_populations_Genetics_164_1139-1160","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article introduces a new general method for genealogical inference that samples independent genealogical histories using importance sampling (IS) and then samples other parameters with Markov chain Monte Carlo (MCMC). It is then possible to more easily utilize the advantages of importance sampling in a fully Bayesian framework. The method is applied to the problem of estimating recent changes in effective population size from temporally spaced gene frequency data. The method gives the posterior distribution of effective population size at the time of the oldest sample and at the time of the most recent sample, assuming a model of exponential growth or decline during the interval. The effect of changes in number of alleles, number of loci, and sample size on the accuracy of the method is described using test simulations, and it is concluded that these have an approximately equivalent effect. The method is used on three example data sets and problems in interpreting the posterior densities are highlighted and discussed.\n</div> \n<p></p>"},{"id":663,"title":"Markov chain Monte Carlo model determination for hierarchical and graphical log-linear model","url":"https://www.researchgate.net/publication/30967982_Markov_chain_Monte_Carlo_model_determination_for_hierarchical_and_graphical_log-linear_model","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We use reversible jump Markov chain Monte Carlo methods (Green, 1995) to develop strategies for calculating posterior probabilities of hierarchical, graphical or decomposable log-linear models for high-dimensional contingency tables. Even for tables of moderate size, these sets of models may be very large. The choice of suitable prior distributions for model parameters is also discussed in detail, and two examples are presented. For the first example, a three-way table, the model probabilities calculated using our reversible jump approach are compared with model probabilities calculated exactly or by using an alternative approximation. The second example is a six-way contingency table for which exact methods are infeasible, because of the large number of possible models. We identify the most probable hierarchical, graphical and decomposable models, and compare the results with alternatives approaches.\n</div> \n<p></p>"},{"id":664,"title":"Covariance Structure of the Gibbs Sampler with Applications to the Comparison of Estimators and Augmentation Schemes","url":"https://www.researchgate.net/publication/243765318_Covariance_Structure_of_the_Gibbs_Sampler_with_Applications_to_the_Comparison_of_Estimators_and_Augmentation_Schemes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We study the covariance structure of a Markov chain generated by the Gibbs sampler, with emphasis on data augmentation. When applied to a Bayesian missing data problem, the Gibbs sampler produces two natural approximations for the posterior distribution of the parameter vector: the empirical distribution based on the sampled values of the parameter vector, and a mixture of complete data posteriors. We prove that Rao-Blackwellization causes a one-lag delay for the autocovariances among dependent samples obtained from data augmentation, and consequently, the mixture approximation produces estimates with smaller variances than the empirical approximation. The covariance structure results are used to compare different augmentation schemes. It is shown that collapsing and grouping random components in a Gibbs sampler with two or three components usually result in more efficient sampling schemes.\n</div> \n<p></p>"},{"id":665,"title":"Bayesian Variable and Link Determination for Generalised Linear Models","url":"https://www.researchgate.net/publication/2585239_Bayesian_Variable_and_Link_Determination_for_Generalised_Linear_Models","abstraction":"this paper, we describe full Bayesian inference for generalised linear models where uncertainty"},{"id":666,"title":"Analyses of infectious disease data from household outbreaks by Markov Chain Monte Carlo methods","url":"https://www.researchgate.net/publication/4772531_Analyses_of_infectious_disease_data_from_household_outbreaks_by_Markov_Chain_Monte_Carlo_methods","abstraction":"The research agendas of psychologists and economists now have several overlaps, with behavioural economics providing theoretical and experimental study of the relationship between behaviour and choice, and hedonic psychology discussing appropriate measures of outcomes of choice in terms of overall utility or life satisfaction. Here we model the relationship between values (understood as principles guiding behaviour), choices and their final outcomes in terms of life satisfaction, and use data from the BHPS to assess whether our ideas on what is important in life (individual values) are broadly connected to what we experience as important in our lives (life satisfaction)."},{"id":667,"title":"Updating Schemes, Correlation Structure, Blocking, and Parametization for the Gibbs Sampler","url":"https://www.researchgate.net/publication/227611763_Updating_Schemes_Correlation_Structure_Blocking_and_Parametization_for_the_Gibbs_Sampler","abstraction":"In this paper many convergence issues concerning the implementation of the Gibbs sampler are investigated. Exact computable rates of convergence for Gaussian target distributions are obtained. Different random and non-random updating strategies and blocking combinations are compared using the rates. The effect of dimensionality and correlation structure on the convergence rates are studied. Some examples are considered to demonstrate the results. For a Gaussian image analysis problem several updating strategies are described and compared. For problems in Bayesian linear models several possible parameterizations are analysed in terms of their convergence rates characterizing the optimal choice."},{"id":668,"title":"A Note on Metropolis-Hastings Kernels for General State Spaces","url":"https://www.researchgate.net/publication/2269174_A_Note_on_Metropolis-Hastings_Kernels_for_General_State_Spaces","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Metropolis-Hastings algorithm is a method of constructing a reversible Markov transition kernel with a specified invariant distribution. This note describes necessary and sufficient conditions on the candidate generation kernel and the acceptance probability function for the resulting transition kernel and invariant distribution to satisfy the detailed balance conditions. A simple general formulation is used that covers a range of special cases treated separately in the literature. In addition, results on a useful partial ordering of finite state space reversible transition kernels are extended to general state spaces and used to compare the performance of two approaches to using mixtures in Metropolis-Hastings kernels. Short Title Metropolis-Hastings Kernels Keywords Markov Chain Monte Carlo, Peskun's Theorem, Mixture Kernels AMS Classifications 60J05,65C05,62-04 1 Introduction The Metropolis-Hastings algorithm (Metropolis et al., 1953; Hastings, 1970) is a method of construct...\n</div> \n<p></p>"},{"id":669,"title":"Bayesian non-parametric inference for $\\Lambda$-coalescents: consistency and a parametric method","url":"https://www.researchgate.net/publication/285648009_Bayesian_non-parametric_inference_for_Lambda-coalescents_consistency_and_a_parametric_method","abstraction":"We investigate Bayesian non-parametric inference for $\\Lambda$-coalescent processes parametrised by probability measures on the unit interval, and provide an implementable, provably consistent MCMC inference algorithm. We give verifiable criteria on the prior for posterior consistency when observations form a time series, and prove that any non-trivial prior is inconsistent when all observations are contemporaneous. We then show that the likelihood given a data set of size $n \\in \\mathbb{N}$ is constant across $\\Lambda$-measures whose leading $n - 2$ moments agree, and focus on inferring truncated sequences of moments. We provide a large class of functionals which can be extremised using finite computation given a credibility region of posterior truncated moment sequences, and a pseudo-marginal Metropolis-Hastings algorithm for sampling the posterior. Finally, we compare the efficiency of the exact and noisy pseudo-marginal algorithms with and without delayed acceptance acceleration using a simulation study."},{"id":670,"title":"The Correlated Pseudo-Marginal Method","url":"https://www.researchgate.net/publication/284096861_The_Correlated_Pseudo-Marginal_Method","abstraction":"The use of unbiased estimators within the Metropolis--Hastings has found numerous applications in Bayesian statistics. The resulting so-called pseudo-marginal algorithm allows us to substitute an unbiased Monte Carlo estimator of the likelihood for the true likelihood which might be intractable or too expensive to compute. Under regularity conditions, it has been established that an efficient implementation of this algorithm requires the variance of the estimator of the log-likelihood ratio appearing in the acceptance probability of the pseudo-marginal algorithm to be of order 1 for $T$ data points as $T\\rightarrow\\infty$, which in turn typically requires scaling the number $N$ of Monte Carlo samples linearly with $T$. We propose a simple and widely applicable modification of the pseudo-marginal algorithm, termed the correlated pseudo-marginal algorithm, which relies on an estimator of the log-likelihood ratio whose variance is shown to be of order $1$ as $N,T\\rightarrow\\infty$ whenever $N/T\\rightarrow0$. Experimentally, the efficiency of computations is increased relative to the standard pseudo-marginal algorithm by up to 180-fold in our simulations."},{"id":671,"title":"Exact ABC using Importance Sampling","url":"https://www.researchgate.net/publication/282268529_Exact_ABC_using_Importance_Sampling","abstraction":"Approximate Bayesian Computation (ABC) is a powerful method for carrying out Bayesian inference when the likelihood is computationally intractable. However, a drawback of ABC is that it is an approximate method that induces a systematic error because it is necessary to set a tolerance level to make the computation tractable. The issue of how to optimally set this tolerance level has been the subject of extensive research. This paper proposes an ABC algorithm based on importance sampling that estimates expectations with respect to the exact posterior distribution given the observed summary statistics. This overcomes the need to select the tolerance level. By exact we mean that there is no systematic error and the Monte Carlo error can be made arbitrarily small by increasing the number of importance samples. We provide a formal justification for the method and study its convergence properties. The method is illustrated in two applications and the empirical results suggest that the proposed ABC based estimators consistently converge to the true values as the number of importance samples increases. Our proposed approach can be applied more generally to any importance sampling problem where an unbiased estimate of the likelihood is required."},{"id":672,"title":"Unsupervised State-Space Modeling Using Reproducing Kernels","url":"https://www.researchgate.net/publication/281127386_Unsupervised_State-Space_Modeling_Using_Reproducing_Kernels","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A novel framework for the design of state-space models (SSMs) is proposed whereby the state-transition function of the model is parametrized using reproducing kernels. The nature of SSMs requires learning a latent function that resides in the state space and for which input-output sample pairs are not available, thus prohibiting the use of gradient-based supervised kernel learning. To this end, we then propose to learn the mixing weights of the kernel estimate by sampling from their posterior density using Monte Carlo methods. We first introduce an offline version of the proposed algorithm, followed by an online version which performs inference on both the parameters and the hidden state through particle filtering. The accuracy of the estimation of the state-transition function is first validated on synthetic data. Next, we show that the proposed algorithm outperforms kernel adaptive filters in the prediction of real-world time series, while also providing probabilistic estimates, a key advantage over standard methods.\n</div> \n<p></p>"},{"id":673,"title":"Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families","url":"https://www.researchgate.net/publication/277959058_Gradient-free_Hamiltonian_Monte_Carlo_with_Efficient_Kernel_Exponential_Families","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive\n <br> MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities\n <br> where classical HMC is not an option due to intractable gradients, KMC\n <br> adaptively learns the target's gradient structure by fitting an exponential\n <br> family model in a Reproducing Kernel Hilbert Space. Computational costs are\n <br> reduced by two novel efficient approximations to this gradient. While being\n <br> asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and\n <br> offers substantial mixing improvements over state-of-the-art gradient free\n <br> samplers. We support our claims with experimental studies on both toy and\n <br> real-world applications, including Approximate Bayesian Computation and\n <br> exact-approximate MCMC.\n</div> \n<p></p>"},{"id":674,"title":"Approximate Bayesian Computation by Modelling Summary Statistics in a Quasi-likelihood Framework","url":"https://www.researchgate.net/publication/276296427_Approximate_Bayesian_Computation_by_Modelling_Summary_Statistics_in_a_Quasi-likelihood_Framework","abstraction":"Approximate Bayesian Computation (ABC) is a useful class of methods for Bayesian inference when the likelihood function is computationally intractable. In practice, the basic ABC algorithm may be inefficient in the presence of discrepancy between prior and posterior. Therefore, more elaborate methods, such as ABC with the Markov chain Monte Carlo algorithm (ABC-MCMC), should be used. However, the elaboration of a proposal density for MCMC is a sensitive issue and very difficult in the ABC setting, where the likelihood is intractable. We discuss an automatic proposal distribution useful for ABC-MCMC algorithms. This proposal is inspired by the theory of quasi-likelihood (QL) functions and is obtained by modelling the distribution of the summary statistics as a function of the parameters. Essentially, given a real-valued vector of summary statistics, we reparametrize the model by means of a regression function of the statistics on parameters, obtained by sampling from the original model in a pilot-run simulation study. The QL theory is well established for a scalar parameter, and it is shown that when the conditional variance of the summary statistic is assumed constant, the QL has a closed-form normal density. This idea of constructing proposal distributions is extended to non constant variance and to real-valued parameter vectors. The method is illustrated by several examples and by an application to a real problem in population genetics."},{"id":675,"title":"The Metropolis—Hastings Algorithm","url":"https://www.researchgate.net/publication/274730206_The_Metropolis-Hastings_Algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This chapter is the first of a series on simulation methods based on Markov chains. However, it is a somewhat strange introduction because it contains a description of the most general algorithm of all. The next chapter (Chapter 8) concentrates on the more specific slice sampler, which then introduces the Gibbs sampler (Chapters 9 and 10), which, in turn, is a special case of the Metropolis–Hastings algorithm. (However, the Gibbs sampler is different in both fundamental methodology and historical motivation.)\n</div> \n<p></p>"},{"id":676,"title":"Variational Bayes with Intractable Likelihood","url":"https://www.researchgate.net/publication/274319771_Variational_Bayes_with_Intractable_Likelihood","abstraction":"Variational Bayes (VB) is rapidly becoming a popular tool for Bayesian inference in statistical modeling. However, the existing VB algorithms are restricted to cases where the likelihood is tractable, which precludes the use of VB in many interesting models such as in state space models and in approximate Bayesian computation (ABC), where application of VB methods was previously impossible. This paper extends the scope of application of VB to cases where the likelihood is intractable, but can be estimated unbiasedly. The proposed VB method therefore makes it possible to carry out Bayesian inference in many statistical models, including state space models and ABC. The method is generic in the sense that it can be applied to almost all statistical models without requiring a model-based derivation, which is a drawback of many existing VB algorithms. We also show how the proposed method can be used to obtain highly accurate VB approximations of marginal posterior distributions."},{"id":677,"title":"Implementation and performance issues in the Bayesian and likelihood fitting of multilevel models. Computational Stat","url":"https://www.researchgate.net/publication/246427191_Implementation_and_performance_issues_in_the_Bayesian_and_likelihood_fitting_of_multilevel_models_Computational_Stat","abstraction":"We use simulation studies (a) to compare Bayesian and likelihood fitting methods, in terms of validity of conclusions, in two-level random-slopes regression (RSR) models, and (b) to compare several Bayesian estimation methods based on Markov chain Monte Carlo, in terms of computational efficiency, in random-effects logistic regression (RELR) models. We find (a) that the Bayesian approach with a particular choice of diffuse inverse Wishart prior distribution for the (co)variance parameters performs at least as well-in terms of bias of estimates and actual coverage of nominal 95% intervals-as maximum likelihood methods in RSR models with medium sample sizes (expressed in terms of the number J of level-2 units), but neither approach performs as well as might be hoped wit small J; and (b) that an adaptive hybrid Metropolis-Gibbs sampling method we have developed for use in the multilevel modeling package MlwiN outperforms adaptive rejection Gibbs sampling in the RELR models we have considered, sometimes by a wide margin."},{"id":678,"title":"On an adaptive version of the Metropolis-Hastings algorithm with independent proposal distribution","url":"https://www.researchgate.net/publication/4788962_On_an_adaptive_version_of_the_Metropolis-Hastings_algorithm_with_independent_proposal_distribution","abstraction":"In this paper, we present a general formulation of an algorithm, the adaptive independent chain (AIC), that was introduced in a special context in Gåsemyr et al. [Methodol. Comput. Appl. Probab. 3 (2001)]. The algorithm aims at producing samples from a specific target distribution ?, and is an adaptive, non-Markovian version of the Metropolis–Hastings independent chain. A certain parametric class of possible proposal distributions is fixed, and the parameters of the proposal distribution are updated periodically on the basis of the recent history of the chain, thereby obtaining proposals that get ever closer to ?. We show that under certain conditions, the algorithm produces an exact sample from ? in a finite number of iterations, and hence that it converges to ?. We also present another adaptive algorithm, the componentwise adaptive independent chain (CAIC), which may be an alternative in particular in high dimensions. The CAIC may be regarded as an adaptive approximation to the Gibbs sampler updating parametric approximations to the conditionals of ?."},{"id":679,"title":"Adaptively Scaling the Metropolis Algorithm Using Expected Squared Jumped Distance","url":"https://www.researchgate.net/publication/228280215_Adaptively_Scaling_the_Metropolis_Algorithm_Using_Expected_Squared_Jumped_Distance","abstraction":"A good choice of the proposal distribution is crucial for the rapid convergence of the Metropolis algorithm. In this paper, given a family of parametric Markovian kernels, we develop an adaptive algorithm for finding the kernel that maximizes the expected squared jumped distance, an ob jective function that characterizes the Markov chain under its d-dimensional stationary distribution. We demonstrate the effectiveness of our method in several examples."},{"id":680,"title":"Coupling and Ergodicity of Adaptive MCMC","url":"https://www.researchgate.net/publication/245810545_Coupling_and_Ergodicity_of_Adaptive_MCMC","abstraction":"We consider basic ergodicity properties of adaptive MCMC algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counter-examples to demonstrate that the assumptions we make are not redundant."},{"id":681,"title":"Regeneration in Markov Chain Samplers","url":"https://www.researchgate.net/publication/2750746_Regeneration_in_Markov_Chain_Samplers","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Markov chain sampling has recently received considerable attention in particular in the context of Bayesian computation and maximum likelihood estimation. This paper discusses the use of Markov chain splitting, originally developed for the theoretical analysis of general state space Markov chains, to introduce regeneration into Markov chain samplers. This allows the use of regenerative methods for analyzing the output of these samplers, and can provide a useful diagnostic of sampler performance. The approach is applied to several samplers, including certain Metropolis samplers that can be used on their own or in hybrid samplers, and is illustrated in several examples. Key Words: Gibbs sampling, Markov chain Monte Carlo, hybrid sampler, Metropolis algorithm, simulation output analysis, split chain. 1 INTRODUCTION In Markov chain Monte Carlo, a distribution ß is examined by obtaining sample paths from a Markov chain constructed to have equilibrium distribution ß. This approach was in...\n</div> \n<p></p>"},{"id":682,"title":"Optimal scaling of the random walk Metropolis on elliptically symmetric unimodal targets","url":"https://www.researchgate.net/publication/45870805_Optimal_scaling_of_the_random_walk_Metropolis_on_elliptically_symmetric_unimodal_targets","abstraction":"Scaling of proposals for Metropolis algorithms is an important practical problem in MCMC implementation. Criteria for scaling based on empirical acceptance rates of algorithms have been found to work consistently well across a broad range of problems. Essentially, proposal jump sizes are increased when acceptance rates are high and decreased when rates are low. In recent years, considerable theoretical support has been given for rules of this type which work on the basis that acceptance rates around 0.234 should be preferred. This has been based on asymptotic results that approximate high dimensional algorithm trajectories by diffusions. In this paper, we develop a novel approach to understanding 0.234 which avoids the need for diffusion limits. We derive explicit formulae for algorithm efficiency and acceptance rates as functions of the scaling parameter. We apply these to the family of elliptically symmetric target densities, where further illuminating explicit results are possible. Under suitable conditions, we verify the 0.234 rule for a new class of target densities. Moreover, we can characterise cases where 0.234 fails to hold, either because the target density is too diffuse in a sense we make precise, or because the eccentricity of the target density is too severe, again in a sense we make precise. We provide numerical verifications of our results. Comment: Published in at http://dx.doi.org/10.3150/08-BEJ176 the Bernoulli (http://isi.cbs.nl/bernoulli/) by the International Statistical Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)"},{"id":683,"title":"Em algorithms for pca and spca","url":"https://www.researchgate.net/publication/2491114_Em_algorithms_for_pca_and_spca","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also naturally accommodates missing information. I also introduce a new variant of PCA called sensible principal component analysis (SPCA) which defines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions.\n</div> \n<p></p>"},{"id":684,"title":"Bédard, M.: Weak convergence of metropolis algorithms for non-i.i.d. target distributions. Ann. Appl. Probab. 17, 1222-1244","url":"https://www.researchgate.net/publication/1771219_Bedard_M_Weak_convergence_of_metropolis_algorithms_for_non-iid_target_distributions_Ann_Appl_Probab_17_1222-1244","abstraction":"In this paper, we shall optimize the efficiency of Metropolis algorithms for multidimensional target distributions with scaling terms possibly depending on the dimension. We propose a method for determining the appropriate form for the scaling of the proposal distribution as a function of the dimension, which leads to the proof of an asymptotic diffusion theorem. We show that when there does not exist any component with a scaling term significantly smaller than the others, the asymptotically optimal acceptance rate is the well-known 0.234."},{"id":685,"title":"On Markov Chain Monte Carlo Acceleration","url":"https://www.researchgate.net/publication/247011232_On_Markov_Chain_Monte_Carlo_Acceleration","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Markov chain Monte Carlo (MCMC) methods are currently enjoying a surge of interest within the statistical community. The goal of this work is to formalize and support two distinct adaptive strategies that typically accelerate the convergence of an MCMC algorithm. One approach is through resampling; the other incorporates adaptive switching of the transition kernel. Support is both by analytic arguments and simulation study. Application is envisioned in low-dimensional but nontrivial problems. Two pathological illustrations are presented. Connections with reparameterization are discussed as well as possible difficulties with infinitely often adaptation.\n</div> \n<p></p>"},{"id":686,"title":"An adaptive independence sampler MCMC algorithm for infinite-dimensional bayesian inference","url":"https://www.researchgate.net/publication/280946496_An_adaptive_independence_sampler_MCMC_algorithm_for_infinite-dimensional_bayesian_inference","abstraction":"<p itemprop=\"description\"> <strong>DESCRIPTION</strong> </p>\n<div>\n Many scientific and engineering problems require to perform Bayesian inferences in\n <br> function spaces, in which the unknowns are of infinite dimension. In such problems, many standard\n <br> Markov Chain Monte Carlo (MCMC) algorithms become arbitrary slow under the mesh refinement,\n <br> which is referred to as being dimension dependent. In this work we develop an independence sampler\n <br> based MCMC method for the infinite dimensional Bayesian inferences. We represent the proposal\n <br> distribution as a mixture of a finite number of specially parametrized Gaussian measures. We show\n <br> that under the chosen parametrization, the resulting MCMC algorithm is dimension independent.\n <br> We also design an efficient adaptive algorithm to adjust the parameter values of the mixtures from\n <br> the previous samples. Finally we provide numerical examples to demonstrate the efficiency and\n <br> robustness of the proposed method, even for problems with multimodal posterior distributions.\n</div> \n<p></p>"},{"id":687,"title":"Evaluation of a MISR-based high-resolution aerosol retrieval method using AERONET DRAGON campaign data","url":"https://www.researchgate.net/publication/273706611_Evaluation_of_a_MISR-based_high-resolution_aerosol_retrieval_method_using_AERONET_DRAGON_campaign_data","abstraction":"Satellite-retrieved aerosol optical depth (AOD) can potentially provide an effective way to complement the spatial coverage limitation of a ground particulate air-pollution monitoring network such as the U.S. Environment Protection Agency's regulatory monitoring network. One of the current state-of-the-art AOD retrieval methods is the National Aeronautics and Space Administration's Multiangle Imaging SpectroRadiometer (MISR) operational algorithm, which has a spatial resolution of 17.6 km $times$ 17.6 km. Although the MISR's aerosol products lead to exciting research opportunities to study particle composition at a regional scale, its spatial resolution is too coarse for analyzing urban areas, where the air pollution has stronger spatial variations and can severely impact public health and the environment. Accordingly, a novel AOD retrieval algorithm with a resolution of 4.4 km $times$ 4.4 km has been recently developed, which is based on hierarchical Bayesian modeling and the Monte Carlo Markov chain (MCMC) inference method. In this paper, we carry out detailed quantitative and qualitative evaluations of the new algorithm, which is called the HB-MCMC algorithm, using recent AErosol RObotic NETwork (AERONET) Distributed Regional Aerosol Gridded Observation Networks (DRAGON) campaign data obtained in the summer of 2011. These data, which were not available in a previous study, contain spatially dense ground measurements of the AOD and other aerosol particle characteristics from the Baltimore–Washington, DC region. Our results show that the HB-MCMC algorithm has 16.2% more AOD retrieval coverage and improves the root-mean-square error by 38.3% compared with the MISR operational algorithm. Our detailed analyses with various metrics show that the improvement of our scheme is coming from the novel mo- eling and inference method. Furthermore, the map overlay of the retrieval results qualitatively confirms the findings of the quantitative analyses."},{"id":688,"title":"Gradient-based MCMC samplers for Dynamic Causal Modelling","url":"https://www.researchgate.net/publication/280496847_Gradient-based_MCMC_samplers_for_Dynamic_Causal_Modelling","abstraction":"In this technical note we derive two MCMC (Markov Chain Monte Carlo) samplers for dynamic causal models (DCMs). Specifically, we use (a) Hamiltonian MCMC (HMC-E) where sampling is simulated using Hamilton's equation of motion and (b) Langevin Monte Carlo algorithm (LMC-R and LMC-E) that simulates the Langevin diffusion of samples using gradients either on a Euclidean (E) or on a Riemannian (R) manifold. Whilst LMC-R requires minimal tuning, the implementation of HMC-E is heavily dependent on its tuning parameters. These parameters are therefore optimised by learning a Gaussian Process model of the time normalised sample correlation matrix. This allows one to formulate an objective function that balances tuning parameter exploration and exploitation, furnishing an intervention-free inference scheme. Using, neural mass models (NMMs) - a class of biophysically motivated DCMs - we find that HMC-E is statistically more efficient than LMC-R (with a Riemannian metric); yet both gradient-based samplers are far superior to the random walk Metropolis algorithm, which proves inadequate to steer away from dynamical instability. Copyright © 2015. Published by Elsevier Inc."},{"id":689,"title":"Geometric ergodicity of the Random Walk Metropolis with position-dependent proposal covariance","url":"https://www.researchgate.net/publication/280330526_Geometric_ergodicity_of_the_Random_Walk_Metropolis_with_position-dependent_proposal_covariance","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider a Metropolis-Hastings method with proposal kernel\n <br> $\\mathcal{N}(x,hG^{-1}(x))$, where $x$ is the current state. After discussing\n <br> specific cases from the literature, we analyse the ergodicity properties of the\n <br> resulting Markov chains. In one dimension we find that suitable choice of\n <br> $G^{-1}(x)$ can change the ergodicity properties compared to the Random Walk\n <br> Metropolis case $\\mathcal{N}(x,h\\Sigma)$, either for the better or worse. In\n <br> higher dimensions we use a specific example to show that judicious choice of\n <br> $G^{-1}(x)$ can produce a chain which will converge at a geometric rate to its\n <br> limiting distribution when probability concentrates on an ever narrower ridge\n <br> as $|x|$ grows, something which is not true for the Random Walk Metropolis.\n</div> \n<p></p>"},{"id":690,"title":"Joint estimation of quantile planes over arbitrary predictor spaces","url":"https://www.researchgate.net/publication/280050382_Joint_estimation_of_quantile_planes_over_arbitrary_predictor_spaces","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In spite of the recent surge of interest in quantile regression, joint estimation of linear quantile planes remains a great challenge in statistics and econometrics. We propose a novel parametrization that characterizes any\n <br> collection of non-crossing quantile planes over arbitrarily shaped convex predictor domains in any dimension by means of unconstrained scalar, vector and function valued parameters. Statistical models based on this parametrization inherit a fast computation of the likelihood function, enabling penalized likelihood or Bayesian approaches to model fitting. We introduce a complete Bayesian methodology by using Gaussian process prior distributions on the function valued parameters and develop a robust and efficient Markov chain Monte Carlo parameter estimation. The resulting method is shown to offer posterior consistency under mild tail and regularity conditions. We present several illustrative examples where the new method is compared against existing\n <br> approaches and is found to offer better accuracy, coverage and model fit.\n</div> \n<p></p>"},{"id":691,"title":"Bayesian computation: a perspective on the current state, and sampling backwards and forwards","url":"https://www.researchgate.net/publication/271855369_Bayesian_computation_a_perspective_on_the_current_state_and_sampling_backwards_and_forwards","abstraction":"The past decades have seen enormous improvements in computational inference based on statistical models, with continual enhancement in a wide range of computational tools, in competition. In Bayesian inference, first and foremost, MCMC techniques continue to evolve, moving from random walk proposals to Langevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical and algorithmic inputs opening wider access to practitioners. However, this impressive evolution in capacity is confronted by an even steeper increase in the complexity of the models and datasets to be addressed. The difficulties of modelling and then handling ever more complex datasets most likely call for a new type of tool for computational inference that dramatically reduce the dimension and size of the raw data while capturing its essential aspects. Approximate models and algorithms may thus be at the core of the next computational revolution."},{"id":692,"title":"Adaptive Scheduling in MCMC and Probabilistic Programming","url":"https://www.researchgate.net/publication/271387816_Adaptive_Scheduling_in_MCMC_and_Probabilistic_Programming","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We introduce an adaptive output-sensitive inference algorithm for MCMC and\n <br> probabilistic programming, Adaptive Random Database. The algorithm is based on\n <br> a single-site updating Metropolis-Hasting sampler, the Random Database (RDB)\n <br> algorithm. Adaptive RDB (ARDB) differs from the original RDB in that the\n <br> schedule of selecting variables proposed for modification is adapted based on\n <br> the output of of the probabilistic program, rather than being fixed and\n <br> uniform. We show that ARDB still converges to the correct distribution. We\n <br> compare ARDB to RDB on several test problems highlighting different aspects of\n <br> the adaptation scheme.\n</div> \n<p></p>"},{"id":693,"title":"The Nearest Neighbor entropy estimate: an adequate tool for adaptive MCMC evaluation","url":"https://www.researchgate.net/publication/279260160_The_Nearest_Neighbor_entropy_estimate_an_adequate_tool_for_adaptive_MCMC_evaluation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Many recent and often adaptive Markov Chain Monte Carlo (MCMC) methods are associated in practice to unknown rates of convergence. We propose a simulation-based methodology to estimate MCMC efficiency, grounded on a Kullback divergence criterion requiring an estimate of the entropy of the algorithm successive densities, computed from iid simulated chains. We recently proved in Chauveau and Vandekerkhove (2013) some consistency results in MCMC setup for an entropy estimate based on Monte-Carlo integration of a kernel density estimate based on Györfi and Van Der Meulen (1989). Since this estimate requires some tuning parameters and deteriorates as dimension increases, we investigate here an alternative estimation technique based on Nearest Neighbor estimates. This approach has been initiated by Kozachenko and Leonenko (1987) but used mostly in univariate situations until recently when entropy estimation has been considered in other fields like neuroscience. Theoretically, we prove that, under certain uniform control conditions, the successive densities of a generic class of Adaptive Metropolis-Hastings algorithms to which most of the strategies proposed in the recent literature belong can be estimated consistently with our method. We then show that in MCMC setup where moderate to large dimensions are common, this estimate seems appealing for both computational and operational considerations, and that the problem inherent to a non neglictible bias arising in high dimension can be overcome. All our algorithms for MCMC simulation and entropy estimation are implemented in an R package taking advantage of recent advances in high performance (parallel) computing.\n</div> \n<p></p>"},{"id":694,"title":"Forgetting the starting distribution in finite interacting tempering","url":"https://www.researchgate.net/publication/262841099_Forgetting_the_starting_distribution_in_finite_interacting_tempering","abstraction":"Markov chain Monte Carlo (MCMC) methods are frequently used to approximately simulate high-dimensional, multimodal probability distributions. In adaptive MCMC methods, the transition kernel is changed \"on the fly\" in the hope to speed up convergence. We study interacting tempering, an adaptive MCMC algorithm based on interacting Markov chains, that can be seen as a simplified version of the equi-energy sampler. Using a coupling argument, we show that under easy to verify assumptions on the target distribution (on a finite space), the interacting tempering process rapidly forgets its starting distribution. The result applies, among others, to exponential random graph models, the Ising and Potts models (in mean field or on a bounded degree graph), as well as (Edwards-Anderson) Ising spin glasses. As a cautionary note, we also exhibit an example of a target distribution for which the interacting tempering process rapidly forgets its starting distribution, but takes an exponential number of steps (in the dimension of the state space) to converge to its limiting distribution. As a consequence, we argue that convergence diagnostics that are based on demonstrating that the process has forgotten its starting distribution might be of limited use for adaptive MCMC algorithms like interacting tempering."},{"id":695,"title":"Bucher, C.G.: Adaptive sampling—an iterative fast Monte Carlo procedure. Struct. Saf. 5(2), 119-126","url":"https://www.researchgate.net/publication/222510542_Bucher_CG_Adaptive_sampling-an_iterative_fast_Monte_Carlo_procedure_Struct_Saf_52_119-126","abstraction":"An iterative Monte-Carlo simulation procedure for structural analysis is suggested. This proposed new approach utilizes results from simulation to adapt the importance sampling density to the specific problem. Considerable reduction of the statistical error of the estimated failure probability is achieved. Most important, problems connected with optimization procedures commonly used in structural reliability are avoided. This makes the suggested procedure especially attractive for systems reliability analyses."},{"id":696,"title":"Importance sampling in structural system","url":"https://www.researchgate.net/publication/248560911_Importance_sampling_in_structural_system","abstraction":"Importance sampling as a technique to improve the Monte Carlo method for probability integration can be shown to be extremely efficient and versatile. This paper addresses the accuracy and efficiency of the method, and its application to series and parallel systems in structures."},{"id":697,"title":"Basic Analysis of Structural Safety","url":"https://www.researchgate.net/publication/245301372_Basic_Analysis_of_Structural_Safety","abstraction":"The present study reviews some of the more significant recent developments in the area of structural reliability analysis, proposes new interpretations for and emphasis in some of these crucial theoretical developments, and introduces additional useful quantities. To be specific, the first-order second-moment methods are reviewed. It is then shown that the Lagrange multiplier formulation (and thus any. algorithm associated with it) can be used to evaluate the safety index and the location of the design point. Monte Carlo techniques are recommended for use in estimating limit state probabilities as a practical alternative to other methods. The use of the Stokes and Gauss divergence theorem in two- and three-dimensional integral expressions of a limit state probability is suggested in order to reduce the dimensionality of the integrations by one. Finally, it is recommended that the point of maximum likelihood be used as an alternative to the design point based on the advanced first-order second-moment method."},{"id":698,"title":"Monte Carlo Methods","url":"https://www.researchgate.net/publication/225828439_Monte_Carlo_Methods","abstraction":"Monte Carlo methods comprise a large and still growing collection of methods of repetitive simulation designed to obtain approximate solutions of various problems by playing games of chance. Often these methods are motivated by randomness inherent in the problem being studied (as, e.g., when simulating the random walks of “particles” undergoing diffusive transport), but this is not an essential feature of Monte Carlo methods. As long ago as the eighteenth century, the distinguished French naturalist Compte de Buffon [1] described an experiment that is by now well known: a thin needle of length l is dropped repeatedly on a plane surface that has been ruled with parallel lines at a fixed distance d apart. Then, as Laplace suggested many years later [2], an empirical estimate of the probability P of an intersection obtained by dropping a needle at random a large number, N, of times and observing the number, n, of intersections provides a practical means for estimating ?"},{"id":699,"title":"Reliability of uncertain dynamical systems with multiple design points","url":"https://www.researchgate.net/publication/228589716_Reliability_of_uncertain_dynamical_systems_with_multiple_design_points","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Asymptotic approximations and importance sampling methods are presented for evaluating a class of probability integrals with multiple design points that may arise in the calculation of the reliability of uncertain dynamical systems. An approximation based on asymptotics is used as a ®rst step to provide a computationally e?cient estimate of the probability integral. The importance sampling method utilizes information of the integrand at the design points to substantially accelerate the convergence of available importance sampling methods that use information from one design point only. Implementation issues related to the choice of importance sampling density and sample generation for reducing the variance of the estimate are addressed. The computational e?ciency and improved accuracy of the proposed methods is demonstrated by investigating the reliability of structures equipped with a tuned mass damper for which multiple design points are shown to contribute signi®cantly to the value of the reliability integral.\n</div> \n<p></p>"},{"id":700,"title":"Asymptotic Expansion","url":"https://www.researchgate.net/publication/229811935_Asymptotic_Expansion","abstraction":"In this appendix  we describe asymptotic expansion methods which have been developed and applied in the study of electron, positron and multiphoton collision processes. We consider first asymptotic expansion solutions of the coupled second-order differential equations which arise in the study of electron and positron collisions with atoms, ions and molecules. We then consider asymptotic expansion  solutions of the coupled second-order differential equations which arise in R-matrix–Floquet theory of multiphoton ionization and laser-assisted electron–atom and electron–ion collisions when the velocity gauge  is adopted in the asymptotic region."},{"id":701,"title":"Multiple design points in first and second-order reliability","url":"https://www.researchgate.net/publication/223307920_Multiple_design_points_in_first_and_second-order_reliability","abstraction":"A method is developed to successively find the multiple design points of a component reliability problem, when they exist on the limit-state surface. FORM or SORM approximations at each design point followed by a series system reliability analysis is shown to lead to improved estimates of the failure probability. Three example applications show the generality and robustness of the method."},{"id":702,"title":"Reliability-Aware Optimization of a Wideband Antenna","url":"https://www.researchgate.net/publication/288834294_Reliability-Aware_Optimization_of_a_Wideband_Antenna","abstraction":"A framework for optimization of wideband antennas including statistical reliability considerations is proposed. Using predetermined tolerances of the manufacturing process, the reliability analysis provides a failure probability of the considered antenna according to imposed specifications. In addition, a sensitivity analysis is conducted in order to quantify the relative weight of the antenna’s parameters on the variability of |S11|. On this basis, a reliability-aware definition of optimization parameters can be derived to minimize failure probability during fabrication. For demonstration, a particular type of antenna, namely the tapered half-mode substrate-integrated waveguide (HMSIW) leaky-wave antenna, is chosen for the reliability analysis and optimization. This antenna is optimized for operation in the band from 7 GHz to 14 GHz (with specification |S11| &lt; ?10 dB) with a calculated failure probability of less than 1%. A fabrication of the designed prototype yields measurements showing a strict adherence to specifications, which exemplifies the relevance of the statistical analysis. The study can be generalized to any wideband antenna provided that a fast computation of cost function can be obtained."},{"id":703,"title":"The generalization of Latin hypercube sampling","url":"https://www.researchgate.net/publication/280498317_The_generalization_of_Latin_hypercube_sampling","abstraction":"Latin hypercube sampling (LHS) is generalized in terms of a spectrum of stratified sampling (SS) designs referred to as partially stratified sample (PSS) designs. True SS and LHS are shown to represent the extremes of the PSS spectrum. The variance of PSS estimates is derived along with some asymptotic properties. PSS designs are shown to reduce variance associated with variable interactions, whereas LHS reduces variance associated with main effects. Challenges associated with the use of PSS designs and their limitations are discussed. To overcome these challenges, the PSS method is coupled with a new method called Latinized stratified sampling (LSS) that produces sample sets that are simultaneously SS and LHS. The LSS method is equivalent to an Orthogonal Array based LHS under certain conditions but is easier to obtain. Utilizing an LSS on the subspaces of a PSS provides a sampling strategy that reduces variance associated with both main effects and variable interactions and can be designed specially to minimize variance for a given problem. Several high-dimensional numerical examples highlight the strengths and limitations of the method. The Latinized partially stratified sampling method is then applied to identify the best sample strategy for uncertainty quantification on a plate buckling problem."},{"id":704,"title":"Supplementary data of \" Impacts of mesic and xeric urban vegetation on outdoor thermal comfort and microclimate in Phoenix, AZ \"","url":"https://www.researchgate.net/publication/286450917_Supplementary_data_of_Impacts_of_mesic_and_xeric_urban_vegetation_on_outdoor_thermal_comfort_and_microclimate_in_Phoenix_AZ","abstraction":"An advanced Markov-Chain Monte Carlo approach called Subset Simulation is described in Au and Beck (2001) [1] was used to quantify parameter uncertainty and model sensitivity of the urban land-atmospheric framework, viz. the coupled urban canopy model-single column model (UCM-SCM). The results show that the atmospheric dynamics are sensitive to land surface conditions. The most sensitive parameters are dimensional parameters, i.e. roof width, aspect ratio, roughness length of heat and momentum, since these parameters control the magnitude of sensible heat flux. The relative insensitive parameters are hydrological parameters since the lawns or green roofs in urban areas are regularly irrigated so that the water availability for evaporation is never constrained."},{"id":705,"title":"Assessment of polynomial correlated function expansion for high-fidelity structural reliability analysis","url":"https://www.researchgate.net/publication/283542543_Assessment_of_polynomial_correlated_function_expansion_for_high-fidelity_structural_reliability_analysis","abstraction":"Accurate prediction of failure probability of a given structural system subjected to parametric uncertainty often leads to a computationally challenging process requiring considerable amount of time. To overcome this issue, it is advantageous to develop non-intrusive model, that approximates the system response and perform all subsequent operations on the developed model. This paper presents a novel non-intrusive algorithm, referred to as polynomial correlated function expansion (PCFE), for high-fidelity reliability analysis. The proposed method expresses the output in a hierarchical order of component functions which facilitate (i) expressing the component functions in term of extended bases, (ii) determination of actual responses at quasi-random sample points, (iii) determination of the unknown coefficients associated with the bases by employing homotopy algorithm and (iv) Monte Carlo simulation. PCFE decouples the stochastic computations and finite element (FE) computations, and consecutively the FE code can be treated as a black box, as in the case of a commercial software. Six numerical problems, involving explicit performance functions and real-life problems described by implicit limit-state functions, have been solved to illustrate the performance of the proposed approach. It is observed that PCFE outperforms the existing approaches."},{"id":706,"title":"Targeted Random Sampling for Time-invariant Reliability Analysis","url":"https://www.researchgate.net/publication/283235980_Targeted_Random_Sampling_for_Time-invariant_Reliability_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Targeted Random Sampling (TRS) method is extended to moderate-to-high dimensional time invariant reliability analysis. TRS is an adaptive sampling method rooted in stratified sampling variance reduction technique and represents a specific algorithm for Refined Stratified Sampling. In this work, a Markov Chain is used to sample the space densely in the vicinity of the limit stat and multi-dimensional stratum division is proposed along with a neural network based approximation of the performance function. The method is shown to perform efficiently and accurately for estimation of probability of failure for a 20 dimensional problem.\n</div> \n<p></p>"},{"id":707,"title":"Efficient Monte Carlo Algorithm For Rare Failure Event Simulation","url":"https://www.researchgate.net/publication/280224186_Efficient_Monte_Carlo_Algorithm_For_Rare_Failure_Event_Simulation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Studying failure scenarios allows one to gain insights into their cause and consequence, providing information for effective mitigation, contingency planning and improving system resilience. A new efficient algorithm is here proposed to solve applications where an expensive-to-evaluate computer model is involved. The algorithms allows to generate the conditional samples for the Subset simulation by representing each random variable by an arbitrary number of hidden variables. The resulting algorithm is very simple yet powerful and it does not required the use of the Markov Chain Monte Carlo method. The proposed algorithm has been implemented in a open source general purpose software, OpenCossan allowing the solution of large scale problems of industrial interest by taking advantages of High Performance Computing facilities. The applicability and flexibility of the proposed approach is shown by solving a number of different problems.\n</div> \n<p></p>"},{"id":708,"title":"A Sampling-based RBDO Algorithm with Local Refinement and Efficient Gradient Estimation","url":"https://www.researchgate.net/publication/280105735_A_Sampling-based_RBDO_Algorithm_with_Local_Refinement_and_Efficient_Gradient_Estimation","abstraction":"This article describes a two stage Reliability-Based Design Optimization (RBDO) algorithm. The first stage consists of solving an approximated RBDO problem using meta-models. In order to use gradient-based techniques, the sensitivity of failure probabilities are derived with respect to hyper-parameters of random variables as well as, and this is a novelty, deterministic variables. The second stage focuses on the local refinement of the meta-models around the first stage solution using generalized \" max-min \" samples. The approach is demonstrated on three examples including a crashworthiness problem with 11 random variables and 10 probabilistic constraints."},{"id":709,"title":"Rare Event Simulation: A Point Process Interpretation With Application In Probability And Quantile Estimation","url":"https://www.researchgate.net/publication/285310904_Rare_Event_Simulation_A_Point_Process_Interpretation_With_Application_In_Probability_And_Quantile_Estimation","abstraction":"This paper addresses the issue of estimating extreme probability and quantile on the output of complex computer codes. We introduce a new approach to this problem in term of a random walk in the output space. This allows us to derive two main results: (1) the number of samples required to get a realisation of a random variable in a given domain of probability measure p is drastically reduced, following a Poisson law with parameter log 1/p; and (2) we get parallel algorithms for estimating probabilities and quantiles and especially the optimal parallel Multilevel Splitting algorithm where there is indeed no subset to define anymore."},{"id":710,"title":"Lecture Notes in Computer Science","url":"https://www.researchgate.net/publication/222540032_Lecture_Notes_in_Computer_Science","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the computation of the volume of the union of high-dimensional geometric objects. While showing that this problem is #P-hard already for very simple bodies (i.e., axis-parallel boxes), we give a fast FPRAS for all objects where one can: (1) test whether a given point lies inside the object, (2) sample a point uniformly, (3) calculate the volume of the object in polynomial time. All three oracles can be weak, that is, just approximate. This implies that Klee’s measure problem and the hypervolume indicator can be approximated efficiently even though they are #P-hard and hence cannot be solved exactly in time polynomial in the number of dimensions unless P = NP. Our algorithm also allows to approximate efficiently the volume of the union of convex bodies given by weak membership oracles.\n <br> \n <br> For the analogous problem of the intersection of high-dimensional geometric objects we prove #P-hardness for boxes and show that there is no multiplicative polynomial-time \n <br> \n <br> -approximation for certain boxes unless NP=BPP, but give a simple additive polynomial-time ?-approximation.\n</div> \n<p></p>"},{"id":711,"title":"Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesley, Boston, MA","url":"https://www.researchgate.net/publication/30870312_Genetic_Algorithms_in_Search_Optimization_and_Machine_Learning_Addison-Wesley_Boston_MA","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n David Goldberg's Genetic Algorithms in Search, Optimization and Machine Learning is by far the bestselling introduction to genetic algorithms. Goldberg is one of the preeminent researchers in the field--he has published over 100 research articles on genetic algorithms and is a student of John Holland, the father of genetic algorithms--and his deep understanding of the material shines through. The book contains a complete listing of a simple genetic algorithm in Pascal, which C programmers can easily understand. The book covers all of the important topics in the field, including crossover, mutation, classifier systems, and fitness scaling, giving a novice with a computer science background enough information to implement a genetic algorithm and describe genetic algorithms to a friend.\n</div> \n<p></p>"},{"id":712,"title":"Thiele, L.: Multiobjective Evolutionary Algorithms: A Comparative Case Study and the Strength Pareto Approach. IEEE Trans. on Evolutionary Computation 3, 257-271","url":"https://www.researchgate.net/publication/2240388_Thiele_L_Multiobjective_Evolutionary_Algorithms_A_Comparative_Case_Study_and_the_Strength_Pareto_Approach_IEEE_Trans_on_Evolutionary_Computation_3_257-271","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Evolutionary algorithms (EAs) are often wellsuited for optimization problems involving several, often conflicting objectives. Since 1985, various evolutionary approaches to multiobjective optimization have been developed that are capable of searching for multiple solutions concurrently in a single run. However, the few comparative studies of different methods presented up to now remain mostly qualitative and are often restricted to a few approaches. In this paper, four multiobjective EAs are compared quantitatively where an extended 0/1 knapsack problem is taken as a basis. Furthermore, we introduce a new evolutionary approach to multicriteria optimization, the Strength Pareto EA (SPEA), that combines several features of previous multiobjective EAs in a unique manner. It is characterized by (a) storing nondominated solutions externally in a second, continuously updated population, (b) evaluating an individual's fitness dependent on the number of external nondominated points that domina...\n</div> \n<p></p>"},{"id":713,"title":"On Set-Based Multiobjective Optimization","url":"https://www.researchgate.net/publication/262405508_On_Set-Based_Multiobjective_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Assuming that evolutionary multiobjective optimization (EMO) mainly deals with set problems, one can identify three core questions in this area of research: 1) how to formalize what type of Pareto set approximation is sought; 2) how to use this information within an algorithm to efficiently search for a good Pareto set approximation; and 3) how to compare the Pareto set approximations generated by different optimizers with respect to the formalized optimization goal. There is a vast amount of studies addressing these issues from different angles, but so far only a few studies can be found that consider all questions under one roof. This paper is an attempt to summarize recent developments in the EMO field within a unifying theory of set-based multiobjective search. It discusses how preference relations on sets can be formally defined, gives examples for selected user preferences, and proposes a general preferenceindependent hill climber for multiobjective optimization with theoretical convergence properties. Furthermore, it shows how to use set preference relations for statistical performance assessment and provides corresponding experimental results. The proposed methodology brings together preference articulation, algorithm design, and performance assessment under one framework and thereby opens up a new perspective on EMO.\n</div> \n<p></p>"},{"id":714,"title":"On metrics for comparing non-dominated sets","url":"https://www.researchgate.net/publication/216300934_On_metrics_for_comparing_non-dominated_sets","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Evolutionary multi-objective optimisation (EMO) boasts a proliferation algorithms and benchmark problems. principled compare performance of different EMO algorithms, is plicated the result EMO a single scalar value, a collection of vectors forming a non-dominated Various metrics non-dominated suggested. compare several, using framework of `outperformance relations\" (Hansen Jaszkiewicz [4]). This enables criticise contrast a variety of published metrics, leading some recommendations which useful ...\n</div> \n<p></p>"},{"id":715,"title":"Properties of an adaptive archiving algorithm for storing nondominated vectors. IEEE Trans Evol Comput","url":"https://www.researchgate.net/publication/3418743_Properties_of_an_adaptive_archiving_algorithm_for_storing_nondominated_vectors_IEEE_Trans_Evol_Comput","abstraction":"Search algorithms for Pareto optimization are designed to obtain multiple solutions, each offering a different trade-off of the problem objectives. To make the different solutions available at the end of an algorithm run, procedures are needed for storing them, one by one, as they are found. In a simple case, this may be achieved by placing each point that is found into an \"archive\" which maintains only nondominated points and discards all others. However, even a set of mutually nondominated points is potentially very large, necessitating a bound on the archive's capacity. But with such a bound in place, it is no longer obvious which points should be maintained and which discarded; we would like the archive to maintain a representative and well-distributed subset of the points generated by the search algorithm, and also that this set converges. To achieve these objectives, we propose an adaptive archiving algorithm, suitable for use with any Pareto optimization algorithm, which has various useful properties as follows. It maintains an archive of bounded size, encourages an even distribution of points across the Pareto front, is computationally efficient, and we are able to prove a form of convergence. The method proposed here maintains evenness, efficiency, and cardinality, and provably converges under certain conditions but not all. Finally, the notions underlying our convergence proofs support a new way to rigorously define what is meant by \"good spread of points\" across a Pareto front, in the context of grid-based archiving schemes. This leads to proofs and conjectures applicable to archive sizing and grid sizing in any Pareto optimization algorithm maintaining a grid-based archive."},{"id":716,"title":"Local-Search and Hybrid Evolutionary Algorithms for Pareto Optimization","url":"https://www.researchgate.net/publication/2570964_Local-Search_and_Hybrid_Evolutionary_Algorithms_for_Pareto_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In recent years, a gradual increase in the sophistication of multiobjective evolutionary algo- rithms (MOEAs) for Pareto optimization has been seen, accompanied by an ever-growing list of applications. Despite this trend, however, the proposition that methods based on local search may be a good alternative approach -- with the advantages of ease-of-use and lower computational overhead -- has not been thoroughly tested. In this thesis we develop a novel, local-sear&amp; algorithm for Pareto optimization, called PAES, enabling us to test and compare MOEAs against this philosophically different search method.\n</div> \n<p></p>"},{"id":717,"title":"SMS-EMOA: multiobjective selection based on dominated hypervolume. Eur J Oper Res","url":"https://www.researchgate.net/publication/222030993_SMS-EMOA_multiobjective_selection_based_on_dominated_hypervolume_Eur_J_Oper_Res","abstraction":"The hypervolume measure (or S metric) is a frequently applied quality measure for comparing the results of evolutionary multiobjective optimisation algorithms (EMOA). The new idea is to aim explicitly for the maximisation of the dominated hypervolume within the optimisation process. A steady-state EMOA is proposed that features a selection operator based on the hypervolume measure combined with the concept of non-dominated sorting. The algorithm’s population evolves to a well-distributed set of solutions, thereby focussing on interesting regions of the Pareto front. The performance of the devised Smetric selection EMOA (SMS-EMOA) is compared to state-of-the-art methods on two- and three-objective benchmark suites as well as on aeronautical real-world applications."},{"id":718,"title":"IMPACT ANALYSIS OF CROSSOVERS IN MULTIOBJECTIVE EVOLUTIONARY ALGORITHM","url":"https://www.researchgate.net/publication/287330461_IMPACT_ANALYSIS_OF_CROSSOVERS_IN_MULTIOBJECTIVE_EVOLUTIONARY_ALGORITHM","abstraction":"Multi-objective optimization has become mainstream for solving several real-world problems which are naturally posed as Multi-objective optimization problems (MOPs) in all fields of engineering and science. Usually, MOPs consist of more than two conflicting objective functions which demand trade-off solutions. Multi-objective evolutionary algorithms (MOEAs) are extremely useful and well-suited for solving MOPs due to their population based nature. MOEAs evolve its population of solutions in a natural way and search for approximate solutions in single simulation run unlike traditional methods. These algorithms make use of various intrinsic search operators in an efficient manner. In this paper, we experimentally study the impact of multiple crossovers in multi-objective evolutionary algorithm based on decomposition (MOEA/D) framework and evaluate its performance over test instances of 2009 IEEE congress on evolutionary computation (CEC’09) developed for MOEAs competition. Based on our experiments, we observe that variation operators are the main source to improve the algorithmic performance of MOEA/D for dealing with CEC’09 complicated test problems."},{"id":719,"title":"Manage the Tradeoff in Data Sanitization","url":"https://www.researchgate.net/publication/282833842_Manage_the_Tradeoff_in_Data_Sanitization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Sharing data might bring the risk of disclosing the sensitive knowledge in it. Usually, the data owner may choose to sanitize data by modifying some items in it to hide sensitive knowledge prior to sharing. This paper focuses on protecting sensitive knowledge in the form of frequent itemsets by data sanitization. The sanitization process may result in side effects, i.e., the data distortion and the damage to the non-sensitive frequent itemsets. How to minimize these side effects is a challenging problem faced by the research community. Actually, there is a trade-off when trying to minimize both side effects simultaneously. In view of this, we propose a data sanitization method based on evolutionary multi-objective optimization (EMO). This method can hide specified sensitive itemsets completely while minimizing the accompanying side effects. Experiments on real datasets show that the proposed approach is very effective in performing the hiding task with fewer damage to the original data and non-sensitive knowledge. Copyright © 2015 The Institute of Electronics, Information and Communication Engineers.\n</div> \n<p></p>"},{"id":720,"title":"Solving a multi-objective dynamic stochastic districting and routing problem with a co-evolutionary algorithm","url":"https://www.researchgate.net/publication/283193499_Solving_a_multi-objective_dynamic_stochastic_districting_and_routing_problem_with_a_co-evolutionary_algorithm","abstraction":"This study considers a multi-objective dynamic stochastic districting and routing problem in which the customers of a territory stochastically evolve over several periods of a planning horizon, and where the number of service vehicles, the compactness of the districts, the dissimilarity measure of the districts and an equity measure of vehicles profit are considered as objectives. The problem is modeled and solved as a two-stage stochastic program, where in each period, districting decisions are made in the first stage, and the Beardwood-Halton-Hammersley formula is used to approximate the expected routing cost of each district in the second stage. An enhanced multi-objective evolutionary algorithm (MOEA), i.e., the preference-inspired co-evolutionary algorithm using mating restriction, is developed for the problem. The algorithm is tested on randomly generated instances and is compared with two state-of-the-art MOEAs. Computational results confirm the superiority and effectiveness of the proposed algorithm. Moreover, a procedure for selecting a preferred design for the proposed problem is described."},{"id":721,"title":"A fuzzy based approach for fitness approximation in multi-objective evolutionary algorithms","url":"https://www.researchgate.net/publication/283685586_A_fuzzy_based_approach_for_fitness_approximation_in_multi-objective_evolutionary_algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Evolutionary Algorithm provides a framework that is largely applicable to particular problems including multiobjective optimization problems, basically for the ease of their implementation and their capability to perform efficient parallel search. Indeed, in some cases, expensive multiobjective optimization evaluations might be a challenge to restrict the number of explicit fitness evaluations in multiobjective evolutionary algorithms. Accordingly, this article presents a novel approach that tackles this problem so as to not only decrease the number of fitness evaluations but also to improve the performance. During evolution, our proposed approach selects fit individuals based on the knowledge acquired throughout the search, and performs explicit fitness evaluations on these individuals. A comprehensive comparative analysis of a wide range of well-established test problems, selected from both traditional and state-of-the-art benchmarks, has been presented. Afterward, the effectiveness of the obtained results is compared with some of the state-of-the-art methods using two well-known metrics- i.e. Hypervolume and Inverted Generational Distance (IGD). The experiments of our implemented approach is performed to illustrate that our proposal seems to be promising and would prove more efficient than other approaches in terms of both the performance and the computational cost.\n</div> \n<p></p>"},{"id":722,"title":"Self-organizing Multiobjective Optimization Based on Decomposition with Neighborhood Ensemble","url":"https://www.researchgate.net/publication/280727316_Self-organizing_Multiobjective_Optimization_Based_on_Decomposition_with_Neighborhood_Ensemble","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Currently, most of the multiobjective evolutionary algorithms (MOEAs) directly adopt the reproduction operators designed for the single-objective optimization. Since these operators do not consider the characteristics of multiobjective optimization problems (MOPs), they can not always perform well in the MOEAs. Inspired by this case, this paper presents a self-organizing reproduction mechanism based on the regularity property of MOPs, and proposes a self-organizing multiobjective evolutionary algorithm based on decomposition with neighborhood ensemble. In the new reproduction, a self-organizing map approach is firstly employed to discover the population structure, and to build a mating pool for each solution. Thereafter, reproductions are only allowed among the solutions within the same mating pools. In order to establish the mating pools, an ensemble of multiple neuron neighborhood sizes is also introduced. The probability of choosing different neighborhood sizes is updated based on their performance on producing new solutions over the last certain generations. Comprehensive experiments denote that the proposed algorithm is efficient and competitive. The contributions of the new reproduction mechanism and neighborhood ensemble are also experimentally validated.\n</div> \n<p></p>"},{"id":723,"title":"A Many-Objective Evolutionary Algorithm With Enhanced Mating and Environmental Selections","url":"https://www.researchgate.net/publication/276175392_A_Many-Objective_Evolutionary_Algorithm_With_Enhanced_Mating_and_Environmental_Selections","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Multiobjective evolutionary algorithms have become prevalent and efficient approaches for solving multiobjective optimization problems. However, their performances deteriorate severely when handling many-objective optimization problems (MaOPs) due to the loss of selection pressure to drive the search toward the Pareto front and the ineffective design in diversity maintenance mechanism. This paper proposes a many-objective evolutionary algorithm (MaOEA) based on directional diversity (DD) and favorable convergence (FC). The main features are the enhancement of two selection schemes to facilitate both convergence and diversity. In the algorithm, a mating selection based on FC is applied to strengthen selection pressure while an environmental selection based on DD and FC is designed to balance diversity and convergence. The proposed algorithm is tested on 64 instances of 16 MaOPs with diverse characteristics and compared with seven state-of-the-art algorithms. Experimental results show that the proposed MaOEA performs competitively with respect to chosen state-of-the-art designs.\n</div> \n<p></p>"},{"id":724,"title":"A Performance Comparison Indicator for Pareto Front Approximations in Many-Objective Optimization","url":"https://www.researchgate.net/publication/280534179_A_Performance_Comparison_Indicator_for_Pareto_Front_Approximations_in_Many-Objective_Optimization","abstraction":"Increasing interest in simultaneously optimizing many objectives (typically more than three objectives) of problems leads to the emergence of various many-objective algorithms in the evolutionary multi-objective optimization field. However , in contrast to the development of algorithm design, how to assess many-objective algorithms has received scant concern. Many performance indicators are designed in principle for any number of objectives, but in practice are invalid or infeasible to be used in many-objective optimization. In this paper, we explain the difficulties that popular performance indicators face and propose a performance comparison indicator (PCI) to assess Pareto front approximations obtained by many-objective algorithms. PCI evaluates the quality of approximation sets with the aid of a reference set constructed by themselves. The points in the reference set are divided into many clusters, and the proposed indicator estimates the minimum moves of solutions in the approximation sets to weakly dominate these clusters. PCI has been verified both by an analytic comparison with several well-known indicators and by an empirical test on four groups of Pareto front approximations with different numbers of objectives and problem characteristics."},{"id":725,"title":"Expectations of linear functions with respect to truncated multinormal distributions – With applications for uncertainty analysis in environmental modelling","url":"https://www.researchgate.net/publication/222814751_Expectations_of_linear_functions_with_respect_to_truncated_multinormal_distributions_-_With_applications_for_uncertainty_analysis_in_environmental_modelling","abstraction":"This paper discusses results concerning multivariate normal distributions that are subject to truncation by a hyperplane and how such results can be applied to uncertainty analysis in the environmental sciences. We present a suite of results concerning truncated multivariate normal distributions, some of which already appear in the mathematical literature. The focus here is to make these types of results more accessible to the environmental science community and to this end we include a conceptually simple alternative derivation of an important result. We illustrate how the theory of truncated multivariate normal distributions can be employed in the environmental sciences by means of an example from the economics of climate change control."},{"id":726,"title":"Bayesian Multivariate Process Modeling for Prediction of Forest Attributes.","url":"https://www.researchgate.net/publication/43279849_Bayesian_Multivariate_Process_Modeling_for_Prediction_of_Forest_Attributes","abstraction":"This article investigates multivariate spatial process models suitable for predicting multiple forest attributes using a multisource forest inventory approach. Such data settings involve several spatially dependent response variables arising in each location. Not only does each variable vary across space, they are likely to be correlated among themselves. Traditional approaches have attempted to model such data using simplifying assumptions, such as a common rate of decay in the spatial correlation or simplified cross-covariance structures among the response variables. Our current focus is to produce spatially explicit, tree species  specific, prediction of forest biomass per hectare over a region of interest. Modeling such associations presents challenges in terms of validity of probability distributions as well as issues concerning identifiability and estimability of parameters. Our template encompasses several models with different correlation structures. These models represent different hypotheses whose tenability are assessed using formal model comparisons. We adopt a Bayesian hierarchical approach offering a sampling-based inferential framework using efficient Markov chain Monte Carlo methods for estimating model parameters."},{"id":727,"title":"Update Strategies for Kriging Models for Use in Variable Fidelity Optimization","url":"https://www.researchgate.net/publication/225363592_Update_Strategies_for_Kriging_Models_for_Use_in_Variable_Fidelity_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Many optimization methods for simulation-based design rely on the sequential use of metamodels to reduce the associated computational burden. In particular, kriging models are frequently used in variable fidelity optimization. Nevertheless, such methods may become computationally inefficient when solving problems with large numbers of design variables and/or sampled data points due to the expensive process of optimizing the kriging model parameters in each iteration. One solution to this problem would be to replace the kriging models with traditional Taylor series response surface models. Kriging models, however, were shown to provide good approximations of computer simulations that incorporate larger amounts of data, resulting in better global accuracy. In this paper, a metamodel update management scheme (MUMS) is proposed to reduce the cost of using kriging models sequentially by updating the kriging model parameters only when they produce a poor approximation. The scheme uses the trust region ratio (TR-MUMS), which is a ratio that compares the approximation to the true model. Two demonstration problems are used to evaluate the proposed method: an internal combustion engine sizing problem and a control-augmented structural design problem. The results indicate that the TR-MUMS approach is very effective; on the demonstration problems, it reduced the number of likelihood evaluations by three orders of magnitude compared to using a global optimizer to find the kriging parameters in every iteration. It was also found that in trust region-based method, the kriging model parameters need not be updated using a global optimizer—local methods perform just as well in terms of providing a good approximation without affecting the overall convergence rate, which, in turn, results in a faster execution time.\n</div> \n<p></p>"},{"id":728,"title":"An algebra of Pareto points","url":"https://www.researchgate.net/publication/4174682_An_algebra_of_Pareto_points","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Multicriteria optimisation problems occur naturally in engineering practices. Pareto analysis has proven to be a powerful tool to characterise potentially interesting realisations of a particular engineering problem for design-space exploration. Depending on the optimisation goals, one of the Pareto-optimal alternatives is the optimal realisation. It occurs however, that partial design decisions have to be taken, leaving other aspects of the optimisation problem to be decided at a later stage, and that Pareto-optimal configurations have to be composed (dynamically) from Pareto-optimal configurations of components. Both aspects are not supported by current analysis methods. This paper introduces a novel, algebraic approach to Pareto analysis. It allows for describing incremental design decisions and composing sets of Pareto-optimal configurations. The algebra can be used to study the operations on Pareto sets and the efficient computation of Pareto sets and their compositions.\n</div> \n<p></p>"},{"id":729,"title":"Single-and multiobjective evolutionary optimization assisted by gaussian random field metamodels. IEEE Trans Evol Comput","url":"https://www.researchgate.net/publication/3418892_Single-and_multiobjective_evolutionary_optimization_assisted_by_gaussian_random_field_metamodels_IEEE_Trans_Evol_Comput","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents and analyzes in detail an efficient search method based on evolutionary algorithms (EA) assisted by local Gaussian random field metamodels (GRFM). It is created for the use in optimization problems with one (or many) computationally expensive evaluation function(s). The role of GRFM is to predict objective function values for new candidate solutions by exploiting information recorded during previous evaluations. Moreover, GRFM are able to provide estimates of the confidence of their predictions. Predictions and their confidence intervals predicted by GRFM are used by the metamodel assisted EA. It selects the promising members in each generation and carries out exact, costly evaluations only for them. The extensive use of the uncertainty information of predictions for screening the candidate solutions makes it possible to significantly reduce the computational cost of singleand multiobjective EA. This is adequately demonstrated in this paper by means of mathematical test cases and a multipoint airfoil design in aerodynamics\n</div> \n<p></p>"},{"id":730,"title":"Optimization and integration of ground vehicle systems","url":"https://www.researchgate.net/publication/242526750_Optimization_and_integration_of_ground_vehicle_systems","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article deals with the optimal design of ground vehicles and their subsystems, with particular reference to ‘active’ safety and comfort. A review of state-of-the-art optimization methods for solving vehicle system design problems, including the integration of electronic controls, is given, thus further encouraging the use of such methods as standard tools for automotive engineers. Particular attention is devoted to the class of methods pertaining to complex system design optimization, as well as approaches for the optimal design of complex systems under uncertainty. Some examples of design optimizations are given in the fields of vehicle system dynamics, powertrain/internal combustion engine design, active safety and ride comfort, vehicle system design and lightweight structures, advanced automotive electronics, and smart vehicles.\n</div> \n<p></p>"},{"id":731,"title":"Sequential Design of Computer Experiments to Minimize Integrated Response Functions","url":"https://www.researchgate.net/publication/216756632_Sequential_Design_of_Computer_Experiments_to_Minimize_Integrated_Response_Functions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In the last ten to fifteen years many,phenomena,that could only be studied using physical experiments can now,be studied by computer experiments. Advances in the mathematical modeling of many physical processes, in algorithms for solving mathematical systems, and in computer speeds, have combined to make it possible to replace some physical experiments with computer experiments. In a computer experiment, a deterministic output, y(x), is computed for each set of in- put variables, x. This paper is concerned with the commonly occuring situation in which there are two types of input variables: suppose x =( xc, xe )w herexc is a set of “manufacturing” (control) variables and xe is a set of “environmental” (noise) variables. Manufacturing variables can be controlled while environmental variables are not controllable but have values governed by some distribution. We introduce a sequential experimental design for finding the optimum of ? (xc )= E{y(xc, Xe)}, where the expectation is taken over the distribution of the environmental variables. The approach is Bayesian; the prior information is that y(x )i s ad,raw from as,ta- tionary Gaussian stochastic process with correlation function from the Mat´ ern class having unknown,parameters. The idea of the method,is to compute,the posterior expected “improvement” over the current optimum,for each untested site; the de- sign selects the next site to maximize the expected improvement. The procedure is illustrated with examples from the literature. Key words and phrases: Computer experiments, control variables, expected im-\n</div> \n<p></p>"},{"id":732,"title":"Combining global and local surrogate models to accelerate evolutionary optimization. IEEE Trans Syst Man Cybern Part C Appl Rev","url":"https://www.researchgate.net/publication/3421747_Combining_global_and_local_surrogate_models_to_accelerate_evolutionary_optimization_IEEE_Trans_Syst_Man_Cybern_Part_C_Appl_Rev","abstraction":"In this paper, we present a novel surrogate-assisted evolutionary optimization framework for solving computationally expensive problems. The proposed framework uses computationally cheap hierarchical surrogate models constructed through online learning to replace the exact computationally expensive objective functions during evolutionary search. At the first level, the framework employs a data-parallel Gaussian process based global surrogate model to filter the evolutionary algorithm (EA) population of promising individuals. Subsequently, these potential individuals undergo a memetic search in the form of Lamarckian learning at the second level. The Lamarckian evolution involves a trust-region enabled gradient-based search strategy that employs radial basis function local surrogate models to accelerate convergence. Numerical results are presented on a series of benchmark test functions and on an aerodynamic shape design problem. The results obtained suggest that the proposed optimization framework converges to good designs on a limited computational budget. Furthermore, it is shown that the new algorithm gives significant savings in computational cost when compared to the traditional evolutionary algorithm and other surrogate assisted optimization frameworks"},{"id":733,"title":"Predicting the Output from a Complex Computer Code When Fast Approximations Are Available","url":"https://www.researchgate.net/publication/31303344_Predicting_the_Output_from_a_Complex_Computer_Code_When_Fast_Approximations_Are_Available","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider prediction and uncertainty analysis for complex computer codes which can be run at different levels of sophistication. In particular, we wish to improve efficiency by combining expensive runs of the most complex versions of the code with relatively cheap runs from one or more simpler approximations. A Bayesian approach is described in which prior beliefs about the codes are represented in terms of Gaussian processes. An example is presented using two versions of an oil reservoir simulator.\n</div> \n<p></p>"},{"id":734,"title":"Designing Computer Experiments to Determine Robust Control Variables","url":"https://www.researchgate.net/publication/216756631_Designing_Computer_Experiments_to_Determine_Robust_Control_Variables","abstraction":"This paper is concerned with the design of computer experiments when there are two types of inputs: control variables and environmental variables. Control variables, also called manufacturing variables, are determined by a product designer while environmental variables, called noise variables in the quality control literature, are uncontrolled in the field but take values that are characterized by a probability distribution. Our goal is to find a set of control variables at which the response is insensitive to the value of the environmental variables, a “robust” choice of control variables. Such a choice ensures that the mean response is as insensitive as possible to perturbations of the nominal environmental variable distribution. We present a sequential strategy to select the inputs at which to observe the response so as to determine a robust setting of the control variables. Our solution is Bayesian; the prior takes the response as a draw from a stationary Gaussian stochastic process. Given the previous information, the sequential algorithm computes for each untested site the “improvement” over the current guess of the optimal robust setting. The design selects the next site to maximize the expected improvement criterion."},{"id":735,"title":"Multiobjective optimization of expensive-to-evaluate deterministic computer simulator models","url":"https://www.researchgate.net/publication/282400016_Multiobjective_optimization_of_expensive-to-evaluate_deterministic_computer_simulator_models","abstraction":"Many engineering design optimization problems contain multiple objective functions all of which are desired to be minimized, say. This paper proposes a method for identifying the Pareto Front and the Pareto Set of the objective functions when these functions are evaluated by expensive-to-evaluate deterministic computer simulators. The method replaces the expensive function evaluations by a rapidly computable approximator based on a Gaussian process (GP) interpolator. It sequentially selects new input sites guided by values of an \"improvement function\" given the current data. The method introduced in this paper provides two advances in the interpolator/improvement framework. First, it proposes an improvement function based on the \"modified maximin fitness function\" which is known to identify well-spaced non-dominated outputs when used in multiobjective evolutionary algorithms. Second, it uses a family of GP models that allows for dependence among output function values but which permits zero covariance should the data be consistent with this model. A closed-form expression is derived for the improvement function when there are two objective functions; simulation is used to evaluate it when there are three or more objectives. Examples from the multiobjective optimization literature are presented to show that the proposed procedure can improve substantially previously proposed statistical improvement criteria for the computationally intensive multiobjective optimization setting."},{"id":736,"title":"Review of surrogate modeling in water resources Water Resources Research 48:n/a-n/a","url":"https://www.researchgate.net/publication/258724624_Review_of_surrogate_modeling_in_water_resources_Water_Resources_Research_48na-na","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Surrogate modeling, also called metamodeling, has evolved and been\n <br> extensively used over the past decades. A wide variety of methods and\n <br> tools have been introduced for surrogate modeling aiming to develop and\n <br> utilize computationally more efficient surrogates of high-fidelity\n <br> models mostly in optimization frameworks. This paper reviews, analyzes,\n <br> and categorizes research efforts on surrogate modeling and applications\n <br> with an emphasis on the research accomplished in the water resources\n <br> field. The review analyzes 48 references on surrogate modeling arising\n <br> from water resources and also screens out more than 100 references from\n <br> the broader research community. Two broad families of surrogates namely\n <br> response surface surrogates, which are statistical or empirical\n <br> data-driven models emulating the high-fidelity model responses, and\n <br> lower-fidelity physically based surrogates, which are simplified models\n <br> of the original system, are detailed in this paper. Taxonomies on\n <br> surrogate modeling frameworks, practical details, advances, challenges,\n <br> and limitations are outlined. Important observations and some guidance\n <br> for surrogate modeling decisions are provided along with a list of\n <br> important future research directions that would benefit the common\n <br> sampling and search (optimization) analyses found in water resources.\n</div> \n<p></p>"},{"id":737,"title":"Quantifying uncertainty on Pareto fronts with Gaussian Process conditional simulations","url":"https://www.researchgate.net/publication/272891057_Quantifying_uncertainty_on_Pareto_fronts_with_Gaussian_Process_conditional_simulations","abstraction":"Multi-objective optimization algorithms aim at finding Pareto-optimal solutions. Recovering Pareto fronts or Pareto sets from a limited number of function evaluations are challenging problems. A popular approach in the case of expensive-to-evaluate functions is to appeal to metamodels. Kriging has been shown efficient as a base for sequential multi-objective optimization, notably through infill sampling criteria balancing exploitation and exploration such as the Expected Hypervolume Improvement. Here we consider Kriging metamodels not only for selecting new points, but as a tool for estimating the whole Pareto front and quantifying how much uncertainty remains on it at any stage of Kriging-based multi-objective optimization algorithms. Our approach relies on the Gaussian process interpretation of Kriging, and bases upon conditional simulations. Using concepts from random set theory, we propose to adapt the Vorob’ev expectation and deviation to capture the variability of the set of non-dominated points. Numerical experiments illustrate the potential of the proposed workflow, and it is shown on examples how Gaussian process simulations and the estimated Vorob’ev deviation can be used to monitor the ability of Kriging-based multi-objective optimization algorithms to accurately learn the Pareto front."},{"id":738,"title":"Adaptive Designs of Experiments for Accurate Approximation of Target Regions","url":"https://www.researchgate.net/publication/29623261_Adaptive_Designs_of_Experiments_for_Accurate_Approximation_of_Target_Regions","abstraction":"This paper addresses the issue of designing experiments for a metamodel that needs to be accurate for a certain level of the response value. Such situation is encountered in particular in constrained optimization and reliability analysis. Here, we propose an iterative strategy to build designs of experiments, which is based on an explicit trade-off between reduction of global uncertainty and exploration of the regions of interest. The method is illustrated on several test-problems. It is shown that a substantial reduction of error can be achieved in the crucial regions, with reasonable loss on the global accuracy. The method is finally applied to a reliability analysis problem; it is found that the adaptive designs significantly outperform classical space-filling designs."},{"id":739,"title":"Default priors for Gaussian processes. Ann Stat","url":"https://www.researchgate.net/publication/2120071_Default_priors_for_Gaussian_processes_Ann_Stat","abstraction":"Motivated by the statistical evaluation of complex computer models, we deal with the issue of objective prior specification for the parameters of Gaussian processes. In particular, we derive the Jeffreys-rule, independence Jeffreys and reference priors for this situation, and prove that the resulting posterior distributions are proper under a quite general set of conditions. A proper flat prior strategy, based on maximum likelihood estimates, is also considered, and all priors are then compared on the grounds of the frequentist properties of the ensuing Bayesian procedures. Computational issues are also addressed in the paper, and we illustrate the proposed solutions by means of an example taken from the field of complex computer model validation."},{"id":740,"title":"Cross-Entropy Method","url":"https://www.researchgate.net/publication/228940071_Cross-Entropy_Method","abstraction":"The cross-entropy method is a powerful heuristic tool for solving difficult estima-tion and optimization problems, based on Kullback–Leibler (or cross-entropy) minimization."},{"id":741,"title":"An informational approach to the global optimization of expensive-to-evaluate functions","url":"https://www.researchgate.net/publication/1960302_An_informational_approach_to_the_global_optimization_of_expensive-to-evaluate_functions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In many global optimization problems motivated by engineering applications, the number of function evaluations is severely limited by time or cost. To ensure that each evaluation contributes to the localization of good candidates for the role of global minimizer, a sequential choice of evaluation points is usually carried out. In particular, when Kriging is used to interpolate past evaluations, the uncertainty associated with the lack of information on the function can be expressed and used to compute a number of criteria accounting for the interest of an additional evaluation at any given point. This paper introduces minimizer entropy as a new Kriging-based criterion for the sequential choice of points at which the function should be evaluated. Based on \\emph{stepwise uncertainty reduction}, it accounts for the informational gain on the minimizer expected from a new evaluation. The criterion is approximated using conditional simulations of the Gaussian process model behind Kriging, and then inserted into an algorithm similar in spirit to the \\emph{Efficient Global Optimization} (EGO) algorithm. An empirical comparison is carried out between our criterion and \\emph{expected improvement}, one of the reference criteria in the literature. Experimental results indicate major evaluation savings over EGO. Finally, the method, which we call IAGO (for Informational Approach to Global Optimization) is extended to robust optimization problems, where both the factors to be tuned and the function evaluations are corrupted by noise.\n</div> \n<p></p>"},{"id":742,"title":"Structural reliability analysis using a standard deterministic finite element code","url":"https://www.researchgate.net/publication/245210410_Structural_reliability_analysis_using_a_standard_deterministic_finite_element_code","abstraction":"A method for performing a reliability analysis of structural systems within a standard finite element code is presented. This numerical procedure can be implemented in any finite element (f.e.) code having an internal optimization routine. The design points of structural problems are determined by calculating the minimum distance from the origin to the failure surface in a set of normalized variables, by using the minimization routine of the f.e. code. In order to test the procedure, simple structural systems are solved and the results are compared with those obtained by using different approaches. Some examples of application of the procedure for the reliability analysis of real structures are presented."},{"id":743,"title":"Graded Learning for Object Detection","url":"https://www.researchgate.net/publication/2240462_Graded_Learning_for_Object_Detection","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Our goal is to detect all instances of a generic object class, such as a face, in greyscale scenes. The design of the algorithm is motivated by computational efficiency. The search is coarse-to-fine in both the exploration of poses and the representation of the object class. Starting from training examples, we recursively learn a hierarchy of spatial arrangements of edge fragments, graded by their size (sparsity). The arrangements have no a priori semantic or geometric interpretation. Instead, they are selected to be \"decomposable\": Each can be split into two correlated subarrangements, each of which can be further divided, etc. As a result, the probability of an arrangement of size k appearing on an object instance decays slowly with k. We demonstrate this both theoretically and in experiments in which detection means finding a sufficient number of arrangements of various sizes. 1. Avant-Projet IMEDIA, INRIA-Rocquencourt, Domaine de Voluceau, B.P.105, 78153 Le Chesnay. Supported in p...\n</div> \n<p></p>"},{"id":744,"title":"Screening, Predicting, and Computer Experiments","url":"https://www.researchgate.net/publication/216301661_Screening_Predicting_and_Computer_Experiments","abstraction":"Many scientific phenomena are now investigated by complex computer models or codes. Given the input values, the code produces one or more outputs via a complex mathematical model. Often the code is expensive to run, and it may be necessary to build a computationally cheaper predictor to enable, for example, optimization of the inputs. If there are many input factors, an initial step in building a predictor is identifying (screening) the active factors. We model the output of the computer code as the realization of a stochastic process. This model has a number of advantages. First, it provides a statistical basis, via the likelihood, for a stepwise algorithm to determine the important factors. Second, it is very flexible, allowing nonlinear and interaction effects to emerge without explicitly modeling such effects. Third, the same data are used for screening and building the predictor, so expensive runs are efficiently used. We illustrate the methodology with two examples, both having 20 input variables. In these examples, we identify the important variables, detect curvature and interactions, and produce a useful predictor with 30–50 runs of the computer code."},{"id":745,"title":"Efficient methodologies for reliability-based design optimization of composite panels","url":"https://www.researchgate.net/publication/289250756_Efficient_methodologies_for_reliability-based_design_optimization_of_composite_panels","abstraction":"The main factors governing the design of composite laminates are the geometrical dimensions, the stacking sequence –including ply thickness and orientation angles–, the mechanical properties of the materials, the applied loads and the performance requirements. Most of these factors are commonly affected by uncertainty and this should be taken into account when designing these structures. Thus, uncertainty quantification should be used to evaluate the performance requirements and a reliability-based procedure is advisable when the design is optimized. However, these methods present several drawbacks, like the lack of trustworthy information about the uncertainty present in the variables of the model or the high computational cost required to apply the algorithms to medium to large models. This paper evaluates several methodologies for design optimization of composite panels under uncertainty. The uncertainty quantification is performed using stochastic expansion and limit state approximation methods. Monte Carlo sampling is also used to verify reliability results. The optimization process is carried out using gradient-based and genetic algorithms, with either continuous or discrete design variables. Surrogate methods, including polynomial, kriging, multivariate adaptive regression splines, and artificial neural networks, as well as parallel computing, have been leveraged to keep analysis times under acceptable levels. An application example of a stiffened composite panel of an aircraft fuselage is presented to demonstrate the computational performance and the accuracy of the methods. Results show major improvement in analysis time without compromising on precision."},{"id":746,"title":"Gaussian process surrogates for failure detection: a Bayesian experimental design approach","url":"https://www.researchgate.net/publication/281895996_Gaussian_process_surrogates_for_failure_detection_a_Bayesian_experimental_design_approach","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n An important task of uncertainty quantification is to identify {the\n <br> probability of} undesired events, in particular, system failures, caused by\n <br> various sources of uncertainties. In this work we consider the construction of\n <br> Gaussian {process} surrogates for failure detection and failure probability\n <br> estimation. In particular, we consider the situation that the underlying\n <br> computer models are extremely expensive, and in this setting, determining the\n <br> sampling points in the state space is of essential importance. We formulate the\n <br> problem as an optimal experimental design for Bayesian inferences of the limit\n <br> state (i.e., the failure boundary) and propose an efficient numerical scheme to\n <br> solve the resulting optimization problem. In particular, the proposed\n <br> limit-state inference method is capable of determining multiple sampling points\n <br> at a time, and thus it is well suited for problems where multiple computer\n <br> simulations can be performed in parallel. The accuracy and performance of the\n <br> proposed method is demonstrated by both academic and practical examples.\n</div> \n<p></p>"},{"id":747,"title":"Estimating the small failure probability of a nuclear passive safety system by means of an efficient Adaptive Metamodel-Based Subset Importance Sampling method","url":"https://www.researchgate.net/publication/280310254_Estimating_the_small_failure_probability_of_a_nuclear_passive_safety_system_by_means_of_an_efficient_Adaptive_Metamodel-Based_Subset_Importance_Sampling_method","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The assessment of the functional failure probability of a thermal-hydraulic (T-H) passive system can be done by Monte Carlo (MC) sampling of the uncertainties affecting the T-H system model and its parameters. The computational effort associated to this approach can be prohibitive because of the large number of lengthy T-H code simulations necessary for the accurate and precise quantification of the (typically small) failure probability. To overcome this issue, in the present paper we propose an Adaptive Metamodel-Based Subset Importance Sampling (AM-SIS) approach that originally and efficiently combines the powerful features of several advanced computational methods of literature: in particular, Subset Simulation (SS) and fast-running Artificial Neural Network (ANN) metamodels are coupled within an adaptive MC-based Importance Sampling (IS) scheme. The objective is to construct a fully nonparametric estimator of the ideal, zero-variance Importance Sampling Density (ISD) and iteratively refine it, in such a way that: (i) the accuracy and precision of the corresponding failure probability estimates are improved and (ii) the number of burdensome T-H code runs is reduced, along with the associated computational cost. The method is demonstrated on a case study of an emergency passive decay heat removal system of a Gas-cooled Fast Reactor (GFR). A thorough comparison is made with respect to several advanced MC methods of literature.\n</div> \n<p></p>"},{"id":748,"title":"A new surrogate modeling technique combining Kriging and polynomial chaos expansions – Application to uncertainty analysis in computational dosimetry","url":"https://www.researchgate.net/publication/272101816_A_new_surrogate_modeling_technique_combining_Kriging_and_polynomial_chaos_expansions_-_Application_to_uncertainty_analysis_in_computational_dosimetry","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In numerical dosimetry, the recent advances in high performance computing led to a strong reduction of the required computational time to assess the specific absorption rate (SAR) characterizing the human exposure to electromagnetic waves. However, this procedure remains time-consuming and a single simulation can request several hours. As a consequence, the influence of uncertain input parameters on the SAR cannot be analyzed using crude Monte Carlo simulation. The solution presented here to perform such an analysis is surrogate modeling. This paper proposes a novel approach to build such a surrogate model from a design of experiments. Considering a sparse representation of the polynomial chaos expansions using least-angle regression as a selection algorithm to retain the most influential polynomials, this paper proposes to use the selected polynomials as regression functions for the universal Kriging model. The leave-one-out cross validation is used to select the optimal number of polynomials in the deterministic part of the Kriging model. The proposed approach, called LARS-Kriging-PC modeling, is applied to three benchmark examples and then to a full-scale metamodeling problem involving the exposure of a numerical fetus model to a femtocell device. The performances of the LARS-Kriging-PC are compared to an ordinary Kriging model and to a classical sparse polynomial chaos expansion. The LARS-Kriging-PC appears to have better performances than the two other approaches. A significant accuracy improvement is observed compared to the ordinary Kriging or to the sparse polynomial chaos depending on the studied case. This approach seems to be an optimal solution between the two other classical approaches. A global sensitivity analysis is finally performed on the LARS-Kriging-PC model of the fetus exposure problem.\n</div> \n<p></p>"},{"id":749,"title":"Polynomial-Chaos-based Kriging","url":"https://www.researchgate.net/publication/272359280_Polynomial-Chaos-based_Kriging","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Computer simulation has become the standard tool in many engineering fields\n <br> for designing and optimizing systems, as well as for assessing their\n <br> reliability. To cope with demanding analysis such as optimization and\n <br> reliability, surrogate models (a.k.a meta-models) have been increasingly\n <br> investigated in the last decade. Polynomial Chaos Expansions (PCE) and Kriging\n <br> are two popular non-intrusive meta-modelling techniques. PCE surrogates the\n <br> computational model with a series of orthonormal polynomials in the input\n <br> variables where polynomials are chosen in coherency with the probability\n <br> distributions of those input variables. On the other hand, Kriging assumes that\n <br> the computer model behaves as a realization of a Gaussian random process whose\n <br> parameters are estimated from the available computer runs, i.e. input vectors\n <br> and response values. These two techniques have been developed more or less in\n <br> parallel so far with little interaction between the researchers in the two\n <br> fields. In this paper, PC-Kriging is derived as a new non-intrusive\n <br> meta-modeling approach combining PCE and Kriging. A sparse set of orthonormal\n <br> polynomials (PCE) approximates the global behavior of the computational model\n <br> whereas Kriging manages the local variability of the model output. An adaptive\n <br> algorithm similar to the least angle regression algorithm determines the\n <br> optimal sparse set of polynomials. PC-Kriging is validated on various benchmark\n <br> analytical functions which are easy to sample for reference results. From the\n <br> numerical investigations it is concluded that PC-Kriging performs better than\n <br> or at least as good as the two distinct meta-modeling techniques. A larger gain\n <br> in accuracy is obtained when the experimental design has a limited size, which\n <br> is an asset when dealing with demanding computational models.\n</div> \n<p></p>"},{"id":750,"title":"A new integral loss function for Bayesian optimization","url":"https://www.researchgate.net/publication/264936662_A_new_integral_loss_function_for_Bayesian_optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the problem of maximizing a real-valued continuous function $f$\n <br> using a Bayesian approach. Since the early work of Jonas Mockus and Antanas\n <br> \\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by\n <br> considering the loss function $\\max f - M_n$ (where $M_n$ denotes the best\n <br> function value observed after $n$ evaluations of $f$). This loss function puts\n <br> emphasis on the value of the maximum, at the expense of the location of the\n <br> maximizer. In the special case of a one-step Bayes-optimal strategy, it leads\n <br> to the classical Expected Improvement (EI) sampling criterion. This is a\n <br> special case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk\n <br> associated to a certain uncertainty measure (here, the expected loss) on the\n <br> quantity of interest is minimized at each step of the algorithm. In this\n <br> article, assuming that $f$ is defined over a measure space $(\\mathbb{X},\n <br> \\lambda)$, we propose to consider instead the integral loss function\n <br> $\\int_{\\mathbb{X}} (f - M_n)_{+}\\, d\\lambda$, and we show that this leads, in\n <br> the case of a Gaussian process prior, to a new numerically tractable sampling\n <br> criterion that we call $\\rm EI^2$ (for Expected Integrated Expected\n <br> Improvement). A numerical experiment illustrates that a SUR strategy based on\n <br> this new sampling criterion reduces the error on both the value and the\n <br> location of the maximizer faster than the EI-based strategy.\n</div> \n<p></p>"},{"id":751,"title":"Sequential Design of Computer Experiments for the Assessment of Fetus Exposure to Electromagnetic Fields","url":"https://www.researchgate.net/publication/277977174_Sequential_Design_of_Computer_Experiments_for_the_Assessment_of_Fetus_Exposure_to_Electromagnetic_Fields","abstraction":"In this paper, we describe four sequential sampling strategies for estimating the quantile of Y = f(X), where X has a known distribution in and f is a deterministic unknown, expensive-to-evaluate real-valued function. These approaches all consist in modeling f as a sample of a well-chosen Gaussian process and aim at estimating the quantile by using as few evaluations of f as possible. The different methodologies are first compared through various numerical experiments. Then, in the framework of the ANR-JST FETUS project, we apply our strategies to a real example corresponding to the exposure of a Japanese pregnant-woman model and her 26-week-old fetus to a plane wave. Finally, we compare our methodologies on a simplified geometric model designed for modeling the fetus exposure to plane waves."},{"id":752,"title":"KrigInv: An efficient and user-friendly implementation of batch-sequential inversion strategies based on Kriging","url":"https://www.researchgate.net/publication/262369166_KrigInv_An_efficient_and_user-friendly_implementation_of_batch-sequential_inversion_strategies_based_on_Kriging","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Several strategies relying on kriging have recently been proposed for adaptively estimating contour lines and excursion sets of functions under severely limited evaluation budget. The recently released R package KrigInv is presented and offers a sound implementation of various sampling criteria for those kinds of inverse problems. KrigInv is based on the DiceKriging package, and thus benefits from a number of options concerning the underlying kriging models. Six implemented sampling criteria are detailed in a tutorial and illustrated with graphical examples. Different functionalities of KrigInv are gradually explained. Additionally, two recently proposed criteria for batch-sequential inversion are presented, enabling advanced users to distribute function evaluations in parallel on clusters or clouds of machines. Finally, auxiliary problems are discussed. These include the fine tuning of numerical integration and optimization procedures used within the computation and the optimization of the considered criteria.\n</div> \n<p></p>"},{"id":753,"title":"Comparisons of regression constants fitted by maximum likelihood to four common transformations of binomial data","url":"https://www.researchgate.net/publication/9452636_Comparisons_of_regression_constants_fitted_by_maximum_likelihood_to_four_common_transformations_of_binomial_data","abstraction":"An electronic computer was used to fit regression constants of arcsine, logit, log-log and inverse normal (probit) transforms of binomial data. The data ranged over a variety of types, biologically and mathematically, but were ‘complex’(with several independent variables). This empirical study leads one to assume, very unequivocally, that there is no practical difference in analyses using these four transformations. I thank Drs Daphne Trasler and Dorothy Warburton for permission to quote unpublished data and the McGill Computing Centre for advice and assistance."},{"id":754,"title":"Quick Method for Choosing a Transf orrnation","url":"https://www.researchgate.net/publication/241735679_Quick_Method_for_Choosing_a_Transf_orrnation","abstraction":"The choice of non-linear transformations in the analysis of data can frequently be simplified by restricting the possible transformations to a particular family. Tukey has shown that the “simple family” has many desirable properties from this point of view. This family can be represented as the set of solutions to a third order differential equation and the constant of this equation provides a convenient index of the family. This index may be approximated by substituting the given data into the corresponding difference equation. The resulting approximation can then be used for rough solutions or as a starting value for the iterative solution of the maximum likelihood equations given by Turner. Two examples are provided to demonstrate the procedure."},{"id":755,"title":"The first toxicological study of the antiozonant and research tool ethylene diurea (EDU) using a Lemna minor L. bioassay: Hints to its mode of action","url":"https://www.researchgate.net/publication/291261988_The_first_toxicological_study_of_the_antiozonant_and_research_tool_ethylene_diurea_EDU_using_a_Lemna_minor_L_bioassay_Hints_to_its_mode_of_action","abstraction":"The antiozonant and research tool ethylene diurea (EDU) is widely studied as a phytoprotectant against the widespread pollutant ground-surface ozone. Although it has been extensively used, its potential toxicity in the absence of ozone is unknown and its mode of action is unclear. The purpose of this research was to toxicologically assess EDU and to further investigate its mode of action using Lemna minor L. as a model organism. Application of EDU concentrations greater than 593 mg L?1 (practically 600 mg L?1) resulted in adverse inhibition of colony growth. As no-observed-toxic-effects concentration (NOEL) we recommend a concentration of 296 mg L?1 (practically 300 mg L?1). A hormetic response was detected, i.e. stimulatory effects of low EDU concentrations, which may indicate overcompensation in response to disruption in homeostasis. Growth inhibition and suppressed biomass were associated with impacted chlorophyll a fluorescence (?PSII, qP and ETR). Furthermore, EDU increased mesophyll thickness, as indicated by frond succulence index. Applications of concentrations ?593 mg L?1 to uncontrolled environments should be avoided due to potential toxicity to sensitive organisms and the environment.  It can be found at the following link: http://dx.doi.org/10.1016/j.envpol.2015.12.051"},{"id":756,"title":"Changes in the composition of ichthyoplankton assemblage and plastic debris in mangrove creeks relative to moon phases","url":"https://www.researchgate.net/publication/287327475_Changes_in_the_composition_of_ichthyoplankton_assemblage_and_plastic_debris_in_mangrove_creeks_relative_to_moon_phases","abstraction":"Lunar influence on the distribution of fish larvae, zooplankton and plastic debris in mangrove creeks of the Goiana Estuary, Brazil, was studied over a lunar cycle. Cetengraulis edentulus, Anchovia clupeoides and Rhinosardinia bahiensis were the most abundant fish larvae (56·6%), independent of the moon phase. The full moon had a positive influence on the abundance of Gobionellus oceanicus, Cynoscion acoupa and Atherinella brasiliensis, and the new moon on Ulaema lefroyi. The full and new moons also influenced the number of zoeae and megalopae of Ucides cordatus, protozoeae and larvae of caridean shrimps, and the number of hard and soft plastic debris, both &lt;5 and &gt;5 mm. Micro and macroplastics were present in samples from all 12 creeks studied, at densities similar to the third most abundant taxon, R. bahiensis. Cetengraulis edentulus and R. bahiensis showed a strong positive correlation with the last quarter moon, when there was less zooplankton available in the creeks and higher abundance of microplastic threads. Anchovia clupeoides, Diapterus rhombeus, U. lefroyi and hard microplastics were positively associated with different moon phases, when calanoid copepods, Caridean larvae and zoeae of U. cordatus were highly available in the creeks. Cynoscion acoupa, G. oceanicus and A. brasiliensis were strongly associated with the full moon, when protozoeae of caridean shrimps and megalopae of U. cordatus were also highly available, as were hard and soft macroplastics, paint chips (&lt;5 mm) and soft microplastics. The results reinforce the role of mangrove creeks as nursery habitats. The moon phases influenced the distribution of fish larvae species, zooplankton and plastic debris by changing their compositions and abundances in the mangrove creeks of the Goiana Estuary when under the influence of different tidal current regimes."},{"id":757,"title":"Vocabulary does not complicate the simple view of reading","url":"https://www.researchgate.net/publication/287214394_Vocabulary_does_not_complicate_the_simple_view_of_reading","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Gough and Tunmer’s (1986) simple view of reading (SVR) proposed that reading comprehension (RC) is a function of language comprehension (LC) and word recognition/decoding. Braze et al. (2007) presented data suggesting an extension of the SVR in which knowledge of vocabulary (V) affected RC over and above the effects of LC. Tunmer and Chapman (2012) found a similar independent contribution of V to RC when the data were analyzed by hierarchical regression. However, additional analysis by factor analysis and structural equation modeling indicated that the effect of V on RC was, in fact, completely captured by LC itself and there was no need to posit a separate direct effect of V on RC. In the present study, we present new data from young adults with sub-optimal reading skill (N = 286). Latent variable and regression analyses support Gough and Tunmer’s original proposal and the conclusions of Tunmer and Chapman that V can be considered a component of LC and not an independent contributor to RC.\n</div> \n<p></p>"},{"id":758,"title":"Portfolio theory as a management tool to guide conservation and restoration of multi-stock fish populations","url":"https://www.researchgate.net/publication/287975036_Portfolio_theory_as_a_management_tool_to_guide_conservation_and_restoration_of_multi-stock_fish_populations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Habitat degradation and harvest have upset the natural buffering mechanism (i.e., portfolio\n <br> effects) of many large-scale multi-stock fisheries by reducing spawning stock diversity that is vital for\n <br> generating population stability and resilience. The application of portfolio theory offers a means to guide\n <br> management activities by quantifying the importance of multi-stock dynamics and suggesting\n <br> conservation and restoration strategies to improve naturally occurring portfolio effects. Our application\n <br> of portfolio theory to Lake Erie Sander vitreus (walleye), a large population that is supported by riverine\n <br> and open-lake reef spawning stocks, has shown that portfolio effects generated by annual inter-stock larval\n <br> fish production are currently suboptimal when compared to potential buffering capacity. Reduced\n <br> production from riverine stocks has resulted in a single open-lake reef stock dominating larval production,\n <br> and in turn, high inter-annual recruitment variability during recent years. Our analyses have shown (1) a\n <br> weak average correlation between annual river and reef larval production ( ¯q ¼ 0.24), suggesting that a\n <br> natural buffering capacity exists in the population, and (2) expanded annual production of larvae (potential\n <br> recruits) from riverine stocks could stabilize the fishery by dampening inter-annual recruitment variation.\n <br> Ultimately, our results demonstrate how portfolio theory can be used to quantify the importance of\n <br> spawning stock diversity and guide management on ecologically relevant scales (i.e., spawning stocks)\n <br> leading to greater stability and resilience of multi-stock populations and fisheries.\n</div> \n<p></p>"},{"id":759,"title":"Site-scale isotopic variations along a river course help localize drainage basin influence on river food webs","url":"https://www.researchgate.net/publication/284805934_Site-scale_isotopic_variations_along_a_river_course_help_localize_drainage_basin_influence_on_river_food_webs","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In human-impacted rivers, nutrient pollution has the potential to disrupt biodiversity organisation and ecosystem functioning, prompting calls for effective monitoring and management. Pollutants, together with natural variations, can modify the isotopic signature of aquatic organisms. Accordingly, we explored the potential of isotopic variations as an indicator of drainage basin influences on river food webs. We assessed stable N and C isotopes within six food webs along a river affected by multiple pollution sources. CORINE land cover maps and Digital Elevation Models (DEMs) were also applied to understand the impact on surface waters of anthropogenic pressures affecting the catchment. N isotopic signatures of taxa fell in association with ammonium inputs from agriculture, indicating that nitrogen pollution was related to synthetic fertilizers. Isotopic variations were consistent across trophic levels, highlighting site-specific communities and identifying taxa exposed to pollutants. This allowed us to locate point sources of disturbance, suggesting that food web structure plays a key role in pollutant compartmentalisation along the river. Thematic maps and DEMs helped understand how the anthropogenic impact on river biota is mediated by hydro-geomorphology. Thus, the integration of site-scale analyses of stable isotopes and land use represents a promising research pathway for explorative nutrient pollution monitoring in human-impacted rivers.\n</div> \n<p></p>"},{"id":760,"title":"Spatial frequency processing in the central and peripheral visual field: Effects of gaze-contingent filter strength and size","url":"https://www.researchgate.net/publication/283986483_Spatial_frequency_processing_in_the_central_and_peripheral_visual_field_Effects_of_gaze-contingent_filter_strength_and_size","abstraction":"Visuo-spatial attention and gaze control depend on the interaction of foveal and peripheral processing. The foveal and peripheral compartments of the visual field are differentially sensitive to parts of the spatial frequency spectrum. In two experiments, we investigated how the selective attenuation of spatial frequencies in the central or the peripheral affects eye-movement behavior during real-world scene viewing. Gaze-contingent low-pass or high-pass filters with varying filter cutoffs (Experiment 1) or filter size (Experiment 2) were applied. Compared with unfiltered control conditions, mean fixation durations increased more with central high-pass and peripheral low-pass filtering than with central low-pass and peripheral high-pass filtering. Increasing filter size and filter level progressively increased fixation durations in half of the conditions, but had no effect with the other conditions. The effects suggest that fixation durations prolong with increasing processing difficulty as long as the available information is useful. When fixation durations were unaffected by increasing processing difficulty, saccade amplitudes were modulated instead, indicating a trade-off between saccade timing and saccadic selection. Taken together, interactions of perception and gaze control are compatible with a perceptual economy account."},{"id":761,"title":"Temporal variation in the synchrony of weather and its consequences for spatiotemporal population dynamics","url":"https://www.researchgate.net/publication/284213209_Temporal_variation_in_the_synchrony_of_weather_and_its_consequences_for_spatiotemporal_population_dynamics","abstraction":"Over large areas, synchronous fluctuations in population density are often attributed to environmental stochasticity (e.g., weather) shared among local populations. This concept was first advanced by Patrick Moran who showed, based on several assumptions, that long-term population synchrony will equal the synchrony of environmental stochasticity among locations. We examine the consequences of violating one of Moran's assumptions, namely that environmental synchrony is constant through time. We demonstrate that the synchrony of weather conditions from regions across the United States varied considerably from 1895 to 2010. Using a simulation model modified from Moran's original study, we show that temporal variation in environmental synchrony can cause changes in population synchrony, which in turn can temporarily increase or decrease the amplitude of regional-scale population fluctuations. A case study using the gypsy moth (Lymantria dispar) provides empirical support for these predictions. This study provides theoretical and empirical evidence that temporal variation in environmental synchrony can be used to identify factors that synchronize population fluctuations and highlights a previously underappreciated cause of variability in population dynamics."},{"id":762,"title":"Time budgets of finishing bulls housed in an uninsulated barn or at pasture","url":"https://www.researchgate.net/publication/283347714_Time_budgets_of_finishing_bulls_housed_in_an_uninsulated_barn_or_at_pasture","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This study aimed at comparing the behaviour of finishing bulls raised in an uninsulated barn (UB) and at pasture (PAS). In experiment 1, dairy bulls were housed in an uninsulated barn (two groups of five bulls, 32 m 2 /pen) or at pasture (groups of four and five bulls, 5000 m 2 /paddock). In experiment 2, Hereford bulls were housed in an unin-sulated barn (three groups of four or five bulls, 32 m 2 /pen) or at pasture (three groups of five bulls, 5000 m 2 /pad-dock). There were no differences in drinking, social licking, butting, other social behaviour, self-licking or idling between the UB and PAS bulls. The UB bulls spent more time in lying, ruminating, oral explorative and manipulative behaviour and rubbing and less time foraging and walking than the PAS bulls. The UB bulls performed more social licking and oral manipulation of objects and less mounting than the PAS bulls. These differences resulted most probably from the different feeding regimes and different space allowances.\n</div> \n<p></p>"},{"id":763,"title":"Subset Simulation and its Application to Seismic Risk Based on Dynamic Analysis","url":"https://www.researchgate.net/publication/228591568_Subset_Simulation_and_its_Application_to_Seismic_Risk_Based_on_Dynamic_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A method is presented for efficiently computing small failure probabilities encountered in seismic risk problems involving dynamic analysis. It is based on a procedure recently developed by the writers called Subset Simulation in which the central idea is that a small failure probability can be expressed as a product of larger conditional failure probabilities, thereby turning the problem of simulating a rare failure event into several problems that involve the conditional simulation of more frequent events. Markov chain Monte Carlo simulation is used to efficiently generate the conditional samples, which is otherwise a nontrivial task. The original version of Subset Simulation is improved by allowing greater flexibility for incorporating prior information about the reliability problem so as to increase the efficiency of the method. The method is an effective simulation procedure for seismic performance assessment of structures in the context of modern performance-based design. This application is illustrated by considering the failure of linear and nonlinear hysteretic structures subjected to uncertain earthquake ground motions. Failure analysis is also carried out using the Markov chain samples generated during Subset Simulation to yield information about the probable scenarios that may occur when the structure fails.\n</div> \n<p></p>"},{"id":764,"title":"Sequential monte carlo samplers","url":"https://www.researchgate.net/publication/1825409_Sequential_monte_carlo_samplers","abstraction":"This paper shows how one can use Sequential Monte Carlo methods to perform what is typically done using Markov chain Monte Carlo methods. This leads to a general class of principled integration and genetic type optimization methods based on interacting particle systems."},{"id":765,"title":"Efficient Monte Carlo simulation via the generalized splitting method","url":"https://www.researchgate.net/publication/220286431_Efficient_Monte_Carlo_simulation_via_the_generalized_splitting_method","abstraction":"We describe a new Monte Carlo algorithm for the consistent and unbiased estimation of multidimensional integrals and the efficient sampling from multidimensional densities. The algorithm is inspired by the classical splitting method and can be applied to general static simulation models. We provide examples from rare-event probability estimation, counting, and sampling, demonstrating that the proposed method can outperform existing Markov chain sampling methods in terms of convergence speed and accuracy."},{"id":766,"title":"A non asymptotic variance theorem for unnormalized Feynman-Kac particle models","url":"https://www.researchgate.net/publication/29617444_A_non_asymptotic_variance_theorem_for_unnormalized_Feynman-Kac_particle_models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present a non asymptotic theorem for interacting particle approximations of unnormalized Feynman-Kac models. We provide an original stochastic analysis based on Feynman-Kac semigroup techniques combined with recently developed coalescent tree-based functional representations of particle block distributions. We present some regularity conditions under which the $\\LL_2$-relative error of these weighted particle measures grows linearly with respect to the time horizon yielding what seems to be the first results of this type for this class of unnormalized models. We also illustrate these results in the context of particle simulation of static Boltzmann-Gibbs measures and restricted distributions, with a special interest in rare event analysis.\n</div> \n<p></p>"},{"id":767,"title":"Sequential Monte Carlo Samplers for Rare Events","url":"https://www.researchgate.net/publication/200707517_Sequential_Monte_Carlo_Samplers_for_Rare_Events","abstraction":"We present novel sequential Monte Carlo (SMC) algorithms for the simulation of two broad classes of rare events which,are suitable for the estimation of tail probabilities and probability density functions in the regions of rare events, as well as the simulation of rare system trajectories. These methods have some,connection with previously proposed importance sampling (IS) and interacting particle system (IPS) methodologies, particularly those of [8, 4], but differ significantly from previous approaches,in a number,of respects: especially in that they operate directly on the path space of the Markov process of interest."},{"id":768,"title":"An Efficient Algorithm for Rare-event Probability Estimation, Combinatorial Optimization, and Counting","url":"https://www.researchgate.net/publication/225469492_An_Efficient_Algorithm_for_Rare-event_Probability_Estimation_Combinatorial_Optimization_and_Counting","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Although importance sampling is an established and effective sampling and estimation technique, it becomes unstable and unreliable\n <br> for high-dimensional problems. The main reason is that the likelihood ratio in the importance sampling estimator degenerates\n <br> when the dimension of the problem becomes large. Various remedies to this problem have been suggested, including heuristics\n <br> such as resampling. Even so, the consensus is that for large-dimensional problems, likelihood ratios (and hence importance\n <br> sampling) should be avoided. In this paper we introduce a new adaptive simulation approach that does away with likelihood\n <br> ratios, while retaining the multi-level approach of the cross-entropy method. Like the latter, the method can be used for\n <br> rare-event probability estimation, optimization, and counting. Moreover, the method allows one to sample exactly from the\n <br> target distribution rather than asymptotically as in Markov chain Monte Carlo. Numerical examples demonstrate the effectiveness\n <br> of the method for a variety of applications.\n</div> \n<p></p>"},{"id":769,"title":"Optimal Probabilistic Fingerprint Codes","url":"https://www.researchgate.net/publication/2864441_Optimal_Probabilistic_Fingerprint_Codes","abstraction":"We construct binary codes for fingerprinting. Our codes for n users that are ?-secure against c pirates have length O(c 2 log(n/?)). This improves the codes proposed by D. Boneh and J. Shaw [IEEE Trans. Inf. Theory 44, 1897–1905 (1998; Zbl 0931.94051)] whose length is approximately the square of this length. Our codes are probabilistic. By proving matching lower bounds we establish that the length of these codes is best within a constant factor for reasonable error probabilities. This lower bound generalizes the bound found independently by C. Peikert, A. Shelat and A. Smith [in: Proceedings of the fourteenth annual ACM-SIAM symposium on discrete algorithms, Baltimore, MD, USA, January 12–14, 2003. New York, NY: Association for Computing Machinery; Philadelphia, PA: Society for Industrial and Applied Mathematics. 472–479 (2003; Zbl 1092.68605)] that applies to a limited class of codes. Our results also imply that randomized fingerprint codes over a binary alphabet are as powerful as those over an arbitrary alphabet, and also the equal strength of two distinct models for fingerprinting."},{"id":770,"title":"Digital fingerprinting codes: Problem statements, constructions, identification of traitors","url":"https://www.researchgate.net/publication/3084692_Digital_fingerprinting_codes_Problem_statements_constructions_identification_of_traitors","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider a general fingerprinting problem of digital data under which coalitions of users can alter or erase some bits in their copies in order to create an illegal copy. Each user is assigned a fingerprint which is a word in a fingerprinting code of size M (the total number of users) and length n. We present binary fingerprinting codes secure against size-t coalitions which enable the distributor (decoder) to recover at least one of the users from the coalition with probability of error exp(-?(n)) for M=exp(?(n)). This is an improvement over the best known schemes that provide the error probability no better than exp(-?(n\n <sup>1</sup>2/)) and for this probability support at most exp(O(n\n <sup>1</sup>2/)) users. The construction complexity of codes is polynomial in n. We also present versions of these constructions that afford identification algorithms of complexity poly(n)=polylog(M), improving over the best previously known complexity of ?(M). For the case t=2, we construct codes of exponential size with even stronger performance, namely, for which the distributor can either recover both users from the coalition with probability 1-exp(?(n)), or identify one traitor with probability 1.\n</div> \n<p></p>"},{"id":771,"title":"An island particle algorithm for rare event analysis","url":"https://www.researchgate.net/publication/284900779_An_island_particle_algorithm_for_rare_event_analysis","abstraction":"Estimating rare event probability with accuracy is of great interest for safety and reliability applications. In this paper, we focus on rare events which can be modeled by a threshold exceedance of a deterministic input–output function with random inputs. Some parameters of this function or density parameters of input random variables may be fixed by an experimenter for simplicity reasons. From a risk analysis point of view, it is not only interesting to evaluate the probability of a critical event but it is also important to determine the impact of such tuning of parameters on the realization of a critical event, because a bad estimation of these parameters can strongly modify rare event probability estimations. In the present paper, we present an example of island particle algorithm referred to as sequential Monte Carlo square (SMC2). This algorithm gives an estimate of the law of random phenomena that leads to critical events. The principles of this statistical technique are described throughout this article and its results are analysed on different realistic aerospace test cases."},{"id":772,"title":"Point Process-based Monte Carlo Estimation","url":"https://www.researchgate.net/publication/269876959_Point_Process-based_Monte_Carlo_Estimation","abstraction":"This paper addresses the issue of estimating the expectation of a real-valued random variable of the form $X = g(\\mathbf{U})$ where $g$ is a deterministic function and $\\mathbf{U}$ can be a random finite- or infinite-dimensional vector. Using recent results on rare event simulation, we propose a unified framework for dealing with both probability and mean estimation for such random variables, \\ie linking algorithms such as Tootsie Pop Algorithm (TPA) or Last Particle Algorithm with nested sampling. Especially, it extends nested sampling as follows: first the random variable $X$ does not need to be bounded any more: it gives the principle of an ideal estimator with an infinite number of terms that is unbiased and always better than a classical Monte Carlo estimator -- in particular it has a finite variance as soon as there exists $k \\in \\mathbb{R} &gt; 1$ such that $E[X^k] &lt; \\infty$. Moreover we address the issue of nested sampling termination and show that a random truncation of the sum can preserve unbiasedness while increasing the variance only by a factor up to 2 compared to the ideal case. We also build an unbiased estimator with fixed computational budget which supports a Central Limit Theorem and discuss parallel implementation of nested sampling, which can dramatically reduce its computational cost. Finally we extensively study the case where $X$ is heavy-tailed."},{"id":773,"title":"A surrogate accelerated multicanonical Monte Carlo method for uncertainty quantification","url":"https://www.researchgate.net/publication/281312187_A_surrogate_accelerated_multicanonical_Monte_Carlo_method_for_uncertainty_quantification","abstraction":"In this work we consider a class of uncertainty quantification problems where the system performance or reliability is characterized by a scalar parameter $y$. The performance parameter $y$ is random due to the presence of various sources of uncertainty in the system, and our goal is to estimate the probability density function (PDF) of $y$. We propose to use the multicanonical Monte Carlo (MMC) method, a special type of adaptive importance sampling algorithm, to compute the PDF of interest. Moreover, we develop an adaptive algorithm to construct local Gaussian process surrogates to further accelerate the MMC iterations. With numerical examples we demonstrate that the proposed method can achieve several orders of magnitudes of speedup over the standard Monte Carlo method."},{"id":774,"title":"Computing transition rates for the 1-D stochastic Ginzburg--Landau--Allen--Cahn equation for finite-amplitude noise with a rare event algorithm","url":"https://www.researchgate.net/publication/280243098_Computing_transition_rates_for_the_1-D_stochastic_Ginzburg--Landau--Allen--Cahn_equation_for_finite-amplitude_noise_with_a_rare_event_algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we compute and analyse the transition rates and duration of\n <br> reactive trajectories of the stochastic 1-D Allen-Cahn equations for both the\n <br> Freidlin-Wentzell regime (weak noise or temperature limit) and finite-amplitude\n <br> white noise, as well as for small and large domain. We demonstrate that\n <br> extremely rare reactive trajectories corresponding to direct transitions\n <br> between two metastable states are efficiently computed using an algorithm\n <br> called adaptive multilevel splitting. This algorithm is dedicated to the\n <br> computation of rare events and is able to provide ensembles of reactive\n <br> trajectories in a very efficient way. In the small noise limit, our numerical\n <br> results are in agreement with large-deviation predictions such as\n <br> instanton-like solutions, mean first passages and escape probabilities. We show\n <br> that the duration of reactive trajectories follows a Gumbel distribution like\n <br> for one degree of freedom systems. Moreover, the mean duration growths\n <br> logarithmically with the inverse temperature. The prefactor given by the\n <br> potential curvature grows exponentially with size. The main novelty of our work\n <br> is that we also perform an analysis of reactive trajectories for large noises\n <br> and large domains. In this case, we show that the position of the reactive\n <br> front is essentially a random walk. This time, the mean duration grows linearly\n <br> with the inverse temperature and quadratically with the size. Using a\n <br> phenomenological description of the system, we are able to calculate the\n <br> transition rate, although the dynamics is described by neither\n <br> Freidlin--Wentzell or Eyring--Kramers type of results. Numerical results\n <br> confirm our analysis.\n</div> \n<p></p>"},{"id":775,"title":"Rare Event Simulation: A Point Process Interpretation With Application In Probability And Quantile Estimation","url":"https://www.researchgate.net/publication/285321983_Rare_Event_Simulation_A_Point_Process_Interpretation_With_Application_In_Probability_And_Quantile_Estimation","abstraction":"This paper addresses the issue of estimating extreme probability and quantile on the output of complex computer codes. We introduce a new approach to this problem in term of a random walk in the output space. This allows us to derive two main results: (1) the number of samples required to get a realisation of a random variable in a given domain of probability measure p is drastically reduced, following a Poisson law with parameter log 1/p; and (2) we get parallel algorithms for estimating probabilities and quantiles and especially the optimal parallel Multilevel Splitting algorithm where there is indeed no subset to define anymore."},{"id":776,"title":"MCMC algorithms for Subset Simulation","url":"https://www.researchgate.net/publication/279070817_MCMC_algorithms_for_Subset_Simulation","abstraction":"Subset Simulation is an adaptive simulation method that efficiently solves structural reliability problems with many random variables. The method requires sampling from conditional distributions, which is achieved through Markov Chain Monte Carlo (MCMC) algorithms. This paper discusses different MCMC algorithms proposed for Subset Simulation and introduces a novel approach for MCMC sampling in the standard normal space. Two variants of the algorithm are proposed: A basic variant, which is simpler than existing algorithms with equal accuracy and efficiency, and a more efficient variant with adaptive scaling. It is demonstrated that the proposed algorithm improves the accuracy of Subset Simulation, without the need for additional model evaluations."},{"id":777,"title":"Adaptive Multilevel Splitting in Molecular Dynamics Simulations","url":"https://www.researchgate.net/publication/276343589_Adaptive_Multilevel_Splitting_in_Molecular_Dynamics_Simulations","abstraction":"Adaptive Multilevel Splitting (AMS) is a replica-based rare event sampling method that has been used successfully in high-dimensional stochastic simulations to identify trajectories across a high potential barrier separating one metastable state from another, and to estimate the probability of observing such a trajectory. An attractive feature of AMS is that, in the limit of a large number of replicas, it remains valid regardless of the choice of reaction coordinate used to characterize the trajectories. Previous studies have shown AMS to be accurate in Monte Carlo simulations. In this study, we extend the application of AMS to molecular dynamics simulations and demonstrate its effectiveness using a simple test system. Our conclusion paves the way for useful applications, such as molecular dynamics calculations of the characteristic time of drug dissociation from a protein target."},{"id":778,"title":"Learning to be Selective in Genetic-Algorithm-Based Design Optimization","url":"https://www.researchgate.net/publication/2349484_Learning_to_be_Selective_in_Genetic-Algorithm-Based_Design_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we describe a method for improving genetic-algorithm-based optimization using search control. The idea is to utilize the sequence of points explored during a search to guide further exploration. The proposed method is particularly suitable for continuous spaces with expensive evaluation functions, such as arise in engineering design. Empirical results in several engineering design domains demonstrate that the proposed method can significantly improve the e#ciency and reliability of the GA optimizer. Keywords: genetic algorithms, design optimization, machine learning To appear in Artificial Intelligence in Engineering, Design, Analysis and Manufacturing 1 Introduction Genetic Algorithms (GAs) [ Goldberg 1989 ] are search algorithms that simulate the process of natural selection. GAs attempt to find a good solution to some problem (e.g., finding the maximum of a function) by randomly generating a collection (\"population \") of potential solutions (\"individuals\") to t...\n</div> \n<p></p>"},{"id":779,"title":"Multiobjective Function Optimization Using Nondominated Sorting Genetic Algorithms","url":"https://www.researchgate.net/publication/2333240_Multiobjective_Function_Optimization_Using_Nondominated_Sorting_Genetic_Algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In trying to solve multiobjective optimization problems, many traditional methods scalarize the objective vector into a single objective. In those cases, the obtained solution is highly sensitive to the weight vector used in the scalarization process and demands the user to have knowledge about the underlying problem. Moreover, in solving multiobjective problems, designers may be interested in a set of Pareto-optimal points, instead of a single point. Since genetic algorithms(GAs) work with a population of points, it seems natural to use GAs in multiobjective optimization problems to capture a number of solutions simultaneously. Although a vector evaluated GA (VEGA) has been implemented by Schaffer and has been tried to solve a number of multiobjective problems, the algorithm seems to have bias towards some regions. In this paper, we investigate Goldberg's notion of nondominated sorting in GAs along with a niche and speciation method to find multiple Pareto-optimal points sim...\n</div> \n<p></p>"},{"id":780,"title":"A New Method to Solve Generalized Multicriteria Optimization Problems using the Simple Genetic Algorithm","url":"https://www.researchgate.net/publication/226165021_A_New_Method_to_Solve_Generalized_Multicriteria_Optimization_Problems_using_the_Simple_Genetic_Algorithm","abstraction":"Genetic algorithms (GAs), which are directed stochastic hill climbing algorithms, are a commonly used optimization technique and are generally applied to single criterion optimization problems with fairly complex solution landscapes. There has been some attempts to apply GA to multicriteria optimization problems. The GA selection mechanism is typically dependent on a single-valued objective function and so no general methods to solve multicriteria optimization problems have been developed so far. In this paper, a new method of transformation of the multiple criteria problem into a single-criterion problem is presented. The problem of transformation brings about the need for the introduction of thePareto set estimation method to perform the multicriteria optimization using GAs. From a given solution set, which is the population of a certain generation of the GA, the Pareto set is found. The fitness of population members in the next GA generation is calculated by a distance metric with a reference to the Pareto set of the previous generation. As we are unable to combine the objectives in some way, we resort to this distance metric in the positive Pareto space of the previous solutions, as the fitness of the current solutions. This new GA-based multicriteria optimization method is proposed here, and it is capable of handling any generally formulated multicriteria optimization problem. The main idea of the method is described in detail in this paper along with a detailed numerical example. Preliminary computer generated results show that our approach produces better, and far more Pareto solutions, than plain stochastic optimization methods."},{"id":781,"title":"SPEA2: Improving the Strength Pareto Evolutionary Algorithm","url":"https://www.researchgate.net/publication/2386811_SPEA2_Improving_the_Strength_Pareto_Evolutionary_Algorithm","abstraction":"The Strength Pareto Evolutionary Algorithm (SPEA) (Zitzler and Thiele 1999) is a relatively recent technique for finding or approximating the Pareto-optimal set for multiobjective optimization problems. In different studies (Zitzler and Thiele 1999; Zitzler, Deb, and Thiele 2000) SPEA has shown very good performance in comparison to other multiobjective evolutionary algorithms, and therefore it has been a point of reference in various recent investigations, e.g., (Corne, Knowles, and Oates 2000). Furthermore, it has been used in different applications, e.g., (Lahanas, Milickovic, Baltas, and Zamboglou 2001). In this paper, an improved version, namely SPEA2, is proposed, which incorporates in contrast to its predecessor a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method. The comparison of SPEA2 with SPEA and two other modern elitist methods, PESA and NSGA-II, on different test problems yields promising results. 1"},{"id":782,"title":"The Pareto Envelop-based Selection Algorithm for Multi-Objective Optimization","url":"https://www.researchgate.net/publication/220701837_The_Pareto_Envelop-based_Selection_Algorithm_for_Multi-Objective_Optimization","abstraction":"We introduce a new multiobjective evolutionary algorithm called PESA (the Pareto Envelope-based Selection Algorithm), in which selection and diversity maintenance are controlled via a simple hyper-grid based scheme. PESA's selection method is relatively unusual in comparison with current well known multiobjective evolutionary algorithms, which tend to use counts based on the degree to which solutions dominate others in the population. The diversity maintenance method is similar to that used by certain other methods. The main attraction of PESA is the integration of selection and diversity maintenance, whereby essentially the same technique is used for both tasks. The resulting algorithm is simple to describe, with full pseudocode provided here and real code available from the authors. We compare PESA with two recent strong-performing MOEAs on some multiobjective test problems recently proposed by Deb. We find that PESA emerges as the best method overall on these problems."},{"id":783,"title":"Informed operators: Speeding up genetic-algorithm-based design optimization using reduced models","url":"https://www.researchgate.net/publication/2631230_Informed_operators_Speeding_up_genetic-algorithm-based_design_optimization_using_reduced_models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we describe a method for improving genetic-algorithm-based optimization using informed genetic operators. The idea is to make the genetic operators such as mutation and crossover more informed using reduced models. In every place where a random choice is made, for example when a point is mutated, instead of generating just one random mutation we generate several, rank them using a reduced model, then take the best to be the result of the mutation. The proposed method is particularly suitable for search spaces with expensive evaluation functions, such as arise in engineering design. Empirical results in several engineering design domains demonstrate that the proposed method can significantly speed up the GA optimizer. 1 Introduction This paper concerns the application of Genetic Algorithms (GAs) in realistic engineering design domains. In such domains a design is represented by a number of continuous design parameters, so that potential solutions are vec...\n</div> \n<p></p>"},{"id":784,"title":"Constrained Multiobjective Biogeography Optimization Algorithm","url":"https://www.researchgate.net/publication/263779142_Constrained_Multiobjective_Biogeography_Optimization_Algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Multiobjective optimization involves minimizing or maximizing multiple objective functions subject to a set of constraints. In this study, a novel constrained multiobjective biogeography optimization algorithm (CMBOA) is proposed. It is the first biogeography optimization algorithm for constrained multiobjective optimization. In CMBOA, a disturbance migration operator is designed to generate diverse feasible individuals in order to promote the diversity of individuals on Pareto front. Infeasible individuals nearby feasible region are evolved to feasibility by recombining with their nearest nondominated feasible individuals. The convergence of CMBOA is proved by using probability theory. The performance of CMBOA is evaluated on a set of 6 benchmark problems and experimental results show that the CMBOA performs better than or similar to the classical NSGA-II and IS-MOEA.\n</div> \n<p></p>"},{"id":785,"title":"Asynchronous Evolutionary Multi-Objective Algorithms with heterogeneous evaluation costs","url":"https://www.researchgate.net/publication/224246932_Asynchronous_Evolutionary_Multi-Objective_Algorithms_with_heterogeneous_evaluation_costs","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Master-slave parallelization of Evolutionary Algorithms (EAs) is straightforward, by distributing all fitness computations to slaves. The benefits of asynchronous steady state approaches are well-known when facing a possible heterogeneity among the evaluation costs in term of runtime, be they due to heterogeneous hardware or non-linear numerical simulations. However, when this heterogeneity depends on some characteristics of the individuals being evaluated, the search might be biased, and some regions of the search space poorly explored. Motivated by a real-world case study of multi-objective optimization problem the optimization of the combustion in a Diesel Engine the consequences of different components of heterogeneity in the evaluation costs on the convergence of two Evolutionary Multi-objective Optimization Algorithms are investigated on artificially-heterogeneous benchmark problems. In some cases, better spread of the population on the Pareto front seem to result from the interplay between the heterogeneity at hand and the evolutionary search.\n</div> \n<p></p>"},{"id":786,"title":"Multi-objective evolutionary simulation-optimisation of a real-world manufacturing problem","url":"https://www.researchgate.net/publication/229344627_Multi-objective_evolutionary_simulation-optimisation_of_a_real-world_manufacturing_problem","abstraction":"Many real-world manufacturing problems are too complex to be modelled analytically. For these problems, simulation can be a powerful tool for system analysis and optimisation. While traditional optimisation methods have been unable to cope with the complexities of many problems approached by simulation, evolutionary algorithms have proven to be highly useful. This paper describes how simulation and evolutionary algorithms have been combined to improve a manufacturing cell at Volvo Aero in Sweden. This cell produces high-technology engine components for civilian and military airplanes, and also for space rockets. Results from the study show that by using simulation and evolutionary algorithms, it is possible to increase the overall utilisation of the cell and at the same time decrease the number of overdue components."},{"id":787,"title":"A Parallel Surrogate-Assisted Multi-Objective Evolutionary Algorithm for Computationally Expensive Optimization Problems","url":"https://www.researchgate.net/publication/224330154_A_Parallel_Surrogate-Assisted_Multi-Objective_Evolutionary_Algorithm_for_Computationally_Expensive_Optimization_Problems","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a new efficient multi-objective evolutionary algorithm for solving computationally-intensive optimization problems. To support a high degree of parallelism, the algorithm is based on a steady-state design. For improved efficiency the algorithm utilizes a surrogate to identify promising candidate solutions and filter out poor ones. To handle the uncertainties associated with the approximative surrogate evaluations, a new method for multi-objective optimization is described which is generally applicable to all surrogate techniques. In this method, basically, surrogate objective values assigned to offspring are adjusted to consider the error of the surrogate. The algorithm is evaluated on the ZDT benchmark functions and on a real-world problem of manufacturing optimization. In assessing the performance of the algorithm, a new performance metric is suggested that combines convergence and diversity into one single measure. Results from both the benchmark experiments and the real-world test case indicate the potential of the proposed algorithm.\n</div> \n<p></p>"},{"id":788,"title":"Parallel Genetic Algorithm for Search and Constrained Multi-Objective Optimization.","url":"https://www.researchgate.net/publication/220949396_Parallel_Genetic_Algorithm_for_Search_and_Constrained_Multi-Objective_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Summary form only given. Parallel genetic algorithm for search and constrained multiobjective optimization introduces the design and complexity analysis of a parallel genetic algorithm to generate a \"best\" path for a robot arm to follow, given a starting position and a goal in three dimensional space. Path generation takes into account any obstacles near the arm. This algorithm uses multiple optimization criteria, independent cross-pollinating populations, and handles multiple hard constraints. Individuals in the population consist of multiple chromosomes. The complexity of the algorithm is the number of generations processed times O(N ) where N is the total number of individuals used for path generation on all of the optimizations.\n</div> \n<p></p>"},{"id":789,"title":"Using a genetic algorithm to optimize the gape of a snake jaw","url":"https://www.researchgate.net/publication/221001480_Using_a_genetic_algorithm_to_optimize_the_gape_of_a_snake_jaw","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n GA's have more success with optimizing a single configuration than with optimizing multiple configurations with connectivity constraints tying them together, i.e., variable geometry problems such as designing the variable geometry wings of a plane. This paper describes a GA based approach to solve variable geometry optimization problems where (a) the connectivity requirement cannot be easily specified or tested, (b) the space of configurations is made up of multiple disconnected spaces, thus making it likely that a GA would find sets of configurations that are not connected and (c) the cost of testing for connectivity, while examining each pair of configurations, is prohibitive. The approach has been tested and evaluated on a problem from computational biology, modeling the bones of a snake jaw.\n</div> \n<p></p>"},{"id":790,"title":"SEARCHING FOR PRESCRIPTIVE TREATMENT SCHEDULES WITH A GENETIC ALGORITHM: A TOOL FOR FOREST MANAGEMENT","url":"https://www.researchgate.net/publication/237630671_SEARCHING_FOR_PRESCRIPTIVE_TREATMENT_SCHEDULES_WITH_A_GENETIC_ALGORITHM_A_TOOL_FOR_FOREST_MANAGEMENT","abstraction":"This thesis describes research on the use of a genetic algorithm (GA) to prescribe treatment plans for forest management at the stand level. Forest management refers to making decisions about when and where to intervene in the natural growth of forests to achieve objectives, such as enhancing the visual quality of a stand or maximizing timber yield. A prescription is a schedule of thinning treatments applied to stands over a planning horizon. When multiple management goals exist treatment prescription becomes a complex multi- objective problem. The effectiveness of a GA depends on selecting an appropriate representation and germane fitness function. These design decisions are reviewed, followed by a series of experiments testing the performance of the GA. Different parameter settings are compared and the GA is contrasted with some other heuristic search methods. The final experiment compares a plan created by the GA to a plan recommended by a human expert."},{"id":791,"title":"EDITORIAL BOARDS, PUBLISHING COUNCIL","url":"https://www.researchgate.net/publication/242286565_EDITORIAL_BOARDS_PUBLISHING_COUNCIL","abstraction":"Informatica is a journal primarily covering the European com- puter science and informatics community; scientific and educa- tional as well as technical, commercial and industrial. Its basic aim is to enhance communications between different European structures on the basis of equal rights and international referee- ing. It publishes scientific papers accepted by at least two ref- erees outside the author's country. In addition, it contains in- formation about conferences, opinions, critical examinations of existing publications and news. Finally, major practical achieve- ments and innovations in the computer and information industry are presented through commercial publications as well as through independent evaluations. Editing and refereeing are distributed. Each editor from the Editorial Board can conduct the refereeing process by appointing two new referees or referees from the Board of Referees or Edi- torial Board. Referees should not be from the author's country. If new referees are appointed, their names will appear in the list of referees. Each paper bears the name of the editor who appointed the referees. Each editor can propose new members for the Edi- torial Board or referees. Editors and referees inactive for a longer period can be automatically replaced. Changes in the Editorial Board are confirmed by the Executive Editors. The coordination necessary is made through the Executive Edi- tors who examine the reviews, sort the accepted articles and main- tain appropriate international distribution. The Executive Board is appointed by the Society Informatika. Informatica is partially supported by the Slovenian Ministry of Science and Technology. Each author is guaranteed to receive the reviews of his article. When accepted, publication in Informatica is guaranteed in less than one year after the Executive Editors receive the corrected version of the article."},{"id":792,"title":"Evolutionary Balancing of Healthy Meals","url":"https://www.researchgate.net/publication/228854616_Evolutionary_Balancing_of_Healthy_Meals","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we present an evolutionary algorithm for solving the nutrition problem of composing and balancing healthy meals. We treat this problem as a single-objective and multiconstrained fractional knapsack problem that is easy to formulate, yet, its decision problem is in the class of NP-complete problems. In other words, some heuristic algorithm is required to provide good problem solutions in reasonable (polynomial) computational time. We applied a genetic algorithm and modified its parameters to yield high-quality and reliable solutions (healthy and balanced meals) that respect multiple weakly-correlated dietary recommendations and guidelines and include as much seasonal functional foods as possible. Functional foods contain physiologically active compounds that provide health benefits beyond their nutrient contributions. Povzetek: V ?lanku predstavljamo evolucijski algoritem za reševanje problema optimalne sestave jedilnika.\n</div> \n<p></p>"},{"id":793,"title":"Efficient Global Reliability Analysis for Nonlinear Implicit Performance Functions","url":"https://www.researchgate.net/publication/253769405_Efficient_Global_Reliability_Analysis_for_Nonlinear_Implicit_Performance_Functions","abstraction":"Many engineering applications are characterized by implicit response functions that are expensive to evaluate and sometimes nonlinear in their behavior, making reliability analysis difficult. This paper develops an efficient reliability analysis method that accurately characterizes the limit state throughout the random variable space. The method begins with a Gaussian process model built from a very small number of samples, and then adaptively chooses where to generate subsequent samples to ensure that the model is accurate in the vicinity of the limit state. The resulting Gaussian process model is then sampled using multimodal adaptive importance sampling to calculate the probability of exceeding (or failing to exceed) the response level of interest. By locating multiple points on or near the limit state, more complex and nonlinear limit states can be modeled, leading to more accurate probability integration. By concentrating the samples in the area where accuracy is important (i.e., in the vicinity of the limit state), only a small number of true function evaluations are required to build a quality surrogate model. The resulting method is both accurate for any arbitrarily shaped limit state and computationally efficient even for expensive response functions. This new method is applied to a collection of example problems including one that analyzes the reliability of a microelectromechanical system device that current available methods have difficulty solving either accurately or efficiently. Copyright © 2008 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved."},{"id":794,"title":"Corrected Kriging Update Formulae for Batch-Sequential Data Assimilation","url":"https://www.researchgate.net/publication/221966156_Corrected_Kriging_Update_Formulae_for_Batch-Sequential_Data_Assimilation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Recently, a lot of effort has been paid to the efficient computation of\n <br> Kriging predictors when observations are assimilated sequentially. In\n <br> particular, Kriging update formulae enabling significant computational savings\n <br> were derived in Barnes and Watson (1992), Gao et al. (1996), and Emery (2009).\n <br> Taking advantage of the previous Kriging mean and variance calculations helps\n <br> avoiding a costly $(n+1) \\times (n+1)$ matrix inversion when adding one\n <br> observation to the $n$ already available ones. In addition to traditional\n <br> update formulae taking into account a single new observation, Emery (2009) also\n <br> proposed formulae for the batch-sequential case, i.e. when $r &gt; 1$ new\n <br> observations are simultaneously assimilated. However, the Kriging variance and\n <br> covariance formulae given without proof in Emery (2009) for the\n <br> batch-sequential case are not correct. In this paper we fix this issue and\n <br> establish corrected expressions for updated Kriging variances and covariances\n <br> when assimilating several observations in parallel.\n</div> \n<p></p>"},{"id":795,"title":"Particle Learning of Gaussian Process Models for Sequential Design and Optimization","url":"https://www.researchgate.net/publication/45875211_Particle_Learning_of_Gaussian_Process_Models_for_Sequential_Design_and_Optimization","abstraction":"We develop a simulation-based method for the online updating of Gaussian process regression and classification models. Our method exploits sequential Monte Carlo to produce a fast sequential design algorithm for these models relative to the established MCMC alternative. The latter is less ideal for sequential design since it must be restarted and iterated to convergence with the inclusion of each new design point. We illustrate some attractive ensemble aspects of our SMC approach, and show how active learning heuristics may be implemented via particles to optimize a noisy function or to explore classification boundaries online. Comment: 18 pages, 5 figures, submitted"},{"id":796,"title":"Finding Near-Optimal Bayesian Experimental Designs via Genetic Algorithms","url":"https://www.researchgate.net/publication/4750207_Finding_Near-Optimal_Bayesian_Experimental_Designs_via_Genetic_Algorithms","abstraction":"This article shows how a genetic algorithm can be used to find near-optimal Bayesian experimental designs for regression models. The design criterion considered is the expected Shannon information gain of the posterior distribution obtained from performing a given experiment compared with the prior distribution. Genetic algorithms are described and then applied to experimental designs. The methodology is then illustrated with a wide range of examples: linear and nonlinear regression, single and multiple factors, and normal and Bernoulli distributed experimental data."},{"id":797,"title":"A Framework for Validation of Computer Models","url":"https://www.researchgate.net/publication/239667584_A_Framework_for_Validation_of_Computer_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we present a framework that enables computer model evaluation oriented towards answering the question: Does the computer model adequately represent reality? The proposed validation framework is a six-step procedure based upon a mix of Bayesian sta- tistical methodology and likelihood methodology. The methodology is particularly suited to treating the major issues associated with the validation process: quantifying multiple sources of error and uncertainty in computer models; combining multiple sources of information; and updating validation assessments as new information is acquired. Moreover, it allows inferential statements to be made about predictive error associated with model predictions in untested situations. The framework is illustrated on two test bed models (a pedagogic example and a resistance spot weld model) that provide context for each of the six steps in the proposed validation process.\n</div> \n<p></p>"},{"id":798,"title":"Bayesian Subset Simulation: a kriging-based subset simulation algorithm for the estimation of small probabilities of failure","url":"https://www.researchgate.net/publication/228445426_Bayesian_Subset_Simulation_a_kriging-based_subset_simulation_algorithmfor_the_estimation_of_small_probabilities_of_failure","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The estimation of small probabilities of failure from computer simulations is\n <br> a classical problem in engineering, and the Subset Simulation algorithm\n <br> proposed by Au &amp; Beck (Prob. Eng. Mech., 2001) has become one of the most\n <br> popular method to solve it. Subset simulation has been shown to provide\n <br> significant savings in the number of simulations to achieve a given accuracy of\n <br> estimation, with respect to many other Monte Carlo approaches. The number of\n <br> simulations remains still quite high however, and this method can be\n <br> impractical for applications where an expensive-to-evaluate computer model is\n <br> involved. We propose a new algorithm, called Bayesian Subset Simulation, that\n <br> takes the best from the Subset Simulation algorithm and from sequential\n <br> Bayesian methods based on kriging (also known as Gaussian process modeling).\n <br> The performance of this new algorithm is illustrated using a test case from the\n <br> literature. We are able to report promising results. In addition, we provide a\n <br> numerical study of the statistical properties of the estimator.\n</div> \n<p></p>"},{"id":799,"title":"Sequential Experiment Design for Contour Estimation From Complex Computer Codes","url":"https://www.researchgate.net/publication/228365548_Sequential_Experiment_Design_for_Contour_Estimation_From_Complex_Computer_Codes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Computer simulation is often used to study complex physical and engineering processes. While a computer simulator can often be viewed as an inexpensive way to gain insight into a system, it can still be computationally costly. Much of the recent work on the design and anal-ysis of computer experiments has focused on scenarios where the goal is to fit a response surface or process optimization. In this article, we develop sequential methodology for estimating a contour from a complex computer code. The approach uses a stochastic process model as a surrogate for the computer simulator. The surrogate model and associated uncertainty are key components in a new criterion used to identify the computer trials aimed specifically at improv-ing the contour estimate. The proposed approach is applied to exploration of a contour for a network queuing system. Issues related to practical implementation of the proposed approach are also addressed.\n</div> \n<p></p>"},{"id":800,"title":"Numerical Computation Of Multivariate Normal Probabilities","url":"https://www.researchgate.net/publication/2463953_Numerical_Computation_Of_Multivariate_Normal_Probabilities","abstraction":"The numerical computation of a multivariate normal probability is often a difficult problem. This article describes a transformation that simplifies the problem and places it into a form that allows efficient calculation using standard numerical multiple integration algorithms. Test results are presented that compare implementations of two algorithms that use the transformation, with currently available software. KEY WORDS: multivariate normal distribution, Monte-Carlo, adaptive integration. 1 Introduction A problem that arises in many statistics applications is that of computing the multivariate normal distribution function F (a; b) = 1 p jSigmaj(2) m Z b 1 a 1 Z b 2 a 2 ::: Z bm am e Gamma 1 2 ` t Sigma Gamma1 ` d`; where ` = (`1 ; `2 ; :::; `m) t and Sigma is an m Theta m symmetric positive definite covariance matrix. If, for some i, a i is Gamma1 and b i is 1, an appropriate transformation allows the ith variable to be integrated explicitly and reduc..."},{"id":801,"title":"A sequential Bayesian algorithm to estimate a probability of failure","url":"https://www.researchgate.net/publication/29609061_A_sequential_Bayesian_algorithm_to_estimate_a_probability_of_failure","abstraction":"This paper deals with the problem of estimating the probability of failure of a system, in the challenging case where only an expensive-to-simulate model is available. In this context, the budget for simulations is usually severely limited and therefore classical Monte~Carlo methods ought to be avoided. We present a new strategy to address this problem, in the framework of sequential Bayesian planning. The method uses kriging to compute an approximation of the probability of failure, and selects the next simulation to be conducted so as to reduce the mean square error of estimation. By way of illustration, we estimate the probability of failure of a control strategy in the presence of uncertainty about the parameters of the plant."},{"id":802,"title":"Asymptotic analysis of covariance parameter estimation for Gaussian processes in the misspecified case","url":"https://www.researchgate.net/publication/269280638_Asymptotic_analysis_of_covariance_parameter_estimation_for_Gaussian_processes_in_the_misspecified_case","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In parametric estimation of covariance function of Gaussian processes, it is\n <br> often the case that the true covariance function does not belong to the\n <br> parametric set used for estimation. This situation is called the misspecified\n <br> case. In this case, it has been observed that, for irregular spatial sampling\n <br> of observation points, Cross Validation can yield smaller prediction errors\n <br> than Maximum Likelihood. Motivated by this comparison, we provide a general\n <br> asymptotic analysis of the misspecified case, for independent observation\n <br> points with uniform distribution. We prove that the Maximum Likelihood\n <br> estimator asymptotically minimizes a Kullback-Leibler divergence, within the\n <br> misspecified parametric set, while Cross Validation asymptotically minimizes\n <br> the integrated square prediction error. In a Monte Carlo simulation, we show\n <br> that the covariance parameters estimated by Maximum Likelihood and Cross\n <br> Validation, and the corresponding Kullback-Leibler divergences and integrated\n <br> square prediction errors, can be strongly contrasting. On a more technical\n <br> level, we provide new increasing-domain asymptotic results for the situation\n <br> where the eigenvalues of the covariance matrices involved are not upper\n <br> bounded.\n</div> \n<p></p>"},{"id":803,"title":"Estimating and Quantifying Uncertainties on Level Sets Using the Vorob’ev Expectation and Deviation with Gaussian Process Models","url":"https://www.researchgate.net/publication/284330223_Estimating_and_Quantifying_Uncertainties_on_Level_Sets_Using_the_Vorob%27ev_Expectation_and_Deviation_with_Gaussian_Process_Models","abstraction":"Several methods based on Kriging have recently been proposed for calculating a probability of failure involving costly-to-evaluate functions. A closely related problem is to estimate the set of inputs leading to a response exceeding a given threshold. Now, estimating such a level set—and not solely its volume—and quantifying uncertainties on it are not straightforward. Here we use notions from random set theory to obtain an estimate of the level set, together with a quantification of estimation uncertainty. We give explicit formulae in the Gaussian process set-up and provide a consistency result. We then illustrate how space-filling versus adaptive design strategies may sequentially reduce level set estimation uncertainty."},{"id":804,"title":"Construction and Efficient Implementation of Adaptive Objective-Based Designs of Experiments","url":"https://www.researchgate.net/publication/272040385_Construction_and_Efficient_Implementation_of_Adaptive_Objective-Based_Designs_of_Experiments","abstraction":"This work is devoted to the development of a new and efficient procedure for the construction of adaptive model-based designs of experiments. This work couples kriging theory with design of experiments optimization techniques and its originality relies on two main ingredients: (i) the definition of a general criterion in the objective function that allows one to take into account the analyst’s choices in order to explore critical regions, and (ii) an efficient numerical implementation including a stabilization step to avoid numerical problems due to kriging matrix inversion and a computational cost reduction strategy that allows for the model’s use in industrial applications. After a full description of these key points, the resulting efficient algorithm is applied to extend over a territory in France a network of probes for environmental monitoring. This study leads to a final design of experiments where the new probes are located in undersampled regions of high population density. Moreover, it can be performed with an affordable computational cost, which is not the case with classical approaches based on an optimization over the whole domain."},{"id":805,"title":"Multiobjective optimization using Gaussian process emulators via stepwise uncertainty reduction","url":"https://www.researchgate.net/publication/257299288_Multiobjective_optimization_using_Gaussian_process_emulators_via_stepwise_uncertainty_reduction","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Optimization of expensive computer models with the help of Gaussian process\n <br> emulators in now commonplace. However, when several (competing) objectives are\n <br> considered, choosing an appropriate sampling strategy remains an open question.\n <br> We present here a new algorithm based on stepwise uncertainty reduction\n <br> principles to address this issue. Optimization is seen as a sequential\n <br> reduction of the volume of the excursion sets below the current best solutions,\n <br> and our sampling strategy chooses the points that give the highest expected\n <br> reduction. Closed-form formulae are provided to compute the sampling criterion,\n <br> avoiding the use of cumbersome simulations. We test our method on numerical\n <br> examples, showing that it provides an efficient trade-off between exploration\n <br> and intensification.\n</div> \n<p></p>"},{"id":806,"title":"Text Classification from Labeled and Unlabeled Documents using EM","url":"https://www.researchgate.net/publication/226507886_Text_Classification_from_Labeled_and_Unlabeled_Documents_using_EM","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.\n</div> \n<p></p>"},{"id":807,"title":"Latent Semantic Indexing: A Probabilistic Analysis","url":"https://www.researchgate.net/publication/222553452_Latent_Semantic_Indexing_A_Probabilistic_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Latent semantic indexing (LSI) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofore been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering.\n</div> \n<p></p>"},{"id":808,"title":"An Experimental Comparison of Model-Based Clustering Methods","url":"https://www.researchgate.net/publication/233489350_An_Experimental_Comparison_of_Model-Based_Clustering_Methods","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We compare the three basic algorithms for model-based clustering on high-dimensional discrete-variable datasets. All three algorithms use the same underlying model: a naive-Bayes model with a hidden root node, also known as a multinomial-mixture model. In the first part of the paper, we perform an experimental comparison between three batch algorithms that learn the parameters of this model: the Expectation?Maximization (EM) algorithm, a ?winner take all? version of the EM algorithm reminiscent of the K-means algorithm, and model-based agglomerative clustering. We find that the EM algorithm significantly outperforms the other methods, and proceed to investigate the effect of various initialization methods on the final solution produced by the EM algorithm. The initializations that we consider are (1) parameters sampled from an uninformative prior, (2) random perturbations of the marginal distribution of the data, and (3) the output of agglomerative clustering. Although the methods are substantially different, they lead to learned models that are similar in quality.\n</div> \n<p></p>"},{"id":809,"title":"Inside Out: Two Jointly Predictive Models for Word Representations and Phrase Representations","url":"https://www.researchgate.net/publication/287206748_Inside_Out_Two_Jointly_Predictive_Models_for_Word_Representations_and_Phrase_Representations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Distributional hypothesis lies in the root of most existing word representation models by inferring word meaning from its external contexts. However, distribu-tional models cannot handle rare and morphologically complex words very well and fail to identify some fine-grained linguistic regularity as they are ignoring the word forms. On the contrary, morphology points out that words are built from some basic units, i.e., morphemes. Therefore, the meaning and function of such rare words can be inferred from the words sharing the same morphemes, and many syntactic relations can be directly identified based on the word forms. However, the limitation of morphology is that it cannot infer the relationship between two words that do not share any morphemes. Considering the advantages and limitations of both approaches, we propose two novel models to build better word representations by modeling both external contexts and internal morphemes in a jointly pre-dictive way, called BEING and SEING. These two models can also be extended to learn phrase representations according to the distributed morphology theory. We evaluate the proposed models on similarity tasks and analogy tasks. The results demonstrate that the proposed models can outperform state-of-the-art models significantly on both word and phrase representation learning.\n</div> \n<p></p>"},{"id":810,"title":"Weighted Multi-label Classification Model for Sentiment Analysis of Online News","url":"https://www.researchgate.net/publication/290911815_Weighted_Multi-label_Classification_Model_for_Sentiment_Analysis_of_Online_News","abstraction":"With the extensive growth of social media services, many users express their feelings and opinions through news articles, blogs and tweets/microblogs. To discover the connections between emotions evoked in a user by varied-scale documents effectively, the paper is concerned with the problem of sentiment analysis over online news. Different from previous models which treat training documents uniformly, a weighted multi-label classification model (WMCM) is proposed by introducing the concept of \" emotional concentration \" to estimate the weight of training documents, in addition to tackle the issue of noisy samples for each emotion. The topic assignment is also used to distinguish different emotional senses of the same word at the semantic level. Experimental evaluations using short news headlines and long documents validate the effectiveness of the proposed WMCM for sentiment prediction."},{"id":811,"title":"Empirical Measure of Learnability: A Tool for Semantic Map Validation","url":"https://www.researchgate.net/publication/289991476_Empirical_Measure_of_Learnability_A_Tool_for_Semantic_Map_Validation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The many approaches to semantic mapping developed recently demand a precise measuring device that would, on the one hand, be sensitive to human subjective experiences (and therefore must involve a human in the loop), and on the other hand, allow comparative study and validation of consistency of individual semantic maps. The idea explored in this work is to measure the ability of a human subject to learn a given semantic map, and in this sense to be able to “make sense” of the map, as estimated based on a given set of test words. The paradigm includes allocating previously unseen test words in the map coordinates. The quantitative measure is the Pearson's correlation between actual map coordinates of test words and coordinates assigned by subjects. The preliminary study indicates that the proposed measure is sufficiently sensitive to discriminate individual semantic maps from each other and to rank them by their learnability, related to their internal consistency. Potential applications include evaluation of methods for automated semantic map construction, as well as diagnostics of semantic dementia, affective and personality disorders.\n</div> \n<p></p>"},{"id":812,"title":"Joint Image-Text News Topic Detection and Tracking with And-Or Graph Representation","url":"https://www.researchgate.net/publication/287250237_Joint_Image-Text_News_Topic_Detection_and_Tracking_with_And-Or_Graph_Representation","abstraction":"In this paper, we aim to develop a method for automatically detecting and tracking topics in broadcast news. We present a hierarchical And-Or graph (AOG) to jointly represent the latent structure of both texts and visuals. The AOG embeds a context sensitive grammar that can describe the hierarchical composition of news topics by semantic elements about people involved, related places and what happened, and model contextual relationships between elements in the hierarchy. We detect news topics through a cluster sampling process which groups stories about closely related events. Swendsen-Wang Cuts (SWC), an effective cluster sampling algorithm, is adopted for traversing the solution space and obtaining optimal clustering solutions by maximizing a Bayesian posterior probability. Topics are tracked to deal with the continuously updated news streams. We generate topic trajectories to show how topics emerge, evolve and disappear over time. The experimental results show that our method can explicitly describe the textual and visual data in news videos and produce meaningful topic trajectories. Our method achieves superior performance compared to state-of-the-art methods on both a public dataset Reuters-21578 and a self-collected dataset named UCLA Broadcast News Dataset."},{"id":813,"title":"From Distributional Semantics to Conceptual Spaces: A Novel Computational Method for Concept Creation","url":"https://www.researchgate.net/publication/291339792_From_Distributional_Semantics_to_Conceptual_Spaces_A_Novel_Computational_Method_for_Concept_Creation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We investigate the relationship between lexical spaces and contextually-defined conceptual spaces, offering applications to creative concept discovery. We define a computational method for discov- ering members of concepts based on semantic spaces: starting with a standard distributional model derived from corpus co-occurrence statistics, we dynamically select characteristic dimensions associated with seed terms, and thus a subspace of terms defining the related concept. This approach performs as well as, and in some cases better than, leading distributional semantic models on a WordNet-based concept discovery task, while also providing a model of concepts as convex regions within a space with interpretable dimensions. In particular, it performs well on more specific, contextualized concepts; to investigate this we therefore move beyond WordNet to a set of human empirical studies, in which we compare output against human responses on a membership task for novel concepts. Finally, a separate panel of judges rate both model output and human responses, showing similar ratings in many cases, and some commonalities and divergences which reveal interesting issues for computational concept discovery.\n</div> \n<p></p>"},{"id":814,"title":"Visual Place Recognition: A Survey","url":"https://www.researchgate.net/publication/284754832_Visual_Place_Recognition_A_Survey","abstraction":"Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines—particularly recognition in computer vision and animal navigation in neuroscience—have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape. We start by introducing the concepts behind place recognition—the role of place recognition in the animal kingdom, how a “place” is defined in a robotics context, and the major components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description."},{"id":815,"title":"Object-Oriented Semisupervised Classification of VHR Images by Combining MedLDA and a Bilateral Filter","url":"https://www.researchgate.net/publication/284141373_Object-Oriented_Semisupervised_Classification_of_VHR_Images_by_Combining_MedLDA_and_a_Bilateral_Filter","abstraction":"A Bayesian hierarchical model is presented to classify very high resolution (VHR) images in a semisupervised manner, in which both a maximum entropy discrimination latent Dirichlet allocation (MedLDA) and a bilateral filter are combined into a novel application framework. The primary contribution of this paper is to nullify the disadvantages of traditional probabilistic topic models on pixel-level supervised information and to achieve the effective classification of VHR remote sensing images. This framework consists of the following two iterative steps. In the training stage, the model utilizes the central labeled pixel and its neighborhood, as a squared labeled image object, to train the classifiers. In the classification stage, each central unlabeled pixel with its neighborhood, as an unlabeled object, is classified as a user-provided geoobject class label with the maximum posterior probability. Gibbs sampling is adopted for model inference. The experimental results demonstrate that the proposed method outperforms two classical SVM-based supervised classification methods and probabilistic-topic-models-based classification methods."},{"id":816,"title":"Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary","url":"https://www.researchgate.net/publication/225741505_Object_Recognition_as_Machine_Translation_Learning_a_Lexicon_for_a_Fixed_Image_Vocabulary","abstraction":"We describe a model of object recognition as machine translation. In this model, recognition is a process of annotating image regions with words. Firstly, images are segmented into regions, which are classified into region types using a variety of features. A mapping between region types and keywords supplied with the images, is then learned, using a method based around EM. This process is analogous with learning a lexicon from an aligned bitext. For the implementation we describe, these words are nouns taken from a large vocabulary. On a large test set, the method can predict numerous words with high accuracy. Simple methods identify words that cannot be predicted well. We show how to cluster words that individually are difficult to predict into clusters that can be predicted well — for example, we cannot predict the distinction between train and locomotive using the current set of features, but we can predict the underlying concept. The method is trained on a substantial collection of images. Extensive experimental results illustrate the strengths and weaknesses of the approach."},{"id":817,"title":"Relevance-based language models. SIGIR '01","url":"https://www.researchgate.net/publication/221299786_Relevance-based_language_models_SIGIR_%2701","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate arelevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data\n</div> \n<p></p>"},{"id":818,"title":"\"Normalized cuts and image segmentation\", IEEE Trans","url":"https://www.researchgate.net/publication/2477494_Normalized_cuts_and_image_segmentation_IEEE_Trans","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images and found results very en- couraging.\n</div> \n<p></p>"},{"id":819,"title":"Document Language Models, Query Models, and Risk Minimization for Information Retrieval","url":"https://www.researchgate.net/publication/2534026_Document_Language_Models_Query_Models_and_Risk_Minimization_for_Information_Retrieval","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present a framework for information retrieval that combines document models and query models using a probabilistic ranking function based on Bayesian decision theory. The framework suggests an operational retrieval model that extends recent developments in the language modeling approach to information retrieval. A language model for each document is estimated, as well as a language model for each query, and the retrieval problem is cast in terms of risk minimization. The query language model can be exploited to model user preferences, the context of a query, synonomy and word senses. While recent work has incorporated word translation models for this purpose, we introduce a new method using Markov chains defined on a set of documents to estimate the query models. The Markov chain method has connections to algorithms from link analysis and social networks. The new approach is evaluated on TREC collections and compared to the basic language modeling approach and vector space models together with query expansion using Rocchio. Significant improvements are obtained over standard query expansion methods for strong baseline TF-IDF systems, with the greatest improvements attained for short queries on Web data.\n</div> \n<p></p>"},{"id":820,"title":"Modeling Annotated Data","url":"https://www.researchgate.net/publication/2950087_Modeling_Annotated_Data","abstraction":"We consider the problem of modeling annotated data---data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models which aim to describe such data, culminating in correspondence latent Dirichlet allocation, a latent variable model that is e#ective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We conduct experiments on the Corel database of images and captions, assessing performance in terms of held-out likelihood, automatic annotation, and text-based image retrieval."},{"id":821,"title":"Vision Texture for Annotation.","url":"https://www.researchgate.net/publication/220460778_Vision_Texture_for_Annotation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper demonstrates a new application of computer vision to digital libraries — the use of texture forannotation, the description of content. Vision-based annotation assists the user in attaching descriptions to large sets of images and video. If a user labels a piece of an image aswater, a texture model can be used to propagate this label to other “visually similar” regions. However, a serious problem is that no single model has been found that is good enough to match reliably human perception of similarity in pictures. Rather than using one model, the system described here knows several texture models, and is equipped with the ability to choose the one that “best explains” the regions selected by the user for annotating. If none of these models suffices, then it creates new explanations by combining models. Examples of annotations propagated by the system on natural scenes are given. The system provides an average gain of four to one in label prediction for a set of 98 images.\n</div> \n<p></p>"},{"id":822,"title":"Relevance-Based Language Models: Estimation and Analysis","url":"https://www.researchgate.net/publication/2373876_Relevance-Based_Language_Models_Estimation_and_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate a relevance model with no training data. We propose a novel technique for estimating such models using the query alone. We demonstrate that our technique can produce highly accurate relevance models. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval. The main contribution of this work is an effective formal method for estimating a relevance model with no training data. 1\n</div> \n<p></p>"},{"id":823,"title":"Blobworld: A System for Region-Based Image Indexing and Retrieval","url":"https://www.researchgate.net/publication/2767517_Blobworld_A_System_for_Region-Based_Image_Indexing_and_Retrieval","abstraction":". Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. The image is segmented into regions by fitting a mixture of Gaussians to the pixel distribution in a joint color-texture-position feature space. Each region (\"blob\") is then associated with color and texture descriptors. Querying is based on the user specifying attributes of one or two regions of interest, rather than a description of the entire image. In order to make largescale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing. 1 Introduction From a user's point of view, an information retrieval system can be measured by the quality and speed with which it answers the user's information need. Several factors contribute to overall ..."},{"id":824,"title":"Robust active learning for the diagnosis of parasites","url":"https://www.researchgate.net/publication/277979149_Robust_active_learning_for_the_diagnosis_of_parasites","abstraction":"We have developed an automated system for the diagnosis of intestinal parasites from optical microscopy images. The objects (species of parasites and impurities) segmented from these images form a large dataset. We are interested in the active learning problem of selecting a reasonably small number of objects to be labeled under an expert's supervision for use in training a pattern classifier. However, impurities are very numerous, constitute several clusters in the feature space, and can be quite similar to some species of parasites, leading to a significant challenge for active learning methods. We propose a technique that pre-organizes the data and then properly balances the selection of samples from all classes and uncertain samples for training. Early data organization avoids reprocessing of the large dataset at each learning iteration, enabling the halting of sample selection after a desired number of samples per iteration, yielding interactive response time. We validate our method by comparing it with state-of-the-art approaches, using a previously labeled dataset of almost 6,000 objects. Moreover, we report results from experiments on a very realistic scenario, consisting of a dataset with over 140,000 unlabeled objects, under unbalanced classes, absence of some classes, and the presence of a very large set of impurities."},{"id":825,"title":"A Multi Criteria Decision Making Based Approach for Semantic Image Annotation","url":"https://www.researchgate.net/publication/275347841_A_Multi_Criteria_Decision_Making_Based_Approach_for_Semantic_Image_Annotation","abstraction":"Automatic image annotation has emerged as an important research topic due to its potential application on both image understanding and web image search. This paper presents a model, which integrates visual topics and regional contexts to automatic image annotation. Regional contexts model the relationship between the regions, while visual topics provide the global distribution of topics over an image. Previous image annotation methods neglected the relationship between the regions in an image, while these regions are exactly explanation of the image semantics, therefore considering the relationship between them are helpful to annotate the images. Regional contexts and visual topics are learned by PLSA (Probability Latent Semantic Analysis) from the training data. The proposed model incorporates these two types of information by MCDM (Multi Criteria Decision Making) approach based on WSM (Weighted Sum Method). Experiments conducted on the 5k Corel dataset demonstrate the effectiveness of the proposed model."},{"id":826,"title":"Automatic Image Tagging Model Based on Multigrid Image Segmentation and Object Recognition","url":"https://www.researchgate.net/publication/286547241_Automatic_Image_Tagging_Model_Based_on_Multigrid_Image_Segmentation_and_Object_Recognition","abstraction":"Since rapid growth of Internet technologies and mobile devices, multimedia data such as images and videos are explosively growing on the Internet. Managing large scale multimedia data with correct tags and annotations is very important task. Incorrect tags and annotations make it hard to manage multimedia data. Accurate tags and annotation ease management of multimedia data and give high quality retrieve results. Fully manual image tagging which is tagged by user will be most accurate tags when the user tags correct information. Nevertheless, most of users do not make effort on task of tagging. Therefore, we suffer from lots of noisy tags. Best solution for accurate image tagging is to tag image automatically. Robust automatic image tagging models are proposed by many researchers and it is still most interesting research field these days. Since there are still lots of limitations in automatic image tagging models, we propose efficient automatic image tagging model using multigrid based image segmentation and feature extraction method. Our model can improve the object descriptions of images and image regions. Our method is tested with Corel dataset and the result showed that our model performance is efficient and effective compared to other models."},{"id":827,"title":"Unsupervised Image Classification by Probabilistic Latent Semantic Analysis for the Annotation of Images","url":"https://www.researchgate.net/publication/268812008_Unsupervised_Image_Classification_by_Probabilistic_Latent_Semantic_Analysis_for_the_Annotation_of_Images","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Image annotation has been identified to be a suitable means by which the semantic gap which has made the accuracy of Content-based image retrieval unsatisfactory be eliminated. However existing methods of automatic annotation of images depends on supervised learning, which can be difficult to implement due to the need for manually annotated training samples which are not always readily available. This paper argues that the unsupervised learning via Probabilistic Latent Semantic Analysis provides a more suitable machine learning approach for image annotation especially due to its potential to based categorisation on the latent semantic content of the image samples, which can bridge the semantic gap present in Content Based Image Retrieval. This paper therefore proposes an unsupervised image categorisation model in which the semantic content of images are discovered using Probabilistic Latent Semantic Analysis, after which they are clustered into unique groups based on semantic content similarities using K-means algorithm, thereby providing suitable annotation exemplars. A common problem with categorisation algorithms based on Bag-of-Visual Words modelling is the loss of accuracy due to spatial incoherency of the Bag-of-Visual Word modelling, this paper also examines the effectiveness of Spatial pyramid as a means of eliminating spatial incoherency in Probabilistic Latent Semantic Analysis classification.\n</div> \n<p></p>"},{"id":828,"title":"Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval","url":"https://www.researchgate.net/publication/266147482_Heterogeneous_Metric_Learning_with_Content-Based_Regularization_for_Software_Artifact_Retrieval","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The problem of software artifact retrieval has the goal to effectively locate\n <br> software artifacts, such as a piece of source code, in a large code repository.\n <br> This problem has been traditionally addressed through the textual query. In\n <br> other words, information retrieval techniques will be exploited based on the\n <br> textual similarity between queries and textual representation of software\n <br> artifacts, which is generated by collecting words from comments, identifiers,\n <br> and descriptions of programs. However, in addition to these semantic\n <br> information, there are rich information embedded in source codes themselves.\n <br> These source codes, if analyzed properly, can be a rich source for enhancing\n <br> the efforts of software artifact retrieval. To this end, in this paper, we\n <br> develop a feature extraction method on source codes. Specifically, this method\n <br> can capture both the inherent information in the source codes and the semantic\n <br> information hidden in the comments, descriptions, and identifiers of the source\n <br> codes. Moreover, we design a heterogeneous metric learning approach, which\n <br> allows to integrate code features and text features into the same latent\n <br> semantic space. This, in turn, can help to measure the artifact similarity by\n <br> exploiting the joint power of both code and text features. Finally, extensive\n <br> experiments on real-world data show that the proposed method can help to\n <br> improve the performances of software artifact retrieval with a significant\n <br> margin.\n</div> \n<p></p>"},{"id":829,"title":"Up next: Retrieval methods for large scale related video suggestion","url":"https://www.researchgate.net/publication/266660230_Up_next_Retrieval_methods_for_large_scale_related_video_suggestion","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The explosive growth in sharing and consumption of the video content on the web creates a unique opportunity for scientific advances in video retrieval, recommendation and discovery. In this paper, we focus on the task of video suggestion, commonly found in many online applications. The current state-of-the-art video suggestion techniques are based on the collaborative filtering analysis, and suggest videos that are likely to be co-viewed with the watched video. In this paper, we propose augmenting the collaborative filtering analysis with the topical representation of the video content to suggest related videos. We propose two novel methods for topical video representation. The first method uses information retrieval heuristics such as tf-idf, while the second method learns the optimal topical representations based on the implicit user feedback available in the online scenario. We conduct a large scale live experiment on YouTube traffic, and demonstrate that augmenting collaborative filtering with topical representations significantly improves the quality of the related video suggestions in a live setting, especially for categories with fresh and topically-rich video content such as news videos. In addition, we show that employing user feedback for learning the optimal topical video representations can increase the user engagement by more than 80% over the standard information retrieval representation, when compared to the collaborative filtering baseline.\n</div> \n<p></p>"},{"id":830,"title":"Ontology based Classification for Multi-label Image Annotation","url":"https://www.researchgate.net/publication/265215529_Ontology_based_Classification_for_Multi-label_Image_Annotation","abstraction":"Image annotation has been an important task for visual information retrieval. It usually involves a multi-class multi-label classification problem. To solve this problem, many researches have been conducted during last two decades, although most of the proposed methods rely on the training data with the ground truth. To prepare such a ground truth is an expensive and laborious task that cannot be easily scaled, and \"semantic gaps\" between low-level visual features and high-level semantics still remain. In this paper, we propose a novel approach, ontology based supervised learning for multi-label image annotation, where classifiers' training is conducted using easily gathered Web data. Moreover, it takes advantage of both low-level visual features and high-level semantic information of given images. Experimental re-sults using 0.507 million Web images database show effectiveness of the proposed framework over existing method."},{"id":831,"title":"A Hybrid Model for Automatic Image Annotation","url":"https://www.researchgate.net/publication/261960509_A_Hybrid_Model_for_Automatic_Image_Annotation","abstraction":"In this work, we present a hybrid model (SVM-DMBRM) combining a generative and a discriminative model for the image annotation task. A support vector machine (SVM) is used as the discriminative model and a Discrete Multiple Bernoulli Relevance Model (DMBRM) is used as the generative model. The idea of combining both the models is to take advantage of the distinct capabilities of each model. The SVM tries to address the problem of poor annotation (images are not annotated with all relevant keywords), while the DMBRM model tries to address the problem of data imbalance (large variations in number of positive samples). Since DMBRM does not work well with high-dimensional data, a Latent Dirichlet Allocation (LDA) model is used to reduce the dimensionality of vector quantized features before using it. The hybrid model's results are comparable to or better than the state-of-the-art results on three standard datasets: Corel-5k, ESP-Game and IAPRTC-12."},{"id":832,"title":"Automatic Image Annotation Based on Co-training","url":"https://www.researchgate.net/publication/274988240_Automatic_Image_Annotation_Based_on_Co-training","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Automatic image annotation is a critical and challenging problem in pattern recognition and image understanding areas. There are some problems in existing automatic image annotation areas. For example, the size of unlabeled data is much larger than the labeled data. Besides, most image annotation models can only use one kind of image segmentation strategy and certain image description method. According to the above problems, an automatic image annotation model based on Co-training is proposed. In this model, four independent feature properties are constructed and then four corresponding sub-classifers are built. In this way, different image segmentation strategies and feature representation methods can be integrated into a unified framework. An adaptive algorithm based on vote and consistency is proposed to extend the training dataset. The proposed method use Co-training algorithm and mass unlabeled data to improve the performance of automatic image annotation. Experiments conducted on Corel 5 K dataset verify the effectiveness of proposed method.\n</div> \n<p></p>"},{"id":833,"title":"Building Friend Wall for Local Photo Repository by Using Social Attribute Annotation","url":"https://www.researchgate.net/publication/260184574_Building_Friend_Wall_for_Local_Photo_Repository_by_Using_Social_Attribute_Annotation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we focus on providing a novel image browsing and visualization experience for local photo repository. The so-called Friend Wall system solves two problems: (1) How to effectively classify the local images with respect to related social characters and events. (2) How to efficiently generate layout to compactly arrange many photos onto a single canvas. For the first problem, we propose a novel image annotation scheme by employing both of the image visual features and Metadata. Motivated by the observation that SNS (Social Networking Service) images, especially those come from the user’s acquaintances always contain rich information, we apply these images as our training set and explore their social attributes. In our definition, social attributes contain a set of intrinsic labels such as Who, When, Where, and What. To effectively arrange the photos on a single canvas, we proposed a binary tree based representation and fast algorithm for layout generation. Experiments show the examples of Friend Wall. The effectiveness of social attribute annotation is proved as well.\n</div> \n<p></p>"},{"id":834,"title":"Clustering art","url":"https://www.researchgate.net/publication/3940729_Clustering_art","abstraction":"We extend a recently developed method (K. Barnard and D. Forsyth, 2001) for learning the semantics of image databases using text and pictures. We incorporate statistical natural language processing in order to deal with free text. We demonstrate the current system on a difficult dataset, namely 10000 images of work from the Fine Arts Museum of San Francisco. The images include line drawings, paintings, and pictures of sculpture and ceramics. Many of the images have associated free text which varies greatly from physical description to interpretation and mood. We use WordNet to provide semantic grouping information and to help disambiguate word senses, as well as emphasize the hierarchical nature of semantic relationships. This allows us to impose a natural structure on the image collection that reflects semantics to a considerable degree. Our method produces a joint probability distribution for words and picture elements. We demonstrate that this distribution can be used: (a) to provide illustrations for given captions, and (b) to generate words for images outside the training set. Results from this annotation process yield a quantitative study of our method. Finally, the annotation process can be seen as a form of object recognizer that has been learned through a partially supervised process."},{"id":835,"title":"Browse and Search Patterns in a Digital Image Database","url":"https://www.researchgate.net/publication/30846745_Browse_and_Search_Patterns_in_a_Digital_Image_Database","abstraction":"A prototype image retrieval system with browse and search capabilities was developed to investigate patterns of searching a collection of digital visual images, as well as factors, such as image size, resolution, and download speed, which affect browsing. The subject populations were art history specialists and non-specialists. Through focus group interviews, a controlled test, post-test interviews and an online survey, data was gathered to compare preferences and actual patterns of use in browsing and searching. While specialists preferred direct search to browsing, and generalists used browsing as their preferred mode, both user groups found each mode to play a role depending on information need, and found value in a system combining both browse and direct search. There were no significant differences in performance among the search modes of browse, search, and combined browse/search models when the quasi-controlled study tested the different modes. Peer Reviewed http://deepblue.lib.umich.edu/bitstream/2027.42/45984/1/10791_2004_Article_252664.pdf"},{"id":836,"title":"End-User Searching Challenges Indexing Practices in the Digital Newspaper Photo Archive","url":"https://www.researchgate.net/publication/2542467_End-User_Searching_Challenges_Indexing_Practices_in_the_Digital_Newspaper_Photo_Archive","abstraction":"Previous research in conceptual indexing methods of images has furnished us with refined theoretical frameworks characterising various aspects of images that could and should be indexed using textual descriptors. The development of digital image processing technologies has bred a brigade of content-based indexing and retrieval methods available for applications. What the users need and in what kinds of environments different indexing and retrieval methods are relevant, has remained an area of less intensive research work. This article presents the results of a field study concentrating on journalists as users of a digital newspaper photo archive. The expressed photo needs, applied selection criteria and observed searching behaviours in journalists' daily work were contrasted with the indexing practices applied by the archivists. The results showed that the journalists achieved satisfactory results when trivial query terms were available, e.g. when photos of named persons were needed. Browsing was the main searching strategy applied by the journalists, but the system did not support browsing well. The access problems faced by the users in particular photo needs are discussed in detail. The paper concludes by discussing the potential approaches in developing both the concept-based and content-based indexing methods as well as the user interfaces in photo retrieval systems."},{"id":837,"title":"Blobworld: Image segmentation using Expectation-Maximization and its application to image querying","url":"https://www.researchgate.net/publication/2336483_Blobworld_Image_segmentation_using_Expectation-Maximization_and_its_application_to_image_querying","abstraction":"Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation which provides a transformation from the raw pixel data to a small set of image regions which are coherent in color and texture. This \"Blobworld\" representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for adjusting the similarity metrics. By finding image regions w..."},{"id":838,"title":"Hierarchical browsing and search of large image databases","url":"https://www.researchgate.net/publication/3327141_Hierarchical_browsing_and_search_of_large_image_databases","abstraction":"The advent of large image databases (&gt;10000) has created a need for tools which can search and organize images automatically by their content. This paper focuses on the use of hierarchical tree-structures to both speed-up search-by-query and organize databases for effective browsing. The first part of this paper develops a fast search algorithm based on best-first branch and bound search. This algorithm is designed so that speed and accuracy may be continuously traded-off through the selection of a parameter ?. We find that the algorithm is most effective when used to perform an approximate search, where it can typically reduce computation by a factor of 20-40 for accuracies ranging from 80% to 90%. We then present a method for designing a hierarchical browsing environment which we call a similarity pyramid. The similarity pyramid groups similar images together while allowing users to view the database at varying levels of resolution. We show that the similarity pyramid is best constructed using agglomerative (bottom up) clustering methods, and present a fast sparse clustering method which dramatically reduces both memory and computation over conventional methods"},{"id":839,"title":"Encoding Local Correspondence in Topic Models","url":"https://www.researchgate.net/publication/258342205_Encoding_Local_Correspondence_in_Topic_Models","abstraction":"Exploiting label correlations is a challenging and crucial problem especially in multi-label learning context. Labels correlations are not necessarily shared by all instances and have generally a local definition. This paper introduces LOCLDA, which is a latent variable model that adresses the problem of modeling annotated data by locally exploiting correlations between annotations. In particular, we represent explicitly local dependencies to define the correspondence between specific objects, i.e. regions of images and their annotations. We conducted experiments on a collection of pictures provided by the Wikipedia \"Picture of the day\" and evaluated our model on the task of \"automatic image annotation\". The results validate the effectiveness of our approach."},{"id":840,"title":"On Taxonomies for Multi-class Image Categorization","url":"https://www.researchgate.net/publication/225444067_On_Taxonomies_for_Multi-class_Image_Categorization","abstraction":"We study the problem of classifying images into a given, pre-determined taxonomy. This task can be elegantly translated into the structured learning framework. However, despite its power, structured learning has known limits in scalability due to its high memory requirements and slow training process. We propose an efficient approximation of the structured learning approach by an ensemble of local support vector machines (SVMs) that can be trained efficiently with standard techniques. Afirst theoretical discussion and experiments on toy-data allow to shed light onto why taxonomy-based classification can outperform taxonomy-free approaches and why an appropriately combined ensemble of local SVMs might be of high practical use. Further empirical results on subsets of Caltech256 and VOC2006 data indeed show that our local SVM formulation can effectively exploit the taxonomy structure and thus outperforms standard multi-class classification algorithms while it achieves on par results with taxonomy-based structured algorithms at a significantly decreased computing time.  KeywordsMulti-class object categorization–Taxonomies–Support vector machine–Structure learning"},{"id":841,"title":"Holistic Context Models for Visual Recognition","url":"https://www.researchgate.net/publication/224253624_Holistic_Context_Models_for_Visual_Recognition","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A novel framework to context modeling based on the probability of co-occurrence of objects and scenes is proposed. The modeling is quite simple, and builds upon the availability of robust appearance classifiers. Images are represented by their posterior probabilities with respect to a set of contextual models, built upon the bag-of-features image representation, through two layers of probabilistic modeling. The first layer represents the image in a semantic space, where each dimension encodes an appearance-based posterior probability with respect to a concept. Due to the inherent ambiguity of classifying image patches, this representation suffers from a certain amount of contextual noise. The second layer enables robust inference in the presence of this noise by modeling the distribution of each concept in the semantic space. A thorough and systematic experimental evaluation of the proposed context modeling is presented. It is shown that it captures the contextual “gist” of natural images. Scene classification experiments show that contextual classifiers outperform their appearance-based counterparts, irrespective of the precise choice and accuracy of the latter. The effectiveness of the proposed approach to context modeling is further demonstrated through a comparison to existing approaches on scene classification and image retrieval, on benchmark data sets. In all cases, the proposed approach achieves superior results.\n</div> \n<p></p>"},{"id":842,"title":"A Novel Multi-modal Integration and Propagation Model for Cross-Media Information Retrieval","url":"https://www.researchgate.net/publication/220988784_A_Novel_Multi-modal_Integration_and_Propagation_Model_for_Cross-Media_Information_Retrieval","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we present a novel Probabilistic Latent Semantic Analysis-based (PLSA-based) aspect model and turn cross-media retrieval into two parts of multi-modal integration and correlation propagation. We first use multivariate Gaussian distributions to model continuous quantity in PLSA, avoiding information loss between feature-instance versus real-world matching. Multi-modal correlations are learned in an asymmetrical manner, giving a better control of the respective influence of each modality in the latent space. Then we propose a new propagation pattern to refine multi-modal correlations by efficiently taking the complementary from multi-modalities. Experimental results demonstrate that our method is accurate and robust for cross-media information retrieval.\n</div> \n<p></p>"},{"id":843,"title":"Optimizing interaction force for global anomaly detection in crowded scenes","url":"https://www.researchgate.net/publication/221430050_Optimizing_interaction_force_for_global_anomaly_detection_in_crowded_scenes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a novel method for global anomaly detection in crowded scenes. The proposed method introduces the Particle Swarm Optimization (PSO) method as a robust algorithm for optimizing the interaction force computed using the Social Force Model (SFM). The main objective of the proposed method is to drift the population of particles towards the areas of the main image motion. Such displacement is driven by the PSO fitness function aimed at minimizing the interaction force, so as to model the most diffused and typical crowd behavior. Experiments are extensively conducted on public available datasets, namely, UMN and PETS 2009, and also on a challenging dataset of videos taken from Internet. The experimental results revealed that the proposed scheme outperforms all the available state-of-the-art algorithms for global anomaly detection.\n</div> \n<p></p>"},{"id":844,"title":"Towards Ontologies for Image Interpretation and Annotation","url":"https://www.researchgate.net/publication/224251250_Towards_Ontologies_for_Image_Interpretation_and_Annotation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Due to the well-known semantic gap problem, a wide number of approaches have been proposed during the last decade for automatic image annotation, i.e. the textual description of images. Since these approaches are still not sufficiently efficient, a new trend is to use semantic hierarchies of concepts or ontologies to improve the image annotation process. This paper presents an overview and an analysis of the use of semantic hierarchies and ontologies to provide a deeper image understanding and a better image annotation in order to furnish retrieval facilities to users.\n</div> \n<p></p>"},{"id":845,"title":"Customer segmentation of multiple category data in e-commerce using a soft-clustering approach","url":"https://www.researchgate.net/publication/220066510_Customer_segmentation_of_multiple_category_data_in_e-commerce_using_a_soft-clustering_approach","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The segmentation of online consumers into multiple categories can contribute to a better understanding and characterization of purchasing behavior in the electronic commerce market. Online shopping databases consist of multiple kinds of data on customer purchasing activity and demographic characteristics, as well as consumption attributes such as Internet usage and satisfaction with services. Information about customers uncovered by segmentation enables company administrators to establish good customer relations and refine their marketing strategies to match customer expectations. To achieve optimal segmentation, we developed a soft clustering method that uses a latent mixed-class membership clustering approach to classify online customers based on their purchasing data across categories. A technique derived from the latent Dirichlet allocation model is used to create the customer segments. Variational approximation is leveraged to generate estimates from the segmentation in a computationally-efficient manner. The proposed soft clustering method yields more promising results than hard clustering and greater within-segment clustering quality than the finite mixture model.\n</div> \n<p></p>"},{"id":846,"title":"An effective CBVR system based on motion, quantized color and edge density features","url":"https://www.researchgate.net/publication/229038173_An_effective_CBVR_system_based_on_motion_quantized_color_and_edge_density_features","abstraction":"Rapid development of the multimedia and the associated technologies urge the processing of a huge database of video clips. The processing efficiency lies on the search methodologies utilized in the video processing system. Usage of inappropriate search methodologies may make the processing system ineffective. Hence, an effective video retrieval system is an essential pre-requisite for searching a relevant video from a huge collection of videos. In this paper, an effective content based video retrieval system based on some dominant features such as motion, color and edge is proposed. The system is comprised of two stages, namely, feature extraction and retrieval of similar video clips for the given query clip. Prior to perform the feature extraction, the database video clips are segmented into different shots. In the feature extraction, firstly, the motion feature is extracted using Squared Euclidean distance. Secondly, color feature is extracted based on color quantization. Thirdly, edge density feature is extracted for the objects present in the database video clips. When a video clip is queried in the system, the second stage of the system retrieves a given number of video clips from the database that are similar to the query clip. The retrieval is performed based on the Latent Semantic Indexing, which measures the similarity between the database video clips and the query clip. The system is evaluated using the video clips of format MPEG-2 and then precision-recall is determined for the test clip. .Keywords Content based video retrieval (CBVR) system, shot segmentation, motion feature, quantized color feature, edge density, Latent Semantic Indexing (LSI)."},{"id":847,"title":"Harvesting Image Databases from the Web","url":"https://www.researchgate.net/publication/49845075_Harvesting_Image_Databases_from_the_Web","abstraction":"The objective of this work is to automatically generate a large number of images for a specified object class. A multimodal approach employing both text, metadata, and visual features is used to gather many high-quality images from the Web. Candidate images are obtained by a text-based Web search querying on the object identifier (e.g., the word penguin). The Webpages and the images they contain are downloaded. The task is then to remove irrelevant images and rerank the remainder. First, the images are reranked based on the text surrounding the image and metadata features. A number of methods are compared for this reranking. Second, the top-ranked images are used as (noisy) training data and an SVM visual classifier is learned to improve the ranking further. We investigate the sensitivity of the cross-validation procedure to this noisy training data. The principal novelty of the overall method is in combining text/metadata and visual features in order to achieve a completely automatic ranking of the images. Examples are given for a selection of animals, vehicles, and other classes, totaling 18 classes. The results are assessed by precision/recall curves on ground-truth annotated data and by comparison to previous approaches, including those of Berg and Forsyth and Fergus et al."},{"id":848,"title":"Keeping Neural Networks Simple by Minimizing the Description Length of the Weights","url":"https://www.researchgate.net/publication/2280218_Keeping_Neural_Networks_Simple_by_Minimizing_the_Description_Length_of_the_Weights","abstraction":"Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-off between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed efficiently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting..."},{"id":849,"title":"An Introduction to Variational Methods for Graphical Models","url":"https://www.researchgate.net/publication/33020867_An_Introduction_to_Variational_Methods_for_Graphical_Models","abstraction":"This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models. We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, showing how upper and lower bounds can be found for local probabilities, and discussing methods for extending these bounds to bounds on global probabilities of interest. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case"},{"id":850,"title":"Variational Inference for Bayesian Mixture of Factor Analysers","url":"https://www.researchgate.net/publication/2281698_Variational_Inference_for_Bayesian_Mixture_of_Factor_Analysers","abstraction":"We present an algorithm that infers the model structure of a mixture of factor analysers using an efficient and deterministic variational approximation to full Bayesian integration over model parameters. This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i.e. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to overfitting. Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples. 1 Introduction Factor analysis (FA) is a method for modelling correlations in multidimensional data. The model assumes that each p-dimensi..."},{"id":851,"title":"A Variational Bayesian Framework for Graphical Models","url":"https://www.researchgate.net/publication/2239998_A_Variational_Bayesian_Framework_for_Graphical_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally nonGaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation. 1 Introduction A standard method to learn a graphical model 1 from data is maximum likelihood (ML). Given a training dataset, ML estimates a single optimal value f...\n</div> \n<p></p>"},{"id":852,"title":"An Approach to Time Series Smoothing and Forecasting Using the EM Algorithm","url":"https://www.researchgate.net/publication/243712636_An_Approach_to_Time_Series_Smoothing_and_Forecasting_Using_the_EM_Algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n . An approach to smoothing and forecasting for time series with missing observations is proposed. For an underlying state-space model, the EM algorithm is used in conjunction with the conventional Kalman smoothed estimators to derive a simple recursive procedure for estimating the parameters by maximum likelihood. An example is given which involves smoothing and forecasting an economic series using the maximum likelihood estimators for the parameters.\n</div> \n<p></p>"},{"id":853,"title":"Bayesian Methods for Mixtures of Experts","url":"https://www.researchgate.net/publication/2251448_Bayesian_Methods_for_Mixtures_of_Experts","abstraction":"We present a Bayesian framework for inferring the parameters of a mixture of experts model based on ensemble learning by variational free energy minimisation. The Bayesian approach avoids the over-fitting and noise level under-estimation problems of traditional maximum likelihood inference. We demonstrate these methods on artificial problems and sunspot time series prediction. INTRODUCTION The task of estimating the parameters of adaptive models such as artificial neural networks using Maximum Likelihood (ML) is well documented eg. Geman, Bienenstock &amp; Doursat (1992). ML estimates typically lead to models with high variance, a process known as \"over-fitting\". ML also yields over-confident predictions; in regression problems for example, ML underestimates the noise level. This problem is particularly dominant in models where the ratio of the number of data points in the training set to the number of parameters in the model is low. In this paper we consider inference of the parameters o..."},{"id":854,"title":"Ensemble Learning for Hidden Markov Models","url":"https://www.researchgate.net/publication/224881830_Ensemble_Learning_for_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The standard method for training Hidden Markov Models optimizes a point estimate of the model parameters. This estimate, which can be viewed as the maximum of a posterior probability density over the model parameters, may be susceptible to overfitting, and contains no indication of parameter uncertainty. Also, this maximummay be unrepresentative of the posterior probability distribution. In this paper we study a method in which we optimize an ensemble which approximates the entire posterior probability distribution. The ensemble learning algorithm requires the same resources as the traditional Baum--Welch algorithm. The traditional training algorithm for hidden Markov models is an expectation-- maximization (EM) algorithm (Dempster et al. 1977) known as the Baum--Welch algorithm. It is a maximum likelihood method, or, with a simple modification, a penalized maximum likelihood method, which can be viewed as maximizing a posterior probability density over the model parameters. Recently, ...\n</div> \n<p></p>"},{"id":855,"title":"Variational Inference For Probabilistic Latent Tensor Factorization with KL Divergence","url":"https://www.researchgate.net/publication/266261674_Variational_Inference_For_Probabilistic_Latent_Tensor_Factorization_with_KL_Divergence","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Probabilistic Latent Tensor Factorization (PLTF) is a recently proposed\n <br> probabilistic framework for modelling multi-way data. Not only the common\n <br> tensor factorization models but also any arbitrary tensor factorization\n <br> structure can be realized by the PLTF framework. This paper presents full\n <br> Bayesian inference via variational Bayes that facilitates more powerful\n <br> modelling and allows more sophisticated inference on the PLTF framework. We\n <br> illustrate our approach on model order selection and link prediction.\n</div> \n<p></p>"},{"id":856,"title":"A Bayesian approach to the Lee–Seung update rules for NMF","url":"https://www.researchgate.net/publication/262641962_A_Bayesian_approach_to_the_Lee-Seung_update_rules_for_NMF","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n NMF is a Blind Source Separation technique decomposing multivariate non-negative data sets into meaningful non-negative basis components and non-negative weights. In its canonical form an NMF algorithm was proposed by Lee and Seung (1999) [31] employing multiplicative update rules. In this study we show how the latter follow from a new variational Bayes NMF algorithm VBNMF employing a Gaussian noise kernel.\n</div> \n<p></p>"},{"id":857,"title":"Variational Bayesian causal connectivity analysis for fMRI","url":"https://www.researchgate.net/publication/262532661_Variational_Bayesian_causal_connectivity_analysis_for_fMRI","abstraction":"The ability to accurately estimate effective connectivity among brain regions from neuroimaging data could help answering many open questions in neuroscience. We propose a method which uses causality to obtain a measure of effective connectivity from fMRI data. The method uses a vector autoregressive model for the latent variables describing neuronal activity in combination with a linear observation model based on a convolution with a hemodynamic response function. Due to the employed modeling, it is possible to efficiently estimate all latent variables of the model using a variational Bayesian inference algorithm. The computational efficiency of the method enables us to apply it to large scale problems with high sampling rates and several hundred regions of interest. We use a comprehensive empirical evaluation with synthetic and real fMRI data to evaluate the performance of our method under various conditions."},{"id":858,"title":"A Structured Prediction Approach for Missing Value Imputation","url":"https://www.researchgate.net/publication/258373922_A_Structured_Prediction_Approach_for_Missing_Value_Imputation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Missing value imputation is an important practical problem. There is a large\n <br> body of work on it, but there does not exist any work that formulates the\n <br> problem in a structured output setting. Also, most applications have\n <br> constraints on the imputed data, for example on the distribution associated\n <br> with each variable. None of the existing imputation methods use these\n <br> constraints. In this paper we propose a structured output approach for missing\n <br> value imputation that also incorporates domain constraints. We focus on large\n <br> margin models, but it is easy to extend the ideas to probabilistic models. We\n <br> deal with the intractable inference step in learning via a piecewise training\n <br> technique that is simple, efficient, and effective. Comparison with existing\n <br> state-of-the-art and baseline imputation methods shows that our method gives\n <br> significantly improved performance on the Hamming loss measure.\n</div> \n<p></p>"},{"id":859,"title":"Applying Discrete PCA in Data Analysis","url":"https://www.researchgate.net/publication/229157546_Applying_Discrete_PCA_in_Data_Analysis","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Methods for analysis of principal components in discrete data have existed\n <br> for some time under various names such as grade of membership modelling,\n <br> probabilistic latent semantic analysis, and genotype inference with admixture.\n <br> In this paper we explore a number of extensions to the common theory, and\n <br> present some application of these methods to some common statistical tasks. We\n <br> show that these methods can be interpreted as a discrete version of ICA. We\n <br> develop a hierarchical version yielding components at different levels of\n <br> detail, and additional techniques for Gibbs sampling. We compare the algorithms\n <br> on a text prediction task using support vector machines, and to information\n <br> retrieval.\n</div> \n<p></p>"},{"id":860,"title":"Robust Autoregression: Student-t Innovations Using Variational Bayes","url":"https://www.researchgate.net/publication/224176534_Robust_Autoregression_Student-t_Innovations_Using_Variational_Bayes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Autoregression (AR) is a tool commonly used to understand and predict time series data. Traditionally the excitation noise is modelled as a Gaussian. However, real-world data may not be Gaussian in nature, and it is known that Gaussian models are adversely affected by the presence of outliers. We introduce a Bayesian AR model in which the excitation noise is assumed to be Student-t distributed. Variational Bayesian approximations to the posterior distributions of the model parameters are used to overcome the intractable integrations inherent in the Bayesian model. Independent automatic relevance determination (ARD) priors over each of the AR coefficients are used to estimate the model order. Using synthetic data, we show that the Student-t model performs well against both Gaussian and leptokurtic data, in terms of parameter estimation (including the model order) and is much more robust to outliers than either Gaussian or finite mixtures of Gaussian models. We apply the model to strongly leptokurtic EEG signals and show that the Student-t model makes more accurate one-step-ahead predictions than the Gaussian model and provides more consistent estimates of the AR coefficients over simultaneously recorded EEG channels.\n</div> \n<p></p>"},{"id":861,"title":"Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes.","url":"https://www.researchgate.net/publication/220320926_Approximate_Riemannian_Conjugate_Gradient_Learning_for_Fixed-Form_Variational_Bayes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin.\n</div> \n<p></p>"},{"id":862,"title":"Variational Bayesian speaker diarization of meeting recordings","url":"https://www.researchgate.net/publication/224149764_Variational_Bayesian_speaker_diarization_of_meeting_recordings","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper investigates the use of the Variational Bayesian (VB) framework for speaker diarization of meetings data extending previous related works on Broadcast News audio. VB learning aims at maximizing a bound, known as Free Energy, on the model marginal likelihood and allows joint model learning and model selection according to the same objective function. While the BIC is valid only in the asymptotic limit, the Free Energy is always a valid bound. The paper proposes the use of Free Energy as objective function in speaker diarization. It can be used to select dynamically without any supervision or tuning, elements that typically affect the diarization performance i.e. the inferred number of speakers, the size of the GMM and the initialization. The proposed approach is compared with a conventional state-of-the-art system on the RT06 evaluation data for meeting recordings diarization and shows an improvement of 8.4% relative in terms of speaker error.\n</div> \n<p></p>"},{"id":863,"title":"A novel approach to infer streamflow signals for ungauged basins","url":"https://www.researchgate.net/publication/229350044_A_novel_approach_to_infer_streamflow_signals_for_ungauged_basins","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper, we present a novel paradigm for inference of streamflow for ungauged basins. Our innovative procedure fuses concepts from both kernel methods and data assimilation. Based on the modularity and flexibility of kernel techniques and the strengths of the variational Bayesian Kalman filter and smoother, we can infer streamflow for ungauged basins whose hydrological and system properties and/or behavior are non-linear and non-Gaussian. We apply the proposed approach to two watersheds, one in California and one in West Virginia. The inferred streamflow signals for the two watersheds appear promising. These preliminary and encouraging validations demonstrate that our new paradigm is capable of providing accurate conditional estimates of streamflow for ungauged basins with unknown and non-linear dynamics.\n</div> \n<p></p>"},{"id":864,"title":"Accelerated Variational Dirichlet Process Mixtures.","url":"https://www.researchgate.net/publication/221618837_Accelerated_Variational_Dirichlet_Process_Mixtures","abstraction":"Dirichlet Process (DP) mixture models are promising candidates for clustering applications where the number of clusters is unknown a priori. Due to computational considerations these models are unfortunately unsuitable for large scale data-mining applications. We propose a class of deterministic accelerated DP mixture models that can routinely handle millions of data-cases. The speedup is achieved by incorporating kd-trees into a variational Bayesian algorithm for DP mixtures in the stick-breaking representation, similar to that of Blei and Jordan (2005). Our algorithm differs in the use of kd-trees and in the way we handle truncation: we only assume that the variational distributions are fixed at their priors after a certain level. Experiments show that speedups relative to the standard variational algorithm can be significant."},{"id":865,"title":"The Variational Approximation for Bayesian Inference Life after the EM algorithm","url":"https://www.researchgate.net/publication/224335433_The_Variational_Approximation_for_Bayesian_Inference_Life_after_the_EM_algorithm","abstraction":"The influence of this Thomas Bayes' work was immense. It was from here that \"Bayesian\" ideas first spread through the mathematical world, as Bayes's own article was ignored until 1780 and played no important role in scientific debate until the 20th century. It was also this article of Laplace's that introduced the mathematical techniques for the asymptotic analysis of posterior distributions that are still employed today. And it was here that the earliest example of optimum estimation can be found, the derivation and characterization of an estimator that minimized a particular measure of posterior expected loss. After more than two centuries, we mathematicians, statisticians cannot only recognize our roots in this masterpiece of our science, we can still learn from it."},{"id":866,"title":"Survey of automatic modulation classification techniques: Classical approaches and new trends","url":"https://www.researchgate.net/publication/3479904_Survey_of_automatic_modulation_classification_techniques_Classical_approaches_and_new_trends","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The automatic recognition of the modulation format of a detected signal, the intermediate step between signal detection and demodulation, is a major task of an intelligent receiver, with various civilian and military applications. Obviously, with no knowledge of the transmitted data and many unknown parameters at the receiver, such as the signal power, carrier frequency and phase offsets, timing information and so on, blind identification of the modulation is a difficult task. This becomes even more challenging in real-world scenarios with multipath fading, frequency-selective and time-varying channels. With this in mind, the authors provide a comprehensive survey of different modulation recognition techniques in a systematic way. A unified notation is used to bring in together, under the same umbrella, the vast amount of results and classifiers, developed for different modulations. The two general classes of automatic modulation identification algorithms are discussed in detail, which rely on the likelihood function and features of the received signal, respectively. The contributions of numerous articles are summarised in compact forms. This helps the reader to see the main characteristics of each technique. However, in many cases, the results reported in the literature have been obtained under different conditions. So, we have also simulated some major techniques under the same conditions, which allows a fair comparison among different methodologies. Furthermore, new problems that have appeared as a result of emerging wireless technologies are outlined. Finally, open problems and possible directions for future research are briefly discussed.\n</div> \n<p></p>"},{"id":867,"title":"Particle filters for mixture models with an unknown number of components","url":"https://www.researchgate.net/publication/220286556_Particle_filters_for_mixture_models_with_an_unknown_number_of_components","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the analysis of data under mixture models where the number of components in the mixture is unknown. We concentrate on mixture Dirichlet process models, and in particular we consider such models under conjugate priors. This conjugacy enables us to integrate out many of the parameters in the model, and to discretize the posterior distribution. Particle filters are particularly well suited to such discrete problems, and we propose the use of the particle filter of Fearnhead and Clifford for this problem. The performance of this particle filter, when analyzing both simulated and real data from a Gaussian mixture model, is uniformly better than the particle filter algorithm of Chen and Liu. In many situations it outperforms a Gibbs Sampler. We also show how models without the required amount of conjugacy can be efficiently analyzed by the same particle filter algorithm.\n</div> \n<p></p>"},{"id":868,"title":"Inequalities for the gamma function","url":"https://www.researchgate.net/publication/226088660_Inequalities_for_the_gamma_function","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Some inequalities for the gamma function are given. These results refine the classical Stirling approximation and its many\n <br> recent improvements.\n</div> \n<p></p>"},{"id":869,"title":"A Sequential Bayesian Inference Framework for Blind Frequency Offset Estimation","url":"https://www.researchgate.net/publication/280491673_A_Sequential_Bayesian_Inference_Framework_for_Blind_Frequency_Offset_Estimation","abstraction":"Precise estimation of synchronization parameters is essential for reliable data detection in digital communications and phase errors can result in significant performance degradation. The literature on estimation of synchronization parameters, including the carrier frequency offset, are based on approximations or heuristics because the optimal estimation problem is analytically intractable for most cases of interest. We develop an online Bayesian inference procedure for blind estimation of the frequency offset, for arbitrary signal constellations. Our unified approach is built on a sequential inference procedure that leverages a novel result on conjugacy of the von Mises and Gaussian distributions. This conjugacy allows for an easily computable, closed form parametric expression for the posterior distribution of the parameters given the streaming data, in which hyperparameters are recursively updated, making the optimal sequential estimation problem mathematically tractable. Our algorithm is computationally efficient and can be implemented in real-time with very low memory requirements. Numerical experiments are also provided and show that our methods outperform approximate sequential maximum-likelihood carrier frequency offset estimators."},{"id":870,"title":"Generation of Alternative Clusterings Using the CAMI Approach","url":"https://www.researchgate.net/publication/220907008_Generation_of_Alternative_Clusterings_Using_the_CAMI_Approach","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Exploratory data analysis aims to discover and generate mul- tiple views of the structure within a dataset. Conventional clustering techniques, however, are designed to only provide a single grouping or clustering of a dataset. In this paper, we introduce a novel algorithm called CAMI, that can un- cover alternative clusterings from a dataset. CAMI takes a mathematically appealing approach, combining the use of mutual information to distinguish between alternative clus- terings, coupled with an expectation maximization frame- work to ensure clustering quality. We experimentally test CAMI on both synthetic and real-world datasets, compar- ing it against a variety of state-of-the-art algorithms. We demonstrate that CAMI's performance is high and that its formulation provides a number of advantages compared to existing techniques.\n</div> \n<p></p>"},{"id":871,"title":"Variational inference for nonparametric multiple clustering","url":"https://www.researchgate.net/publication/228925262_Variational_inference_for_nonparametric_multiple_clustering","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Most clustering algorithms produce a single clustering solution. Similarly, feature selection for clustering tries to find one feature subset where one interesting clustering solution resides. However, a single data set may be multi-faceted and can be grouped and in-terpreted in many different ways, especially for high dimensional data, where feature selection is typically needed. Moreover, differ-ent clustering solutions are interesting for different purposes. In-stead of committing to one clustering solution, in this paper we introduce a probabilistic nonparametric Bayesian model that can discover several possible clustering solutions and the feature subset views that generated each cluster partitioning simultaneously. We provide a variational inference approach to learn the features and clustering partitions in each view. Our model allows us not only to learn the multiple clusterings and views but also allows us to auto-matically learn the number of views and the number of clusters in each view.\n</div> \n<p></p>"},{"id":872,"title":"Discovering Multiple Clustering Solutions: Grouping Objects in Different Views of the Data","url":"https://www.researchgate.net/publication/220766442_Discovering_Multiple_Clustering_Solutions_Grouping_Objects_in_Different_Views_of_the_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Traditional clustering algorithms identify just a single clustering of the data. Today's complex data, however, allow multiple interpretations leading to several valid groupings hidden in different views of the database. Each of these multiple clustering solutions is valuable and interesting as different perspectives on the same data and several meaningful groupings for each object are given. Especially for high dimensional data, where each object is described by multiple attributes, alternative clusters in different attribute subsets are of major interest. In this tutorial, we describe several real world application scenarios for multiple clustering solutions. We abstract from these scenarios and provide the general challenges in this emerging research area. We describe state-of-the-art paradigms, we highlight specific techniques, and we give an overview of this topic by providing a taxonomy of the existing clustering methods. By focusing on open challenges, we try to attract young researchers for participating in this emerging research field.\n</div> \n<p></p>"},{"id":873,"title":"Multiple Non-Redundant Spectral Clustering Views","url":"https://www.researchgate.net/publication/221346451_Multiple_Non-Redundant_Spectral_Clustering_Views","abstraction":"in several different ways for different purposes. For example, images of faces of people can be grouped based Many clustering algorithms only find one on their pose or identity. Web pages collected from clustering solution. However, data can of-universities can be clustered based on the type of webten be grouped and interpreted in many difpage’s owner, {faculty, student, staff}, field, {physics, ferent ways. This is particularly true in math, engineering, computer science}, or identity of the high-dimensional setting where differ-the university. In some cases, a data analyst wishes ent subspaces reveal different possible group-to find a single clustering, but this may require an alings of the data. Instead of committing gorithm to consider multiple clusterings and discard to one clustering solution, here we intro-those that are not of interest. In other cases, one may duce a novel method that can provide sev-wish to summarize and organize the data according to eral non-redundant clustering solutions to multiple possible clustering views. In either case, it is the user. Our approach simultaneously learns important to find multiple clustering solutions which non-redundant subspaces that provide multi-are non-redundant. ple views and finds a clustering solution in each view. We achieve this by augmenting a spectral clustering objective function to incorporate dimensionality reduction and multiple views and to penalize for redundancy between the views. 1."},{"id":874,"title":"Bayesian Co-clustering","url":"https://www.researchgate.net/publication/220765219_Bayesian_Co-clustering","abstraction":"In recent years, co-clustering has emerged as a powerful data mining tool that can analyze dyadic data connecting two entities. However, almost all existing co-clustering techniques are partitional, and allow individual rows and columns of a data matrix to belong to only one cluster. Several current applications, such as recommendation systems and market basket analysis, can substantially benefit from a mixed membership of rows and columns. In this paper, we present Bayesian co-clustering (BCC) models, that allow a mixed membership in row and column clusters. BCC maintains separate Dirichlet priors for rows and columns over the mixed membership and assumes each observation to be generated by an exponential family distribution corresponding to its row and column clusters. We propose a fast variational algorithm for inference and parameter estimation. The model is designed to naturally handle sparse matrices as the inference is done only based on the non-missing entries. In addition to finding a co-cluster structure in observations, the model outputs a low dimensional co-embedding, and accurately predicts missing values in the original matrix. We demonstrate the efficacy of the model through experiments on both simulated and real data."},{"id":875,"title":"Comparing Partitions","url":"https://www.researchgate.net/publication/24056046_Comparing_Partitions","abstraction":"The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between ±1."},{"id":876,"title":"A supervised machine learning algorithm for arrhythmia analysis","url":"https://www.researchgate.net/publication/3730982_A_supervised_machine_learning_algorithm_for_arrhythmia_analysis","abstraction":"A new machine learning algorithm for the diagnosis of cardiac arrhythmia from standard 12 lead ECG recordings is presented. The algorithm is called VF15 for Voting Feature Intervals. VF15 is a supervised and inductive learning algorithm for inducing classification knowledge from examples. The input to VF15 is a training set of records. Each record contains clinical measurements, from ECG signals and some other information such as sex, age, and weight, along with the decision of an expert cardiologist. The knowledge representation is based on a recent technique called Feature Intervals, where a concept is represented by the projections of the training cases on each feature separately. Classification in VF15 is based on a majority voting among the class predictions made by each feature separately. The comparison of the VF15 algorithm indicates that it outperforms other standard algorithms such as Naive Bayesian and Nearest Neighbor classifiers"},{"id":877,"title":"Joint hop timing and frequency estimation for collision resolution in FH networks","url":"https://www.researchgate.net/publication/3433312_Joint_hop_timing_and_frequency_estimation_for_collision_resolution_in_FH_networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n With the rapid growth of frequency-hopped (FH) wireless networks, interference due to frequency collisions has become one of the main performance-limiting challenges. This paper proposes a novel multiuser detection method for joint hop timing and frequency estimation, which is capable of unraveling and demodulating multiple FH transmissions in the presence of collisions and unknown hop patterns without retransmission. The method is based on the principle of dynamic programming (DP) coupled with two-dimensional harmonic retrieval (2-D HR) or low-rank trilinear decomposition, and it remains operational even with multiple unknown hop rates, frequency offsets, and asynchronism. The model is based on frequency-shift keying (FSK) and phase-shift keying (PSK) modulation, but the algorithms are also evaluated with Gaussian minimum-shift keying (GMSK) modulation and shown to be robust.\n</div> \n<p></p>"},{"id":878,"title":"Hop-timing estimation for FH signals using a coarsely channelized receiver","url":"https://www.researchgate.net/publication/3158954_Hop-timing_estimation_for_FH_signals_using_a_coarsely_channelized_receiver","abstraction":"This paper addresses new hop-timing (epoch) estimation schemes which employ a coarsely channelized preprocessor in order to suppress the frequency and phase dependence in random frequency-hopping (FH) signals. Coarse channelization implies a bank of filters, covering the total spread bandwidth of the FH signal, whose number is much smaller than the size of the candidate hop frequency set. Linear and nonlinear combinations of preprocessor outputs are explored and compared. It is found that post-processing, rather than the size of the filter bank alone, is the determining factor on estimation performance. Performance evaluation is presented via both extensive computer simulations and analytical lower bounds. Comparisons with existing optimal and suboptimal systems are also provided"},{"id":879,"title":"Blind high-resolution localization and tracking of multiple frequency hopped signals","url":"https://www.researchgate.net/publication/3318268_Blind_high-resolution_localization_and_tracking_of_multiple_frequency_hopped_signals","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper considers the problem of blind localization and\n <br> tracking of multiple frequency-hopped spread-spectrum signals using a\n <br> uniform linear antenna array without knowledge of hopping patterns or\n <br> directions of arrival. As a preprocessing step, we propose to identify a\n <br> hop-free subset of data by discarding high-entropy spectral slices from\n <br> the spectrogram. High-resolution localization is then achieved via\n <br> either quadrilinear regression of four-way data generated by\n <br> capitalizing on both spatial and temporal shift invariance or a new\n <br> maximum likelihood (ML)-based two-dimensional (2-D) harmonic retrieval\n <br> algorithm. The latter option achieves the best-known model\n <br> identifiability bound while remaining close to the Cramer-Rao bound even\n <br> at low signal-to-noise ratios (SNRs). Following beamforming using the\n <br> recovered directions, a dynamic programming approach is developed for\n <br> joint ML estimation of signal frequencies and hop instants in\n <br> single-user tracking. The efficacy of the proposed algorithms is\n <br> illustrated in pertinent simulations\n</div> \n<p></p>"},{"id":880,"title":"ML-based frequency estimation and synchronization of frequency hopping signals","url":"https://www.researchgate.net/publication/220324558_ML-based_frequency_estimation_and_synchronization_of_frequency_hopping_signals","abstraction":"A maximum likelihood (ML)-based algorithm for frequency estimation and synchronization of frequency hopping signals is proposed in this paper. By using a two-hop signal model that incorporates the unknown hop transition time, the likelihood function of the received frequency hopping signal is formulated. A new iterative method is then derived to estimate the hopping frequencies and hop transition time. Without using any pilot signal or sync bit, the new algorithm is able to implement synchronization and frequency estimation at the same time. Unlike the time-frequency distribution (TFD) and the wavelet-based algorithms in papers by Barbarossa and Scaglione (1997) and by Khalil and Hippenstiel (1996), the new ML algorithm does not require the selection of a kernel or mother wavelet function. In addition, compared with the TFD-based algorithm, it has a better performance with a lower implementation complexity."},{"id":881,"title":"Bayesian multi-Task compressive sensing with Dirichlet process priors","url":"https://www.researchgate.net/publication/221346525_Bayesian_multi-Task_compressive_sensing_with_Dirichlet_process_priors","abstraction":"Compressive sensing (CS) is an emerging £eld that, under appropriate conditions, can signi£cantly reduce the number of measurements required for a given signal. In many applications, one is interested in multiple signals that may be measured in multiple CS-type measurements, where here each signal corresponds to a sensing \"task\". In this paper we propose a novel multitask compressive sensing framework based on a Bayesian formalism, where a Dirichlet process (DP) prior is employed, yielding a principled means of simultaneously inferring the appropriate sharing mechanisms as well as CS inversion for each task. A variational Bayesian (VB) inference algorithm is employed to estimate the full posterior on the model parameters."},{"id":882,"title":"An Improved Auto-Calibration Algorithm Based on Sparse Bayesian Learning Framework","url":"https://www.researchgate.net/publication/260637032_An_Improved_Auto-Calibration_Algorithm_Based_on_Sparse_Bayesian_Learning_Framework","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This letter considers the multiplicative perturbation problem in compressive sensing, which has become an increasingly important issue on obtaining robust performance for practical applications. The problem is formulated in a probabilistic model and an auto-calibration sparse Bayesian learning algorithm is proposed. In this algorithm, signal and perturbation are iteratively estimated to achieve sparsity by leveraging a variational Bayesian expectation maximization technique. Results from numerical experiments have demonstrated that the proposed algorithm has achieved improvements on the accuracy of signal reconstruction.\n</div> \n<p></p>"},{"id":883,"title":"An Autofocus Technique for High-Resolution Inverse Synthetic Aperture Radar Imagery","url":"https://www.researchgate.net/publication/262527854_An_Autofocus_Technique_for_High-Resolution_Inverse_Synthetic_Aperture_Radar_Imagery","abstraction":"For inverse synthetic aperture radar imagery, the inherent sparsity of the scatterers in the range-Doppler domain has been exploited to achieve a high-resolution range profile or Doppler spectrum. Prior to applying the sparse recovery technique, preprocessing procedures are performed for the minimization of the translational-motion-induced Doppler effects. Due to the imperfection of coarse motion compensation, the autofocus technique is further required to eliminate the residual phase errors. This paper considers the phase error correction problem in the context of the sparse signal recovery technique. In order to encode sparsity, a multitask Bayesian model is utilized to probabilistically formulate this problem in a hierarchical manner. In this novel method, a focused high-resolution radar image is obtained by estimating the sparse scattering coefficients and phase errors in individual and global stages, respectively, to statistically make use of the sparsity. The superiority of this algorithm is that the uncertainty information of the estimation can be properly incorporated to obtain enhanced estimation accuracy. Moreover, the proposed algorithm achieves guaranteed convergence and avoids a tedious parameter-tuning procedure. Experimental results based on synthetic and practical data have demonstrated that our method has a desirable denoising capability and can produce a relatively well-focused image of the target, particularly in low signal-to-noise ratio and high undersampling ratio scenarios, compared with other recently reported methods."},{"id":884,"title":"An Empirical Bayesian Strategy for Solving the Simultaneous Sparse Approximation Problem","url":"https://www.researchgate.net/publication/3320324_An_Empirical_Bayesian_Strategy_for_Solving_the_Simultaneous_Sparse_Approximation_Problem","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Given a large overcomplete dictionary of basis vectors, the goal is to simultaneously represent L&gt;1 signal vectors using coefficient expansions marked by a common sparsity profile. This generalizes the standard sparse representation problem to the case where multiple responses exist that were putatively generated by the same small subset of features. Ideally, the associated sparse generating weights should be recovered, which can have physical significance in many applications (e.g., source localization). The generic solution to this problem is intractable and, therefore, approximate procedures are sought. Based on the concept of automatic relevance determination, this paper uses an empirical Bayesian prior to estimate a convenient posterior distribution over candidate basis vectors. This particular approximation enforces a common sparsity profile and consistently places its prominent posterior mass on the appropriate region of weight-space necessary for simultaneous sparse recovery. The resultant algorithm is then compared with multiple response extensions of matching pursuit, basis pursuit, FOCUSS, and Jeffreys prior-based Bayesian methods, finding that it often outperforms the others. Additional motivation for this particular choice of cost function is also provided, including the analysis of global and local minima and a variational derivation that highlights the similarities and differences between the proposed algorithm and previous approaches.\n</div> \n<p></p>"},{"id":885,"title":"Blind Frequency Hopping Spectrum Estimation: A Bayesian Approach","url":"https://www.researchgate.net/publication/273139744_Blind_Frequency_Hopping_Spectrum_Estimation_A_Bayesian_Approach","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Frequency hopping signals have been widely employed in wireless networks due to its robustness in anti-jamming and interference. In the scenario of coexistent networks, however , each user inevitably receives multiple unknown frequency hopping signals from different networks. In wireless networks with energy constraint, conventional re-transmission strategy is not affordable if collision happens. This paper considers the blind frequency hopping signal estimation problem in energy-constraint wireless senor networks, where re-transmission can be avoided due to the capability of robust spectrum estimation. We develop a novel high-resolution time-frequency representation by exploiting sparsity to allow signal sampled with sub-Nyquist ratio and achieve robust estimation performance. In our work, this problem is formulated in a probabilistic framework to induce sparsity statistically. Apart from sparsity, the piecewise smoothness in time-frequency domain is further leveraged with a clustering alike procedure by exerting a dependent Dirichlet process prior over the variance parametric space in an integrated manner. Results of numerical experiments show that the proposed algorithm can achieve superior performance particularly in sub-Nyquist sampling and low signal-to-noise ratio (SNR) scenarios compared with other recently reported ones.\n</div> \n<p></p>"},{"id":886,"title":"Sparse Component Analysis Using Time-Frequency Representations for Operational Modal Analysis","url":"https://www.researchgate.net/publication/273781351_Sparse_Component_Analysis_Using_Time-Frequency_Representations_for_Operational_Modal_Analysis","abstraction":"Sparse component analysis (SCA) has been widely used for blind source separation(BSS) for many years. Recently, SCA has been applied to operational modal analysis (OMA), which is also known as output-only modal identification. This paper considers the sparsity of sources' time-frequency (TF) representation and proposes a new TF-domain SCA under the OMA framework. First, the measurements from the sensors are transformed to the TF domain to get a sparse representation. Then, single-source-points (SSPs) are detected to better reveal the hyperlines which correspond to the columns of the mixing matrix. The K-hyperline clustering algorithm is used to identify the direction vectors of the hyperlines and then the mixing matrix is calculated. Finally, basis pursuit de-noising technique is used to recover the modal responses, from which the modal parameters are computed. The proposed method is valid even if the number of active modes exceed the number of sensors. Numerical simulation and experimental verification demonstrate the good performance of the proposed method."},{"id":887,"title":"Hierarchical Sparsity-Regularized Framework Based Frequency Hopping Spectrum Estimation With Antenna Array System","url":"https://www.researchgate.net/publication/278728904_Hierarchical_Sparsity-Regularized_Framework_Based_Frequency_Hopping_Spectrum_Estimation_With_Antenna_Array_System","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Frequency hopping (FH) signals have been widely employed in wireless communication networks to combat interference and avoid collision. This paper considers the blind FH signal estimation problem in antenna array systems, where the direction-of-arrivals, hopping time and frequency are all unknown to the users. A hierarchical sparsity-aware technique is developed to estimate these parameters in an optimization framework. More concretely, sparsity in spatial domain and time-frequency domain are exploited in a hierarchical and iterative manner, respectively, where more accurate parameter estimation performance can be obtained. Compared to prior state-of-the-arts, conventional model-order selection procedure can be conveniently avoided due to the utilization of sparsity-regularized framework. Results of numerical experiments show that the proposed algorithm can achieve superior performance particularly in sub-Nyquist sampling and low signal-to-noise ratio (SNR) scenarios compared with other recently reported ones.\n</div> \n<p></p>"},{"id":888,"title":"Support Vector Clustering","url":"https://www.researchgate.net/publication/220320417_Support_Vector_Clustering","abstraction":"We present a novel clustering method using the approach of support vector machines. Data points are mapped by means of a Gaussian kernel to a high dimensional feature space, where we search for the minimal enclosing sphere. This sphere, when mapped back to data space, can separate into several components, each enclosing a separate cluster of points. We present a simple algorithm for identifying these clusters. The width of the Gaussian kernel controls the scale at which the data is probed while the soft margin constant helps in coping with outliers and overlapping clusters. The structure of a dataset is explored by varying the two parameters, maintaining a minimal number of support vectors to assure smooth cluster boundaries. We demonstrate the performance of our algorithm on several datasets. 1"},{"id":889,"title":"Representation Learning: A Review and New Perspectives","url":"https://www.researchgate.net/publication/240308775_Representation_Learning_A_Review_and_New_Perspectives","abstraction":"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."},{"id":890,"title":"Online Passive-Aggressive Algorithms","url":"https://www.researchgate.net/publication/220320111_Online_Passive-Aggressive_Algorithms","abstraction":"We present a family of margin based online learning algorithms for various prediction tasks. In particular we derive and analyze algorithms for binary and multiclass categorization, regression, uniclass prediction and sequence prediction. The update steps of our different algorithms are all based on analytical solutions to simple constrained optimization problems. This unified view al- lows us to prove worst-case loss bounds for the different algorithms and for the various decision problems based on a single lemma. Our bounds on the cumulative loss of the algorithms are relative to the smallest loss that can be attained by any fixed hypothes is, and as such are applicable to both realizable and unrealizable settings. We demonstrate some of the merits of the proposed algorithms in a series of experiments with synthetic and real data sets."},{"id":891,"title":"Why Does Unsupervised Pre-training Help Deep Learning?","url":"https://www.researchgate.net/publication/220319875_Why_Does_Unsupervised_Pre-training_Help_Deep_Learning","abstraction":"Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main quest ion investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre- training guides the learning towards basins of attraction o f minima that support better generalization from the training data set; the evidence from these results s upports a regularization explanation for the effect of pre-training."},{"id":892,"title":"Dirichlet Process Mixtures of Generalized Linear Models","url":"https://www.researchgate.net/publication/45875143_Dirichlet_Process_Mixtures_of_Generalized_Linear_Models","abstraction":"We propose Dirichlet Process mixtures of Generalized Linear Models (DP-GLM), a new method of nonparametric regression that accommodates continuous and categorical inputs, and responses that can be modeled by a generalized linear model. We prove conditions for the asymptotic unbiasedness of the DP-GLM regression mean function estimate. We also give examples for when those conditions hold, including models for compactly supported continuous distributions and a model with continuous covariates and categorical response. We empirically analyze the properties of the DP-GLM and why it provides better results than existing Dirichlet process mixture regression models. We evaluate DP-GLM on several data sets, comparing it to modern methods of nonparametric regression like CART, Bayesian trees and Gaussian processes. Compared to existing techniques, the DP-GLM provides a single model (and corresponding inference algorithms) that performs well in many regression settings."},{"id":893,"title":"Reducing the Dimensionality of Data with Neural Networks","url":"https://www.researchgate.net/publication/6912170_Reducing_the_Dimensionality_of_Data_with_Neural_Networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.\n</div> \n<p></p>"},{"id":894,"title":"A Fast Learning Algorithm for Deep Belief Nets","url":"https://www.researchgate.net/publication/7017915_A_Fast_Learning_Algorithm_for_Deep_Belief_Nets","abstraction":"We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."},{"id":895,"title":"Learning A Task-Specific Deep Architecture For Clustering","url":"https://www.researchgate.net/publication/281487831_Learning_A_Task-Specific_Deep_Architecture_For_Clustering","abstraction":"While deep networks show to be highly effective in extensive applications, few efforts have been spent on studying its potential in clustering. In this paper, we argue that the successful domain expertise of sparse coding in clustering is still valuable, and can be combined with the key ingredients of deep learning. A novel feed-forward architecture, named TAG-LISTA, is constructed from graph-regularized sparse coding. It is then trained with task-specific loss functions from end to end. The inner connections of the proposed network to sparse coding leads to more effective training. Moreover, by introducing auxiliary clustering tasks to the hierarchy of intermediate features, we present DTAG-LISTA and obtain a further performance boost. We demonstrate extensive experiments on several benchmark datasets, under a wide variety of settings. The results verify that the proposed model performs significantly outperforms the generic architectures of the same parameter capacity, and also gains remarkable margins over several state-of-the-art methods."},{"id":896,"title":"Gaussian Process Dynamical Models for Human Motion","url":"https://www.researchgate.net/publication/5764824_Gaussian_Process_Dynamical_Models_for_Human_Motion","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We introduce Gaussian process dynamical models (GPDM) for nonlinear time series analysis, with applications to learning models of human pose and motion from high-dimensionalmotion capture data. A GPDM is a latent variable model. It comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian process priors for both the dynamics and the observation mappings. This results in a non-parametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach, and compare four learning algorithms on human motion capture data in which each pose is 50-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces.\n</div> \n<p></p>"},{"id":897,"title":"Gaussian Process for Machine Learning","url":"https://www.researchgate.net/publication/41781559_Gaussian_Process_for_Machine_Learning","abstraction":"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes."},{"id":898,"title":"Ferguson Distributions Via Polya Urn Schemes","url":"https://www.researchgate.net/publication/38357646_Ferguson_Distributions_Via_Polya_Urn_Schemes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The Polya urn scheme is extended by allowing a continuum of colors. For the extended scheme, the distribution of colors after $n$ draws is shown to converge as $n \\rightarrow \\infty$ to a limiting discrete distribution $\\mu^\\ast$. The distribution of $\\mu^\\ast$ is shown to be one introduced by Ferguson and, given $\\mu^\\ast$, the colors drawn from the urn are shown to be independent with distribution $\\mu^\\ast$.\n</div> \n<p></p>"},{"id":899,"title":"Latent Dirichlet Allocation","url":"https://www.researchgate.net/publication/262157385_Latent_Dirichlet_Allocation","abstraction":"We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model."},{"id":900,"title":"Inadequacy of interval estimates corresponding to variational Bayesian approximations","url":"https://www.researchgate.net/publication/246452549_Inadequacy_of_interval_estimates_corresponding_to_variational_Bayesian_approximations","abstraction":"In this paper we investigate the properties of the covariance matrices associated with variational Bayesian approximations, based on data from mixture models, and com- pare them with the true covariance matri- ces, corresponding to Fisher information ma- trices. It is shown that the covariance ma- trices from the variational Bayes approxima- tions are normally 'too small' compared with those for the maximum likelihood estimator, so that resulting interval estimates for the pa- rameters will be unrealistically narrow, espe- cially if the components of the mixture model are not well separated."},{"id":901,"title":"Two problems with variational expectation maximisation for time-series models","url":"https://www.researchgate.net/publication/255061313_Two_problems_with_variational_expectation_maximisation_for_time-series_models","abstraction":"Variational methods are a key component of the approximate inference and learn- ing toolbox. These methods fill an important middle ground, r etaining distribu- tional information about uncertainty in latent variables, unlike maximum a pos- teriori methods (MAP), and yet requiring fewer computational resources than Monte Carlo Markov Chain methods. In particular the variational Expectation Maximisation (vEM) and variational Bayes algorithms, both involving variational optimisation of a free energy, are widely used in time-serie s modelling. Here, we investigate the success of vEM in simple probabilistic time-series models. First we consider the inference step of vEM, and show that a consequence of the well- known compactness property is a failure to propagate uncertainty in time, thus limiting the usefulness of the retained distributional inf ormation. In particular, the uncertainty may appear to be smallest precisely when the approximation is poor- est. Second, we consider parameter learning and analytical ly reveal systematic biases in the parameters found by vEM. Surprisingly, simpler variational approxi- mations (such a mean-field) can lead to less bias than more com plicated structured approximations."},{"id":902,"title":"Efficient Learning in Boltzmann Machines Using Linear Response Theory","url":"https://www.researchgate.net/publication/220500257_Efficient_Learning_in_Boltzmann_Machines_Using_Linear_Response_Theory","abstraction":"The learning process in Boltzmann Machines is computationally very expensive. The computational complexity of the exact algorithm is exponential in the number of neurons. We present a new approximate learning algorithm for Boltzmann Machines, which is based on mean field theory and the linear response theorem. The computational complexity of the algorithm is cubic in the number of neurons. In the absence of hidden units, we show how the weights can be directly computed from the fixed point equation of the learning rules. Thus, in this case we do not need to use a gradient descent procedure for the learning process. We show that the solutions of this method are close to the optimal solutions and give a significant improvement when correlations play a significant role. Finally, we apply the method to a pattern completion task and show good performance for networks up to 100 neurons. 1 Introduction Boltzmann Machines (BMs) (Ackley et al., 1985), are networks of binary neurons with a stoc..."},{"id":903,"title":"Linear Response Algorithms for Approximate Inference in Graphical Models","url":"https://www.researchgate.net/publication/7975586_Linear_Response_Algorithms_for_Approximate_Inference_in_Graphical_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Belief propagation (BP) on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph. However, it does not prescribe a way to compute joint distributions over pairs of distant nodes in the graph. In this article, we propose two new algorithms for approximating these pairwise probabilities, based on the linear response theorem. The first is a propagation algorithm that is shown to converge if BP converges to a stable fixed point. The second algorithm is based on matrix inversion. Applying these ideas to gaussian random fields, we derive a propagation algorithm for computing the inverse of a matrix.\n</div> \n<p></p>"},{"id":904,"title":"Using EM to Obtain Asymptotic Variance-covariance Matrices: The SEM Algorithm","url":"https://www.researchgate.net/publication/236853450_Using_EM_to_Obtain_Asymptotic_Variance-covariance_Matrices_The_SEM_Algorithm","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The expectation maximization (EM) algorithm is a popular, and often remarkably simple, method for maximum likelihood estimation in incomplete-data problems. One criticism of EM in practice is that asymptotic variance–covariance matrices for parameters (e.g., standard errors) are not automatic byproducts, as they are when using some other methods, such as Newton–Raphson. In this article we define and illustrate a procedure that obtains numerically stable asymptotic variance–covariance matrices using only the code for computing the complete-data variance–covariance matrix, the code for EM itself, and code for standard matrix operations. The basic idea is to use the fact that the rate of convergence of EM is governed by the fractions of missing information to find the increased variability due to missing information to add to the complete-data variance–covariance matrix. We call this supplemented EM algorithm the SEM algorithm. Theory and particular examples reinforce the conclusion that the SEM algorithm can be a practically important supplement to EM in many problems. SEM is especially useful in multiparameter problems where only a subset of the parameters are affected by missing information and in parallel computing environments. SEM can also be used as a tool for monitoring whether EM has converged to a (local) maximum.\n</div> \n<p></p>"},{"id":905,"title":"A Split-Merge Markov Chain Monte Carlo Procedure for the Dirichlet Process Mixture Model","url":"https://www.researchgate.net/publication/2243140_A_Split-Merge_Markov_Chain_Monte_Carlo_Procedure_for_the_Dirichlet_Process_Mixture_Model","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n . We propose a split-merge Markov chain algorithm to address the problem of inefficient sampling for conjugate Dirichlet process mixture models. Traditional Markov chain Monte Carlo methods for Bayesian mixture models, such as Gibbs sampling, can become trapped in isolated modes corresponding to an inappropriate clustering of data points. This article describes a Metropolis-Hastings procedure that can escape such local modes by splitting or merging mixture components. Our Metropolis-Hastings algorithm employs a new technique in which an appropriate proposal for splitting or merging components is obtained by using a restricted Gibbs sampling scan. We demonstrate empirically that our method outperforms the Gibbs sampler in situations where two or more components are similar in structure. Key words: Dirichlet process mixture model, Markov chain Monte Carlo, Metropolis-Hastings algorithm, Gibbs sampler, split-merge updates 1 Introduction Mixture models are often applied to density estim...\n</div> \n<p></p>"},{"id":906,"title":"Feature Allocations, Probability Functions, and Paintboxes","url":"https://www.researchgate.net/publication/235326519_Feature_Allocations_Probability_Functions_and_Paintboxes","abstraction":"The problem of inferring a clustering of a data set has been the subject of much research in Bayesian analysis, and there currently exists a solid mathematical foundation for Bayesian approaches to clustering. In particular, the class of probability distributions over partitions of a data set has been characterized in a number of ways, including via exchangeable partition probability functions (EPPFs) and the Kingman paintbox. Here, we develop a generalization of the clustering problem, called feature allocation, where we allow each data point to belong to an arbitrary, non-negative integer number of groups, now called features or topics. We define and study an \"exchangeable feature probability function\" (EFPF)---analogous to the EPPF in the clustering setting---for certain types of feature models. Moreover, we introduce a \"feature paintbox\" characterization---analogous to the Kingman paintbox for clustering---of the class of exchangeable feature models. We provide a further characterization of the subclass of feature allocations that have EFPF representations."},{"id":907,"title":"Detecting Duplicates in a Homicide Registry Using a Bayesian Partitioning Approach","url":"https://www.researchgate.net/publication/264425113_Detecting_Duplicates_in_a_Homicide_Registry_Using_a_Bayesian_Partitioning_Approach","abstraction":"Finding duplicates in homicide registries is an important step in keeping an accurate account of lethal violence. This task is not trivial when unique identifiers of the individuals are not available, and it is especially challenging when records are subject to errors and missing values. Traditional approaches to duplicate detection output independent decisions on the coreference status of each pair of records, which often leads to non-transitive decisions that have to be reconciled in some ad-hoc fashion. The task of finding duplicate records in a datafile can be alternatively posed as partitioning the datafile into groups of coreferent records. We present an approach that targets this partition of the file as the parameter of interest, thereby ensuring transitive decisions. Our Bayesian implementation allows us to incorporate prior information on the reliability of the fields in the datafile, which is especially useful when no training data are available, and it also provides a proper account of the uncertainty in the duplicate detection decisions. We present a study to detect killings that were reported multiple times to the United Nations Truth Commission for El Salvador."},{"id":908,"title":"SMERED: A Bayesian Approach to Graphical Record Linkage and De-duplication","url":"https://www.researchgate.net/publication/260482474_SMERED_A_Bayesian_Approach_to_Graphical_Record_Linkage_and_De-duplication","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We propose a novel unsupervised approach for linking records across\n <br> arbitrarily many files, while simultaneously detecting duplicate records within\n <br> files. Our key innovation is to represent the pattern of links between records\n <br> as a {\\em bipartite} graph, in which records are directly linked to latent true\n <br> individuals, and only indirectly linked to other records. This flexible new\n <br> representation of the linkage structure naturally allows us to estimate the\n <br> attributes of the unique observable people in the population, calculate $k$-way\n <br> posterior probabilities of matches across records, and propagate the\n <br> uncertainty of record linkage into later analyses. Our linkage structure lends\n <br> itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm,\n <br> which overcomes many obstacles encountered by previously proposed methods of\n <br> record linkage, despite the high dimensional parameter space. We assess our\n <br> results on real and simulated data.\n</div> \n<p></p>"},{"id":909,"title":"Bayesian benchmarking with applications to small area estimation","url":"https://www.researchgate.net/publication/227317297_Bayesian_benchmarking_with_applications_to_small_area_estimation","abstraction":"It is well-known that small area estimation needs explicit or at least implicit use of models (cf. Rao in Small Area Estimation, Wiley, New York, 2003). These model-based estimates can differ widely from the direct estimates, especially for areas with very low sample sizes. While model-based small area estimates are very useful, one potential difficulty with such estimates is that when aggregated, the overall estimate for a larger geographical area may be quite different from the corresponding direct estimate, the latter being usually believed to be quite reliable. This is because the original survey was designed to achieve specified inferential accuracy at this higher level of aggregation. The problem can be more severe in the event of model failure as often there is no real check for validity of the assumed model. Moreover, an overall agreement with the direct estimates at an aggregate level may sometimes be politically necessary to convince the legislators of the utility of small area estimates.  One way to avoid this problem is the so-called “benchmarking approach”, which amounts to modifying these model-based estimates so that we get the same aggregate estimate for the larger geographical area. Currently, the most popular approach is the so-called “raking” or ratio adjustment method, which involves multiplying all the small area estimates by a constant data-dependent factor so that the weighted total agrees with the direct estimate. There are alternate proposals, mostly from frequentist considerations, which meet also the aforementioned benchmarking criterion.  We propose in this paper a general class of constrained Bayes estimators which also achieve the necessary benchmarking. Many of the frequentist estimators, including some of the raked estimators, follow as special cases of our general result. Explicit Bayes estimators are derived which benchmark the weighted mean or both the weighted mean and weighted variability. We illustrate our methodology by developing poverty rates in school-aged children at the state level and then benchmarking these estimates to match at the national level. Unlike the existing frequentist benchmarking literature, which is primarily based on linear models, the proposed Bayesian approach can accommodate any arbitrary model, and the benchmarked Bayes estimators are based only on the posterior mean and the posterior variance-covariance matrix.  KeywordsArea-level–Penalty parameter–Two-stage–Weighted mean–Weighted variability"},{"id":910,"title":"An Information Potential Approach for Tracking and Surveilling Multiple Moving Targets using Mobile Sensor Agents","url":"https://www.researchgate.net/publication/252776282_An_Information_Potential_Approach_for_Tracking_and_Surveilling_Multiple_Moving_Targets_using_Mobile_Sensor_Agents","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The problem of surveilling moving targets using mobile sensor agents (MSAs) is applicable to a variety of fields, including environmental monitoring, security, and manufacturing. Several authors have shown that the performance of a mobile sensor can be greatly improved by planning its motion and control strategies based on its sensing objectives. This paper presents an information potential approach for computing the MSAs' motion plans and control inputs based on the feedback from a modified particle filter used for tracking moving targets. The modified particle filter, as presented in this paper implements a new sampling method (based on supporting intervals of density functions), which accounts for the latest sensor measurements and adapts, accordingly, a mixture representation of the probability density functions (PDFs) for the target motion. It is assumed that the target motion can be modeled as a semi-Markov jump process, and that the PDFs of the Markov parameters can be updated based on real-time sensor measurements by a centralized processing unit or MSAs supervisor. Subsequently, the MSAs supervisor computes an information potential function that is communicated to the sensors, and used to determine their individual feedback control inputs, such that sensors with bounded field-of-view (FOV) can follow and surveil the target over time.\n</div> \n<p></p>"},{"id":911,"title":"Bearing-only Target Tracking using a Bank of MAP Estimators","url":"https://www.researchgate.net/publication/224252865_Bearing-only_Target_Tracking_using_a_Bank_of_MAP_Estimators","abstraction":"Nonlinear estimation problems, such as bearing-only tracking, are often addressed using linearized estimators, e.g., the extended Kalman filter (EKF). These estimators generally suffer from linearization errors as well as the inability to track multimodal probability density functions (pdfs). In this paper, we propose a bank of batch maximum a posteriori (MAP) estimators as a general estimation framework that provides relinearization of the entire state history, multi-hypothesis tracking, and an efficient hypothesis generation scheme. Each estimator in the bank is initialized using a locally optimal state estimate for the current time step. Every time a new measurement becomes available, we convert the nonlinear cost function corresponding to this relaxed one-step subproblem into polynomial form, allowing to analytically and efficiently compute all stationary points. This local optimization generates highly probable hypotheses for the target trajectory and greatly improves the quality of the overall MAP estimate. Additionally, pruning and marginalization are employed to control the computational cost. Monte Carlo simulations and real-world experiments show that the proposed approach significantly outperforms the EKF, the standard batch MAP estimator, and the particle filter (PF), in terms of accuracy and consistency."},{"id":912,"title":"Optimal sensor placement and motion coordination for target tracking","url":"https://www.researchgate.net/publication/222579717_Optimal_sensor_placement_and_motion_coordination_for_target_tracking","abstraction":"This work studies optimal sensor placement and motion coordination strategies for mobile sensor networks. For a target-tracking application with range sensors, we investigate the determinant of the Fisher Information Matrix and compute it in the 2D and 3D cases, characterizing the global minima in the 2D case. We propose motion coordination algorithms that steer the mobile sensor network to an optimal deployment and that are amenable to a decentralized implementation. Finally, our numerical simulations illustrate how the proposed algorithms lead to improved performance of an extended Kalman filter in a target-tracking scenario."},{"id":913,"title":"An Introduction to the Kalman Filter","url":"https://www.researchgate.net/publication/200045331_An_Introduction_to_the_Kalman_Filter","abstraction":"In 1960, R.E. Kalman published his famous paper describing a recursive solution to the discrete-data linear filtering problem. Since that time, due in large part to advances in digital computing, the Kalman filter has been the subject of extensive research and application, particularly in the area of autonomous or assisted navigation.  The Kalman filter is a set of mathematical equations that provides an efficient computational (recursive) means to estimate the state of a process, in a way that minimizes the mean of the squared error. The filter is very powerful in several aspects: it supports estimations of past, present, and even future states, and it can do so even when the precise nature of the modeled system is unknown.  The purpose of this paper is to provide a practical introduction to the discrete Kalman filter. This introduction includes a description and some discussion of the basic discrete Kalman filter, a derivation, description and some discussion of the extended Kalman filter, and a relatively simple (tangible) example with real numbers &amp; results."},{"id":914,"title":"A New Extension of the Kalman Filter to Nonlinear Systems","url":"https://www.researchgate.net/publication/2445827_A_New_Extension_of_the_Kalman_Filter_to_Nonlinear_Systems","abstraction":"The Kalman filter(KF) is one of the most widely used methods for tracking and estimation due to its simplicity, optimality, tractability and robustness. However, the application of the KF to nonlinear systems can be difficult. The most common approach is to use the Extended Kalman Filter (EKF) which simply linearises all nonlinear models so that the traditional linear Kalman filter can be applied. Although the EKF (in its many forms) is a widely used filtering strategy, over thirty years of experience with it has led to a general consensus within the tracking and control community that it is difficult to implement, difficult to tune, and only reliable for systems which are almost linear on the time scale of the update intervals. In this paper a new linear estimator is developed and demonstrated. Using the principle that a set of discretely sampled points can be used to parameterise mean and covariance, the estimator yields performance equivalent to the KF for linear systems yet general..."},{"id":915,"title":"Control of nonholonomic mobile robot: backstepping kinematics into dynamics","url":"https://www.researchgate.net/publication/229022875_Control_of_nonholonomic_mobile_robot_backstepping_kinematics_into_dynamics","abstraction":"A dynamical extension that makes possible the integration of a kinematic controller and a torque controller for nonholonomic mobile robots is presented. A combined kinematic/torque control law is developed using backstepping, and asymptotic stability is guaranteed by Lyapunov theory. Moreover, this control algorithm can be applied to the three basic nonholonomic navigation problems: tracking a reference trajectory, path following, and stabilization about a desired posture. The result is a general structure for controlling a mobile robot that can accommodate different control techniques, rang-ing from a conventional computed-torque controller, when all dynamics are known, to robust-adaptive controllers if this is not the case. A robust-adaptive controller based on neural networks (NNs) is proposed in this work. The NN controller can deal with unmodeled bounded disturbances and/or unstructured unmodeled dynamics in the vehicle. On-line NN weight tuning algorithms that do not require off-line learning yet guarantee small tracking errors and bounded control signals are utilized."},{"id":916,"title":"Tracking and Regulation Control of an Underactuated Surface Vessel With Nonintegrable Dynamics","url":"https://www.researchgate.net/publication/3024329_Tracking_and_Regulation_Control_of_an_Underactuated_Surface_Vessel_With_Nonintegrable_Dynamics","abstraction":"A continuous, time-varying tracking controller is designed that globally exponentially forces the position/orientation tracking error of an underactuated surface vessel to a neighborhood about zero that can be made arbitrarily small i.e. global uniformly ultimately boundedness (GUUB). The result is facilitated by fusing a filtered tracking error transformation with a dynamic oscillator design. We also illustrate that the proposed tracking controller yields a GUUB result for the regulation problem"},{"id":917,"title":"A Geometric Transversals Approach to Analyzing the Probability of Track Detection for Maneuvering Targets","url":"https://www.researchgate.net/publication/262497772_A_Geometric_Transversals_Approach_to_Analyzing_the_Probability_of_Track_Detection_for_Maneuvering_Targets","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In the sensor tracking and estimation literature, there is considerable precedence for modeling maneuvering targets by Markov motion models in order to estimate the target state from multiple, distributed sensor measurements. Although the transition probability density functions of these Markov models are routinely outputted by tracking and estimation algorithms, little work has been done to use them as a feedback to sensor coordination and control algorithms. This paper presents a geometric transversals approach for representing the probability of track detection by multiple, distributed sensors, as a function of the Markov model transition probabilities. By this approach, the Markov parameters of maneuvering targets that may be detected by the sensors are represented by three-dimensional cones that are finitely generated by the sensors' fields-of-view in a spatio-temporal Euclidian space. Then, the problem of deploying a sensor network for the purpose of maximizing the expected number of target detections can be formulated as a nonlinear program that can be solved numerically for the optimal sensor placement. Numerical results show that the optimal sensor placements obtained by this geometric transversals approach significantly outperform greedy, grid, or randomized sensor deployments.\n</div> \n<p></p>"},{"id":918,"title":"A Bayesian Nonparametric Approach to Modeling Motion Patterns","url":"https://www.researchgate.net/publication/220473947_A_Bayesian_Nonparametric_Approach_to_Modeling_Motion_Patterns","abstraction":"The most difficult—and often most essential—aspect of many interception and tracking tasks is constructing motion models of the targets. Experts rarely can provide complete information about a target’s expected motion pattern, and fitting parameters for complex motion patterns can require large amounts of training data. Specifying how to parameterize complex motion patterns is in itself a difficult task.  In contrast, Bayesian nonparametric models of target motion are very flexible and generalize well with relatively little training data. We propose modeling target motion patterns as a mixture of Gaussian processes (GP) with a Dirichlet process (DP) prior over mixture weights. The GP provides an adaptive representation for each individual motion pattern, while the DP prior allows us to represent an unknown number of motion patterns. Both automatically adjust the complexity of the motion model based on the available data. Our approach outperforms several parametric models on a helicopter-based car-tracking task on data collected from the greater Boston area."},{"id":919,"title":"An Information Roadmap Method for Robotic Sensor Path Planning","url":"https://www.researchgate.net/publication/220062175_An_Information_Roadmap_Method_for_Robotic_Sensor_Path_Planning","abstraction":"A new probabilistic roadmap method is presented for planning the path of a robotic sensor deployed in order to classify multiple fixed targets located in an obstacle-populated workspace. Existing roadmap methods have been successful at planning a robot path for the purpose of moving from an initial to a final configu- ration in a workspace by a minimum distance. But they are not directly applicable to robots whose primary objective is to gather target information with an on-board sensor. In this paper, a novel information roadmap method is developed in which obstacles, targets, sensor's platform and field-of-view are represented as closed and bounded subsets of an Euclidean workspace. The information roadmap is sampled from a normalized information theoretic function that favors samples with a high expected value of information in configuration space. The method is applied to a landmine classification problem to plan the path of a robotic ground-penetrating radar, based on prior remote measurements and other geospatial data. Experiments show that paths obtained from the information roadmap exhibit a classification efficiency several times higher than that of existing search strategies. Also, the information roadmap can be used to deploy non-overpass capable robots that must avoid targets as well as obstacles."},{"id":920,"title":"Electronic Imaging 2004","url":"https://www.researchgate.net/publication/234972690_Electronic_Imaging_2004","abstraction":"This paper presents details of a system that allows for an evolutionary introduction of depth perception into the existing 2D digital TV framework. The work is part of the European Information Society Technologies (IST) project \"Advanced Three-Dimensional Television System Technologies\" (ATTEST), an activity, where industries, research centers and universities have joined forces to design a backwards-compatible, flexible and modular broadcast 3D-TV system. At the very heart of the described new concept is the generation and distribution of a novel data representation format, which consists of monoscopic color video and associated per-pixel depth information. From these data, one or more \"virtual\" views of a real-world scene can be synthesized in real-time at the receiver side (i.e. a 3D-TV set-top box) by means of so-called depth-image-based rendering (DIBR) techniques. This publication will provide: (1) a detailed description of the fundamentals of this new approach on 3D-TV; (2) a comparison with the classical approach of \"stereoscopic\" video; (3) a short introduction to DIBR techniques in general; (4) the development of a specific DIBR algorithm that can be used for the efficient generation of high-quality \"virtual\" stereoscopic views; (5) a number of implementation details that are specific to the current state of the development; (6) research on the backwards-compatible compression and transmission of 3D imagery using state-of-the-art MPEG (Moving Pictures Expert Group) tools.© (2004) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only."},{"id":921,"title":"A Variational Bayesian Inference Framework for Multiview Depth Image Enhancement","url":"https://www.researchgate.net/publication/261075118_A_Variational_Bayesian_Inference_Framework_for_Multiview_Depth_Image_Enhancement","abstraction":"In this paper, a general model-based framework for multiview depth image enhancement is proposed. Depth imagery plays a pivotal role in emerging free-viewpoint television. This technology requires high quality virtual view synthesis to enable viewers to move freely in a dynamic real world scene. Depth imagery of different viewpoints is used to synthesize an arbitrary number of novel views. Usually, the depth imagery is estimated individually by stereo-matching algorithms and, hence, shows lack of inter-view consistency. This inconsistency affects the quality of view synthesis negatively. This paper enhances the inter-view consistency of multiview depth imagery by using a variational Bayesian inference framework. First, our approach classifies the color information in the multiview color imagery. Second, using the resulting color clusters, we classify the corresponding depth values in the multiview depth imagery. Each clustered depth image is subject to further sub clustering. Finally, the resulting mean of the sub-clusters is used to enhance the depth imagery at multiple viewpoints. Experiments show that our approach improves the quality of virtual views by up to 0.25 dB."},{"id":922,"title":"Free-viewpoint TV","url":"https://www.researchgate.net/publication/224206053_Free-viewpoint_TV","abstraction":"Free-viewpoint television (FTV) is an innovative visual media that enables us to view a three-dimensional (3-D) scene by freely changing our viewpoints. We proposed the concept of FTV and constructed the world???s first real-time system including the complete chain of operation from image capture to display. We also carried out the FTV on a single personal computer (PC) and a mobile player. FTV is based on the ray-space method that represents one ray in real space with one point in the ray-space. We have developed several types of ray capture systems and interfaces such as a 360° capture/ray-reproducing display. FTV is regarded as the ultimate 3DTV, since it can generate infinite number of views. Thus, FTV is the key to immersive communication. Regarding FTV as the most challenging 3-D media, the Motion Picture Experts Group (MPEG) has been conducting its international standardization activities. This article reviews FTV and its related technologies."},{"id":923,"title":"State of the Art in Stereoscopic and Autostereoscopic Displays","url":"https://www.researchgate.net/publication/228855682_State_of_the_Art_in_Stereoscopic_and_Autostereoscopic_Displays","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This overview covers most of the 3-D displays that are in use today and presents recent developments and advances in this field. ABSTRACT | Underlying principles of stereoscopic direct-view displays, binocular head-mounted displays, and autostereo-scopic direct-view displays are explained and some early work as well as the state of the art in those technologies are re-viewed. Stereoscopic displays require eyewear and can be categorized based on the multiplexing scheme as: 1) color multiplexed (old technology but there are some recent devel-opments; low-quality due to color reproduction and crosstalk issues; simple and does not require additional electronics hardware); 2) polarization multiplexed (requires polarized light output and polarization-based passive eyewear; high-resolution and high-quality displays available); and 3) time multiplexed (requires faster display hardware and active glasses synchronized with the display; high-resolution com-mercial products available). Binocular head-mounted displays can readily provide 3-D, virtual images, immersive experience, and more possibilities for interactive displays. However, the bulk of the optics, matching of the left and right ocular images and obtaining a large field of view make the designs quite challenging. Some of the recent developments using uncon-ventional optical relays allow for thin form factors and open up new possibilities. Autostereoscopic displays are very attractive as they do not require any eyewear. There are many possibi-lities in this category including: two-view (the simplest implementations are with a parallax barrier or a lenticular screen), multiview, head tracked (requires active optics to redirect the rays to a moving viewer), and super multiview (potentially can solve the accommodation–convergence mis-match problem). Earlier 3-D booms did not last long mainly due to the unavailability of enabling technologies and the content. Current developments in the hardware technologies provide a renewed interest in 3-D displays both from the consumers and the display manufacturers, which is evidenced by the recent commercial products and new research results in this field.\n</div> \n<p></p>"},{"id":924,"title":"Model-Based Gaussian and Non-Gaussian Clustering","url":"https://www.researchgate.net/publication/265505693_Model-Based_Gaussian_and_Non-Gaussian_Clustering","abstraction":"The classification maximum likelihood approach is sufficiently general to encompass many current clustering algorithms, including those based on the sum of squares criterion and on the criterion of H. P. Friedman and J. Rubin [J. Am. Stat. Assoc. 62, 1159-1178 (1967)]. However, as currently implemented, it does not allow the specification of which features (orientation, size, and shape) are to be common to all clusters and which may differ between clusters. Also, it is restricted to Gaussian distributions and it does not allow for noise. We propose ways of overcoming these limitations. A reparameterization of the covariance matrix allows us to specify that some, but not all, features be the same for all clusters. A practical framework for non- Gaussian clustering is outlined, and a means of incorporating noise in the form of a Poisson process is described. An approximate Bayesian method for choosing the number of clusters is given. The performance of the proposed methods is studied by simulation, with encouraging results. The methods are applied to the analysis of a data set arising in the study of diabetes, and the results seem better than those of previous analyses. A magnetic resonance image (MRI) of the brain is also analyzed, and the methods appear successful in extracting the main features of anatomical interest. The methods described here have been implemented in both Fortran and S-PLUS versions, and the software is freely available through StatLib."},{"id":925,"title":"Depth consistency testing for improved view interpolation","url":"https://www.researchgate.net/publication/224203640_Depth_consistency_testing_for_improved_view_interpolation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Multiview video will play a pivotal role in the next generation visual communication media services like three-dimensional (3D) television and free-viewpoint television. These advanced media services provide natural 3D impressions and enable viewers to move freely in a dynamic real world scene by changing the viewpoint. High quality virtual view interpolation is required to support free viewpoint viewing. Usually, depth maps of different viewpoints are used to reconstruct a novel view. As these depth maps are usually estimated individually by stereo-matching algorithms, they have very weak spatial consistency. The inconsistency of depth maps affects the quality of view interpolation. In this paper, we propose a method for depth consistency testing to improve view interpolation. The method addresses the problem by warping more than two depth maps from multiple reference viewpoints to the virtual viewpoint. We test the consistency among warped depth values and improve the depth value information of the virtual view. With that, we enhance the quality of the interpolated virtual view.\n</div> \n<p></p>"},{"id":926,"title":"Variational Inference for Large-Scale Models of Discrete Choice","url":"https://www.researchgate.net/publication/1903443_Variational_Inference_for_Large-Scale_Models_of_Discrete_Choice","abstraction":"Discrete choice models are commonly used by applied statisticians in numerous fields, such as marketing, economics, finance, and operations research. When agents in discrete choice models are assumed to have differing preferences, exact inference is often intractable. Markov chain Monte Carlo techniques make approximate inference possible, but the computational cost is prohibitive on the large data sets now becoming routinely available. Variational methods provide a deterministic alternative for approximation of the posterior distribution. We derive variational procedures for empirical Bayes and fully Bayesian inference in the mixed multinomial logit model of discrete choice. The algorithms require only that we solve a sequence of unconstrained optimization problems, which are shown to be convex. Extensive simulations demonstrate that variational methods achieve accuracy competitive with Markov chain Monte Carlo, at a small fraction of the computational cost. Thus, variational methods permit inferences on data sets that otherwise could not be analyzed without bias-inducing modifications to the underlying model. Comment: 29 pages, 2 tables, 2 figures"},{"id":927,"title":"Construction of semantic bootstrapping models for relation extraction","url":"https://www.researchgate.net/publication/275897443_Construction_of_semantic_bootstrapping_models_for_relation_extraction","abstraction":"Traditionally, pattern-based relation extraction methods are usually based on iterative bootstrapping model which generally implies semantic drift or low recall problem. In this paper, we present a novel semantic bootstrapping framework that uses semantic information of patterns and flexible match method to address such problem. We introduce formalization for this class of bootstrapping models, which allows semantic constraint to guide learning iterations and use flexible bottom-up kernel to compare patterns. To obtain the insights of reliability and applicability of our framework, we applied it to the English Slot Filling (ESF) task of Knowledge Based Population (KBP) at Text Analysis Conference (TAC). Experimental results show that our framework obtains performance superior to the state of the art."},{"id":928,"title":"Multiview depth map enhancement by variational bayes inference estimation of Dirichlet mixture models","url":"https://www.researchgate.net/publication/261344934_Multiview_depth_map_enhancement_by_variational_bayes_inference_estimation_of_Dirichlet_mixture_models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n High quality view synthesis is a prerequisite for future free-viewpoint television. It will enable viewers to move freely in a dynamic real world scene. Depth image based rendering algorithms will play a pivotal role when synthesizing an arbitrary number of novel views by using a subset of captured views and corresponding depth maps only. Usually, each depth map is estimated individually by stereo-matching algorithms and, hence, shows lack of inter-view consistency. This inconsistency affects the quality of view synthesis negatively. This paper enhances the inter-view consistency of multiview depth imagery. First, our approach classifies the color information in the multiview color imagery by modeling color with a mixture of Dirichlet distributions where the model parameters are estimated in a Bayesian framework with variational inference. Second, using the resulting color clusters, we classify the corresponding depth values in the multiview depth imagery. Each clustered depth image is subject to further sub-clustering. Finally, the resulting mean of each sub-cluster is used to enhance the depth imagery at multiple viewpoints. Experiments show that our approach improves the average quality of virtual views by up to 0.8 dB when compared to views synthesized by using conventionally estimated depth maps.\n</div> \n<p></p>"},{"id":929,"title":"Probabilistic Multiview Depth Image Enhancement Using Variational Inference","url":"https://www.researchgate.net/publication/269167429_Probabilistic_Multiview_Depth_Image_Enhancement_Using_Variational_Inference","abstraction":"An inference-based multiview depth image enhancement algorithm is introduced and investigated in this paper. Multiview depth imagery plays a pivotal role in free-viewpoint television. This technology requires high-quality virtual view synthesis to enable viewers to move freely in a dynamic real world scene. Depth imagery of different viewpoints is used to synthesize an arbitrary number of novel views. Usually, the depth imagery is estimated individually by stereo-matching algorithms and, hence, shows inter-view inconsistency. This inconsistency affects the quality of view synthesis negatively. This paper enhances the multiview depth imagery at multiple viewpoints by probabilistic weighting of each depth pixel. First, our approach classifies the color pixels in the multiview color imagery. Second, using the resulting color clusters, we classify the corresponding depth values in the multiview depth imagery. Each clustered depth image is subject to further subclustering. Clustering based on generative models is used for assigning probabilistic weights to each depth pixel. Finally, these probabilistic weights are used to enhance the depth imagery at multiple viewpoints. Experiments show that our approach consistently improves the quality of virtual views by 0.2 dB to 1.6 dB, depending on the quality of the input multiview depth imagery."},{"id":930,"title":"Variational Bayesian Matrix Factorization for Bounded Support Data","url":"https://www.researchgate.net/publication/273397554_Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data","abstraction":"A novel Bayesian matrix factorization method for bounded support data is presented. Each entry in the observation matrix is assumed to be beta distributed. As the beta distribution has two parameters, two parameter matrices can be obtained, which matrices contain only nonnegative values. In order to provide low-rank matrix factorization, the nonnegative matrix factorization (NMF) technique is applied. Furthermore, each entry in the factorized matrices, i.e., the basis and excitation matrices, is assigned with gamma prior. Therefore, we name this method as beta-gamma NMF (BG-NMF). Due to the integral expression of the gamma function, estimation of the posterior distribution in the BG-NMF model can not be presented by an analytically tractable solution. With the variational inference framework and the relative convexity property of the log-inverse-beta function, we propose a new lower-bound to approximate the objective function. With this new lower-bound, we derive an analytically tractable solution to approximately calculate the posterior distributions. Each of the approximated posterior distributions is also gamma distributed, which retains the conjugacy of the Bayesian estimation. In addition, a sparse BG-NMF can be obtained by including a sparseness constraint to the gamma prior. Evaluations with synthetic data and real life data demonstrate the good performance of the proposed method."},{"id":931,"title":"Im2Sketch: Sketch generation by unconflicted perceptual grouping","url":"https://www.researchgate.net/publication/276107206_Im2Sketch_Sketch_generation_by_unconflicted_perceptual_grouping","abstraction":"Effectively solving the problem of sketch generation, which aims to produce human-drawing-like sketches from real photographs, opens the door for many vision applications such as sketch-based image retrieval and non-photorealistic rendering. In this paper, we approach automatic sketch generation from a human visual perception perspective. Instead of gathering insights from photographs, for the first time, we extract information from a large pool of human sketches. In particular, we study how multiple Gestalt rules can be encapsulated into a unified perceptual grouping framework for sketch generation. We further show that by solving the problem of Gestalt confliction, i.e., encoding the relative importance of each rule, more similar to human-made sketches can be generated. For that, we release a manually labeled sketch dataset of 96 object categories and 7,680 sketches. A novel evaluation framework is proposed to quantify human likeness of machine-generated sketches by examining how well they can be classified using models trained from human data. Finally, we demonstrate the superiority of our sketches under the practical application of sketch-based image retrieval."},{"id":932,"title":"Generating Realistic Labelled, Weighted Random Graphs","url":"https://www.researchgate.net/publication/286420001_Generating_Realistic_Labelled_Weighted_Random_Graphs","abstraction":"Generative algorithms for random graphs have yielded insights into the structure and evolution of real-world networks. Most networks exhibit a well-known set of properties, such as heavy-tailed degree distributions, clustering and community formation. Usually, random graph models consider only structural information, but many real-world networks also have labelled vertices and weighted edges. In this paper, we present a generative model for random graphs with discrete vertex labels and numeric edge weights. The weights are represented as a set of Beta Mixture Models (BMMs) with an arbitrary number of mixtures, which are learned from real-world networks. We propose a Bayesian Variational Inference (VI) approach, which yields an accurate estimation while keeping computation times tractable. We compare our approach to state-of-the-art random labelled graph generators and an earlier approach based on Gaussian Mixture Models (GMMs). Our results allow us to draw conclusions about the contribution of vertex labels and edge weights to graph structure."},{"id":933,"title":"Exponential Family Sparse Coding with Application to Self-taught Learning.","url":"https://www.researchgate.net/publication/220813491_Exponential_Family_Sparse_Coding_with_Application_to_Self-taught_Learning","abstraction":"Abstract Sparse coding is an unsupervised,learning algo- rithm for finding concise, slightly higher-level rep- resentations of inputs, and has been successfully applied to self-taught learning, where the goal is to use unlabeled data to help on a supervised learning task, even if the unlabeled data cannot be associ- ated with the labels of the supervised task [Raina et al., 2007]. However, sparse coding uses a Gaussian noise model and a quadratic loss function, and thus performs poorly if applied to binary valued, integer valued, or other non-Gaussian data, such as text. Drawing on ideas from generalized linear models (GLMs), we present a generalization of sparse cod- ing to learning with data drawn from any exponen- tial family distribution (such as Bernoulli, Poisson, etc). This gives a method,that we argue is much better suited to model other data types than Gaus- sian. We present an algorithm for solving the L1- regularized optimization problem,defined by this model, and show that it is especially efficient when the optimal solution is sparse. We also show that the new,model,results in significantly improved self-taught learning performance,when,applied to text classification and to a robotic perception task."},{"id":934,"title":"Bayesian k -Means as a “Maximization-Expectation” Algorithm","url":"https://www.researchgate.net/publication/23986893_Bayesian_k_-Means_as_a_Maximization-Expectation_Algorithm","abstraction":"We introduce a new class of \"maximization-expectation\" (ME) algorithms where we maximize over hidden variables but marginalize over random parameters. This reverses the roles of expectation and maximization in the classical expectation-maximization algorithm. In the context of clustering, we argue that these hard assignments open the door to very fast implementations based on data structures such as kd-trees and conga lines. The marginalization over parameters ensures that we retain the ability to infer model structure (i.e., number of clusters). As an important example, we discuss a top-down Bayesian k-means algorithm and a bottom-up agglomerative clustering algorithm. In experiments, we compare these algorithms against a number of alternative algorithms that have recently appeared in the literature."},{"id":935,"title":"Torralba, A.: Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope. Int. J. Comput. Vision 42, 145-175","url":"https://www.researchgate.net/publication/220660299_Torralba_A_Modeling_the_Shape_of_the_Scene_A_Holistic_Representation_of_the_Spatial_Envelope_Int_J_Comput_Vision_42_145-175","abstraction":"In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category."},{"id":936,"title":"Kernel Codebooks for Scene Categorization","url":"https://www.researchgate.net/publication/221305396_Kernel_Codebooks_for_Scene_Categorization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper introduces a method for scene categorization by modeling ambiguity in the popular codebook approach. The codebook\n <br> approach describes an image as a bag of discrete visual codewords, where the frequency distributions of these words are used\n <br> for image categorization. There are two drawbacks to the traditional codebook model: codeword uncertainty and codeword plausibility.\n <br> Both of these drawbacks stem from the hard assignment of visual features to a single codeword. We show that allowing a degree\n <br> of ambiguity in assigning codewords improves categorization performance for three state-of-the-art datasets.\n</div> \n<p></p>"},{"id":937,"title":"Linear spatial pyramid matching using sparse coding for image classification","url":"https://www.researchgate.net/publication/221364080_Linear_spatial_pyramid_matching_using_sparse_coding_for_image_classification","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.\n</div> \n<p></p>"},{"id":938,"title":"Joint dictionary learning and topic modeling for image clustering","url":"https://www.researchgate.net/publication/224246065_Joint_dictionary_learning_and_topic_modeling_for_image_clustering","abstraction":"A new Bayesian model is proposed, integrating dictionary learning and topic modeling into a unified framework. The model is applied to cluster multiple images, and a subset of the images may be annotated. Example results are presented on the MNIST digit data and on the Microsoft MSRC multi-scene image data. These results reveal the working mechanisms of the model and demonstrate state-of-the-art performance."},{"id":939,"title":"Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations.","url":"https://www.researchgate.net/publication/221618878_Non-Parametric_Bayesian_Dictionary_Learning_for_Sparse_Image_Representations","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Non-parametric Bayesian techniques are considered for learning dictionaries for sparse image representations, with applications in denoising, inpainting and com- pressive sensing (CS). The beta process is employed as a prior for learning the dictionary, and this non-parametric method naturally infers an appropriate dic- tionary size. The Dirichlet process and a probit stick-breaking process are also considered to exploit structure within an image. The proposed method can learn a sparse dictionary in situ; training images may be exploited if available, but they are not required. Further, the noise variance need not be known, and can be non- stationary. Another virtue of the proposed method is that sequential inference can be readily employed, thereby allowing scaling to large images. Several example results are presented, using both Gibbs and variational Bayesian inference, with comparisons to other state-of-the-art approaches.\n</div> \n<p></p>"},{"id":940,"title":"Attractive Faces Are Only Average","url":"https://www.researchgate.net/publication/233821774_Attractive_Faces_Are_Only_Average","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Scientists and philosophers have searched for centuries for a parsimonious answer to the question of what constitutes beauty. We approached this problem from both an evolutionary and information-processing rationale and predicted that faces representing the average value of the population would be consistently judged as attractive. To evaluate this hypothesis, we digitized samples of male and female faces, mathematically averaged them, and had adults judge the attractiveness of both the individual faces and the computer-generated composite images. Both male (three samples) and female (three samples) composite faces were judged as more attractive than almost all the individual faces comprising the composites. A strong linear trend also revealed that the composite faces became more attractive as more faces were entered. These data showing that attractive faces are only average are consistent with evolutionary pressures that favor characteristics close to the mean of the population and with cognitive processes that favor prototypical category members.\n</div> \n<p></p>"},{"id":941,"title":"\"Their Ideas of Beauty Are, on the Whole, the Same as Ours\": Consistency and Variability in the Cross-Cultural Perception of Female Physical Attractiveness","url":"https://www.researchgate.net/publication/232466605_Their_Ideas_of_Beauty_Are_on_the_Whole_the_Same_as_Ours_Consistency_and_Variability_in_the_Cross-Cultural_Perception_of_Female_Physical_Attractiveness","abstraction":"The consistency of physical attractiveness ratings across cultural groups was examined. In Study 1, recently arrived native Asian and Hispanic students and White Americans rated the attractiveness of Asian, Hispanic, Black, and White photographed women. The mean correlation between groups in attractiveness ratings was  r?=?.93. Asians, Hispanics, and Whites were equally influenced by many facial features, but Asians were less influenced by some sexual maturity and expressive features. In Study 2, Taiwanese attractiveness ratings correlated with prior Asian, Hispanic, and American ratings, mean  r?=?.91. Supporting Study 1, the Taiwanese also were less positively influenced by certain sexual maturity and expressive features. Exposure to Western media did not influence attractiveness ratings in either study. In Study 3, Black and White American men rated the attractiveness of Black female facial photos and body types. Mean facial attractiveness ratings were highly correlated ( r?=?.94), but as predicted Blacks and Whites varied in judging bodies. (PsycINFO Database Record (c) 2012 APA, all rights reserved)"},{"id":942,"title":"Experiments on the edited condensed nearest neighbor rule","url":"https://www.researchgate.net/publication/256181891_Experiments_on_the_edited_condensed_nearest_neighbor_rule","abstraction":"Tomek's preprocessing scheme is discussed for editing the training set prior to analyzing it by Hart's condensed nearest neighbor technique. Preprocessing was performed by a ?-nearest-neighbor pdf estimation scheme, although other methods are suggested in this paper. The procedure was studied experimentally and was found to achieve a significant reduction in the storage requirements of the CNN method while maintaining approximately the same error rate, or even improving it."},{"id":943,"title":"The Reduced Nearest Neighbor Rule","url":"https://www.researchgate.net/publication/243785032_The_Reduced_Nearest_Neighbor_Rule","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A further modification to Cover and Hart's nearest neighbor decision rule, the reduced nearest neighbor rule, is introduced. Experimental results demonstrate its accuracy and efficiency.\n</div> \n<p></p>"},{"id":944,"title":"Automatic Analysis of Facial Attractiveness from Video","url":"https://www.researchgate.net/publication/265726717_Automatic_Analysis_of_Facial_Attractiveness_from_Video","abstraction":"There has been a growing interest in the computer science field for automatic analysis and recognition of facial beauty and attractiveness. Most of the proposed studies attempt to model and predict facial attractiveness using a single static facial image. While a static image provides limited information about facial attractiveness, using a video clip that contains information about the motion and the dynamic behaviour of the face provides a richer understanding and valuable insights into analysing facial attractiveness. With this motivation, we propose to use dynamic features obtained from video clips along with static features obtained from static frames for automatic analysis of facial attractiveness. Support Vector Machine (SVM) and Random Forest (RF) are utilised to create and train models of attractiveness using the features extracted. Experimental results show that combining static and dynamic features improve performance over using either of these feature sets alone, and SVM provides the best prediction performance."},{"id":945,"title":"Female Facial Beauty Analysis for Assesment of Facial Attractivness","url":"https://www.researchgate.net/publication/259442100_Female_Facial_Beauty_Analysis_for_Assesment_of_Facial_Attractivness","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a hybrid approach to estimate female facial beauty based on Machine Learning techniques. We use a combination of two approaches: Beauty Mask and Facial Proportions, to find the features that constitute Ideal Female facial beauty and thus, develop a female facial beauty scoring system based on the same. The dataset used in this work consists of 30 images being rated by 29 people. These are the front facial images of Winners, 1st Runner-up and 2nd Runner-up of Miss Universe Beauty Pageant from 2002 to 2011. Images are represented by a 50 element vector consisting of control points being selected manually with reference to the Beauty Mask. These points are used to calculate a total of 12 distances and 7 ratios for each image. These distances and ratios are also calculated for the Beauty Mask, and the final score is given on the basis of similarity between the respective ratios. A correlation of 67.78% shows the validity of our approach. Using this approach, an application is programmed to give scores to input facial images. Apart from scoring, the application provides two separate features: First, some suggestions to improve the facial beauty for the input image and second, an auto-beautified image of the face. The distinguished image dataset, high correlation and additional features make the approach worth.\n</div> \n<p></p>"},{"id":946,"title":"The Intrinsic Dimensionality of Attractiveness: A Study in Face Profiles","url":"https://www.researchgate.net/publication/233970262_The_Intrinsic_Dimensionality_of_Attractiveness_A_Study_in_Face_Profiles","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The study of human attractiveness with pattern analysis techniques is an emerging research field. One still largely unresolved problem is which are the facial features relevant to attractiveness, how they combine together, and the number of independent parameters required for describing and identifying harmonious faces. In this paper, we present a first study about this problem, applied to face profiles. First, according to several empirical results, we hypothesize the existence of two well separated manifolds of attractive and unattractive face profiles. Then, we analyze with manifold learning techniques their intrinsic dimensionality. Finally, we show that the profile data can be reduced, with various techniques, to the intrinsic dimensions, largely without loosing their ability to discriminate between attractive and unattractive faces.\n</div> \n<p></p>"},{"id":947,"title":"The Analysis of Facial Beauty: An Emerging Area of Research in Pattern Analysis","url":"https://www.researchgate.net/publication/221472863_The_Analysis_of_Facial_Beauty_An_Emerging_Area_of_Research_in_Pattern_Analysis","abstraction":"Much research presented recently supports the idea that the human perception of attractiveness is data-driven and largely irrespective of the perceiver. This suggests using pattern analysis techniques for beauty analysis. Several scientific papers on this subject are appearing in image processing, computer vision and pattern analysis contexts, or use techniques of these areas. In this paper, we will survey the recent studies on automatic analysis of facial beauty, and discuss research lines and practical applications."},{"id":948,"title":"Personalized facial attractiveness prediction","url":"https://www.researchgate.net/publication/221292192_Personalized_facial_attractiveness_prediction","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present a fully automatic approach to learning the personal facial attractiveness preferences of individual users directly from example images. The target application is computer assisted search of partners in online dating services. The proposed approach is based on the use of epsiv-SVMs to learn a regression function that maps low level image features onto attractiveness ratings. We present empirical results based on a dataset of images collected from a large online dating site. Our system achieved correlations of up to 0.45 (Pearson correlation) on the attractiveness predictions for individual users. We show evidence that the approach learned not just a universal sense of attraction shared by multiple users, but capitalized on the preferences of individual subjects. Our results are promising and could already be used to facilitate the personalized search of partners in online dating.\n</div> \n<p></p>"},{"id":949,"title":"Face Fusion: An Automatic Method For Virtual Plastic Surgery","url":"https://www.researchgate.net/publication/224281277_Face_Fusion_An_Automatic_Method_For_Virtual_Plastic_Surgery","abstraction":"This paper describes a system that replaces an individual's facial features with corresponding features of another individual -possibly of different skin color- and fuses the replaced features with the original face, such that the resulting face looks natural. The final face resulting from the fusion of the original face with exogenous features, lacks the characteristic discontinuities that would have been expected if only a replacement operation was performed. The proposed system could be used to simulate and predict the outcome of aesthetic maxillofacial plastic surgeries. To achieve its task, the system uses five modules: face detection, feature detection, replacement, shifting and blending. While these modules are designed to address the problem of face fusion, some of the novel algorithms and techniques introduced in this paper could be useful in other image processing and fusion applications"},{"id":950,"title":"FEMALE FACIAL AESTHETICS BASED ON SOFT BIOMETRICS AND PHOTO-QUALITY","url":"https://www.researchgate.net/publication/252064007_FEMALE_FACIAL_AESTHETICS_BASED_ON_SOFT_BIOMETRICS_AND_PHOTO-QUALITY","abstraction":"In this work we study the connection between subjective eval- uation of facial aesthetics and selected objective parameters based on photo-quality and facial soft biometrics. The ap- proach is novel in that it jointly considers both previous re- sults on photo quality and beauty assessment, as well as it incorporates non-permanent facial characteristics and expres- sions in the context of female facial aesthetics. This study helps us understand the role of this specific set of features in affecting the way humans perceive facial images. Based on the above objective parameters, we further con- struct a simple linear metric that hints modifiable parameters for aesthetics enhancement, as well as tunes soft biometric systems that would seek to predict the way humans perceive facial aesthetics."},{"id":951,"title":"A New Computer-aided Technique for Planning the Aesthetic Outcome of Plastic Surgery","url":"https://www.researchgate.net/publication/228342727_A_New_Computer-aided_Technique_for_Planning_the_Aesthetic_Outcome_of_Plastic_Surgery","abstraction":"Plastic surgery plays a major role in today health care. Planning plastic face surgery requires dealing with the elusive concept of attractiveness for evaluating feasible beautification of a particular face. The existing computer tools essentially allow to manually warp 2D images or 3D face scans, in order to produce images simulating possible surgery outcomes. How to manipulate faces, as well as the evaluation of the results, are left to the surgeons judgement. We propose a new quantitative approach able to automatically suggest effective patient-specific improvements of facial attractiveness. The general idea is to compare the face of the patient with a large database of attractive faces, excluding the facial feature to be improved. Then, the feature of the faces more similar is applied, with a suitable morphing, to the face of the patient. In this paper we present a first application of the general idea in the field of nose surgery. Aesthetically effective rhinoplasty is suggested on the base of the entire face profile, a very important 2D feature for rating face attractiveness."},{"id":952,"title":"Performance Evaluation of an Automated Classifier of Female Facial Beauty.","url":"https://www.researchgate.net/publication/221296943_Performance_Evaluation_of_an_Automated_Classifier_of_Female_Facial_Beauty","abstraction":"The fact that facial beauty might be a universal concept and can be measured by using mathematical ratios of facial features has long been debated amongst psychologists and anthropologists. Accordingly, in this paper we present results of experiments to evaluate the extent of universal beauty. The experiments were performed by asking a number of diverse human referees to grade a same collection of female face images. These grades were later compared to the results obtained from an automated classifier. Results obtained prove that the classifier is able to reproduce the human judgment reliably and could be used as an automated tool for objective classification of female facial beauty."},{"id":953,"title":"Automated classification of female facial beauty by image analysis and supervised learning","url":"https://www.researchgate.net/publication/252513124_Automated_classification_of_female_facial_beauty_by_image_analysis_and_supervised_learning","abstraction":"The fact that perception of facial beauty may be a universal concept has long been debated amongst psychologists and anthropologists. In this paper, we performed experiments to evaluate the extent of beauty universality by asking a number of diverse human referees to grade a same collection of female facial images. Results obtained show that the different individuals gave similar votes, thus well supporting the concept of beauty universality. We then trained an automated classifier using the human votes as the ground truth and used it to classify an independent test set of facial images. The high accuracy achieved proves that this classifier can be used as a general, automated tool for objective classification of female facial beauty. Potential applications exist in the entertainment industry and plastic surgery."},{"id":954,"title":"Pattern &amp; Growth in Personality","url":"https://www.researchgate.net/publication/232461648_Pattern_Growth_in_Personality","abstraction":"A revision of the author's 1937 publication, \"Personality: A Psychological Interpretation\" (see  11: 1964). The most important recent \"fruits of personological research\" are surveyed. Intended for \"college students who have little or no background in psychology,\" this edition incorporates a restatement of Allport's thesis that man is not a reactive robot but a being with unique potential for growth and a revision of his argument regarding functional autonomy. New chapters dealing with cultural factors in personality, cognition, the self, learning, personality assessment, and person perception are included. The philosophical consequences of endorsing one psychological interpretation of personality rather than another are discussed. From Psyc Abstracts 36:01:3HA93A. (PsycINFO Database Record (c) 2012 APA, all rights reserved)"},{"id":955,"title":"Personality Patterns of Psychiatrists. A Study of Methods for Selecting Residents","url":"https://www.researchgate.net/publication/49301649_Personality_Patterns_of_Psychiatrists_A_Study_of_Methods_for_Selecting_Residents","abstraction":"The soft cover second volume \"contains detailed accounts of the research methods and many quantitative findings. It is arranged as a series of appendices to the chapters of Vol. I\" (see  33: 5751). Included are further comments on measures of competence, criteria studies, test manuals, and predictive errors, plus a summary of expert opinion on personality requisites for psychotherapists. (PsycINFO Database Record (c) 2012 APA, all rights reserved)"},{"id":956,"title":"The Varieties of Human Physique","url":"https://www.researchgate.net/publication/232471427_The_Varieties_of_Human_Physique","abstraction":"Based on a detailed study of frontal, dorsal and lateral photographs of 4000 male subjects of college age, a 3 dimensional scheme for describing human physique is formulated. Kretschmer's constitutional typology is discarded in favor of one based on 3 first order variables or components, endomorphy, mesomorphy, and ectomorphy, each of which is found in an individual physique and indicated by one of a set of 3 numerals designating a somatotype or patterning of these morphological components. Seventy-six different somatotypes are described and illustrated. These somatotypical designations are objectively assigned on the basis of the use of 18 anthropometric indices. Second-order variables also isolated and studied are dysplasia, gynandromorphy, texture and hirsutism. Historical trends in constitutional research are summarized. A detailed description is given of the development of the somatotyping technique combining anthroposcopic and anthropometric methods. Reference is made to somatotyping with the aid of a specially devised machine. Topics discussed include: the choice of variables, morphological scales, a geometrical representation of somatotypes, the independence of components, correlational data, the problem of norms, the modifiability of a somatotype, hereditary and endocrine influences and the relation of constitution to temperament, mental disease, clinical studies, crime and delinquency, and the differential education of children. Descriptive sketches of variants of the ectomorphic components are given. Appendices list tables for somatotyping and a series of drawings of 9 female somatotypes. An annotated bibliography is followed by a more general one. 272 photographs and drawings illustrate the somatotypes. (PsycINFO Database Record (c) 2012 APA, all rights reserved)"},{"id":957,"title":"We Click, We Align, We Learn: Impact of Influence and Convergence Processes on Student Learning and Rapport Building","url":"https://www.researchgate.net/publication/282629941_We_Click_We_Align_We_Learn_Impact_of_Influence_and_Convergence_Processes_on_Student_Learning_and_Rapport_Building","abstraction":"Behavioral convergence has been identified as one (largely subconscious) contributor to successful conversations, while rapport is one of the central constructs that explains development of personal relationships between these speakers over time. Social factors such as these have been shown to play a potent role in learning. Therefore, in this work, we investigate the relationship in dyadic peer tutoring conversations of convergence, building up of interpersonal rapport over time, and student learning, while positing a novel mechanism that links these constructs. We develop an approach for hierarchical computational modeling of convergence by accounting for time-based dependencies that arise in longitudinal interaction streams, and can thus a)quantify the effect of one partner's behavior on the other and differentiate between driver and recipient (Influence), b)extrapolate the outcome of directional influence to determine adaptation in partners' behaviors (Convergence). Our results illustrate that influence, convergence and rapport in the peer tutoring dialog are correlated with learning gains and provide concrete evidence for rapport being a causal mechanism that leads to convergence of speech rate in the interaction. We discuss the implications of our work for the development of peer tutoring agents that can improve learning gains through convergence to and from the human learner's behavior."},{"id":958,"title":"An institutional palimpsest? The case of Cambodia’s political order, 1970 and beyond","url":"https://www.researchgate.net/publication/283338779_An_institutional_palimpsest_The_case_of_Cambodia%27s_political_order_1970_and_beyond","abstraction":"How do continuity and change coexist and coevolve? How does continuity enable change and change reinforce continuity? These are central questions in organizational and political research, as organizational and institutional systems benefit from the presence of both reproduction and transformation. However, the relation between the processes of change and continuity still raises significant questions. To contribute to this discussion, we analyse the coexistence of deep institutional continuity and radical political change in the second half of twentieth-century Cambodia. Over a two-decade period, Cambodia was ruled by radically different political systems of organization: a traditional monarchy with feudal characteristics, a failing republic, a totalitarian communist regime, and a Vietnamese protectorate, before being governed by the UN and finally becoming a constitutional monarchy. We use an historical approach to study how a succession of radical changes may in reality signal deep lines of continuity."},{"id":959,"title":"Children’s spontaneous emotional expressions while receiving (un)wanted prizes in the presence of peers","url":"https://www.researchgate.net/publication/281865096_Children%27s_spontaneous_emotional_expressions_while_receiving_unwanted_prizes_in_the_presence_of_peers","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Although current emotion theories emphasize the importance of contextual factors for emotional expressive behavior, developmental studies that examine such factors are currently thin on the ground. In this research, we studied the course of emotional expressions of 8- and 11-year-old children after winning a (large) ?rst prize or a (substantially smaller) consolation prize, while playing a game competing against the computer or a physically co-present peer. We analyzed their emotional reactions by conducting two perception tests in which participants rated children’s level of happiness. Results showed that co-presence positively affected children’s happiness only when receiving the ?rst prize. Moreover, for children who were in the presence of a peer, we found that eye contact affected children’s expressions of happiness, but that the effect was different for different age groups: 8-year-old children were negatively affected, and 11-year-old children positively. Overall, we can conclude that as children grow older and their social awareness increases, the presence of a peer affects their non-verbal expressions, regardless of their appreciation of their prize.\n</div> \n<p></p>"},{"id":960,"title":"The effect of leader emotional intelligence on leader–follower chemistry: a study of construction project managers","url":"https://www.researchgate.net/publication/281684725_The_effect_of_leader_emotional_intelligence_on_leader-follower_chemistry_a_study_of_construction_project_managers","abstraction":"Extending Nicolini’s notion of project ‘chemistry’, a ‘leader–follower chemistry’ model associated with the quality of dyadic interpersonal communication in construction projects is developed. The focus is on the project manager as leader in an attempt to deepen understanding of the effect of a project manager’s emotional intelligence (EI) on the quality of interpersonal communication with their followers, being other members of the project team. While a project manager’s EI, with its associated emotional competencies, is often seen as critical in achieving good relationships with members of the project team, it remains a largely understudied concept, particularly in construction projects. Primary data collected using a series of analytical surveys and live observations of site-based project meetings was used to examine the relationship between a project manager’s emotional competencies, particularly sensitivity and expressiveness, and leader–follower chemistry. Overall, 68 construction professionals participated in the study. The findings suggest that a project manager’s emotional sensitivity and expressiveness (particularly head gestures) may explain variance in the quality of leader–follower chemistry. Based on the empirical evidence in the context of team communication, a leader–follower chemistry model is introduced, which emphasizes the importance of leaders’ emotional sensitivity and expressiveness in a leader–follower communication dyad. The model may be particularly salient in complex project networks with a large number of prominent actors."},{"id":961,"title":"A multimodal framework for recognizing emotional feedback in conversational recommender systems","url":"https://www.researchgate.net/publication/285245147_A_multimodal_framework_for_recognizing_emotional_feedback_in_conversational_recommender_systems","abstraction":"A conversational recommender system should interactively assist users in order to understand their needs and preferences and produce personalized recommendations accordingly. While traditional recommender systems use a single-shot approach, the conversational ones refine their suggestions during the conversation since they gain more knowledge about the user. This approach can be useful in case the recommender is embodied in a conversational agent acting as a shopping assistant in a smart retail context. In this case, knowledge about the user preferences may be acquired during the conversation and by observing the user behavior. In such a setting, besides \"rational\" information, the agent may grasp information also about extra-rational factors such as attitudes, emotions, likes and dislikes. This paper describes the study performed in order to develop a multimodal framework for recognizing the shopping attitude of the user during the interaction with DIVA, a Dress-shopping InteractiVe Assistant. In particular, speech prosody, gestures and facial expressions have been taken into account for providing feedback to the system and refining the recommendation accordingly."},{"id":962,"title":"Accuracy of Social Perception: An Integration and Review of Meta-Analyses","url":"https://www.researchgate.net/publication/281521830_Accuracy_of_Social_Perception_An_Integration_and_Review_of_Meta-Analyses","abstraction":"This review examines the overall accuracy of social perception across several research topics and identifies factors that influence the accuracy of social perception. Findings from 14 meta-analyses examining topics such as social/personality judgments, health judgments, legal judgments, and academic/vocational judgments were obtained. Social perception accuracy was generally moderate, yielding an average effect size (r) of .32. However, individual meta-analytic effects varied widely, with some topics yielding small effects (e.g., lie detection, eyewitness identification) and other topics yielding large effects (e.g., educational judgments, health judgments). Several moderators of social perception accuracy were identified, including the nature of the information source, familiarity of the target, type of personality trait, and severity of the outcome being judged. These findings provide a comprehensive summary and novel integration of disparate findings on the accuracy of social perception. Concluding remarks highlight avenues for future research and call for cross-disciplinary collaborations that would enhance our understanding of social perception."},{"id":963,"title":"The link between perception of clinical empathy and nonverbal behavior: The effect of a doctor's gaze and body orientation","url":"https://www.researchgate.net/publication/281092615_The_link_between_perception_of_clinical_empathy_and_nonverbal_behavior_The_effect_of_a_doctor%27s_gaze_and_body_orientation","abstraction":"Clinical empathy is considered to be one of the most important skills for medical professionals. It is primarily conveyed by nonverbal behavior; however, little is known about the importance of different types of cues and their relation to engagement and sincerity as possible correlates of perceived clinical empathy (PCE). In this study, we explored the effect of doctor's gaze and body orientation on PCE with the help of 32 video vignettes. Actors impersonating medical interns displayed different combinations of gaze and body orientation while uttering an empathetic verbal statement. The video vignettes were evaluated in terms of the perceived clinical and general empathy, engagement and sincerity. A principal component analysis revealed a possible single-factor solution for the scales measuring the two types of empathy, engagement and sincerity; therefore, they were subsumed under general perceived empathy (GPE). An analysis of variance showed a main effect of gaze and body orientation, with a stronger effect of gaze, on GPE. We subsequently performed a linear random effects analysis, which indicated possible gender-related differences in the perception of gaze. The outcomes of our experiment confirm that both gaze and body orientation have an influence on the GPE. The effect of gaze, however, appears to be gender-dependent: in the experiment, males were perceived as slightly more empathetic with patient-centered gaze, while for females averted gaze resulted in higher GPE scores. The findings are directly relevant in the context of medical communication training. Perception of clinical empathy supports medical information transfer, diagnosis quality and other patient outcomes. Copyright © 2015 Elsevier Ireland Ltd. All rights reserved."},{"id":964,"title":"Managing the Unknowable: The Effectiveness of Early-stage Investor Gut Feel in Entrepreneurial Investment Decisions","url":"https://www.researchgate.net/publication/281892668_Managing_the_Unknowable_The_Effectiveness_of_Early-stage_Investor_Gut_Feel_in_Entrepreneurial_Investment_Decisions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Using an inductive theory-development study, a field experiment, and a longitudinal field test, we examine early-stage entrepreneurial investment decision making under conditions of extreme uncertainty. Building on existing literature on decision making and risk in organizations, intuition, and theories of entrepreneurial financing, we test the effectiveness of angel investors’ criteria for making investment decisions. We found that angel investors’ decisions have several characteristics that have not been adequately captured in existing theory: angel investors have clear objectives—risking small stakes to find extraordinarily profitable investments, fully expecting to lose their entire investment in most cases—and they rely on a combination of expertise-based intuition and formal analysis in which intuition trumps analysis, contrary to reports in other investment contexts. We also found that their reported emphasis on assessments of the entrepreneur accurately predicts extraordinarily profitable venture success four years later. We develop this theory by examining situations in which uncertainty is so extreme that it qualifies as unknowable, using the term “gut feel” to describe their dynamic emotion-cognitions in which they blend analysis and intuition in ways that do not impair intuitive processes and that effectively predict extraordinarily profitable investments.\n</div> \n<p></p>"},{"id":965,"title":"Nonverbal communication and relational identification with the supervisor: Evidence from two countries","url":"https://www.researchgate.net/publication/278245530_Nonverbal_communication_and_relational_identification_with_the_supervisor_Evidence_from_two_countries","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Purpose – Nonverbal communication comprises a core element of the interactions between leader and follower. Nevertheless, there is limited empirical attention regarding the impact of nonverbal cues on followers’ attitudinal outcomes. The purpose of this paper is to contribute to this gap by linking a salient form of nonverbal communication, kinesics, to an under-researched leader-follower relationship outcome, that is relational identification (RI) with the supervisor. In doing so, the authors also highlight the mediating role of leader-member exchange (LMX) in the aforementioned relationship. Design/methodology/approach – The authors conducted two studies in different countries. Moreover, the authors examined the hypotheses using hierarchical regression and bootstrap analysis. Findings – As hypothesized, the present results showed that kinesics have both a direct and an indirect effect, through LMX, on RI with the supervisor. Originality/value – To the best of authors’ knowledge this is the first study that links a form of nonverbal communication to both LMX and RI.\n</div> \n<p></p>"},{"id":966,"title":"Critical comment on Hicks-Caskey and Potter, \"Effect of the full moon on a sample of developmentally delayed, institutionalized women\".","url":"https://www.researchgate.net/publication/21331530_Critical_comment_on_Hicks-Caskey_and_Potter_Effect_of_the_full_moon_on_a_sample_of_developmentally_delayed_institutionalized_women","abstraction":"Hicks-Caskey and Potter (1991) claim to have found a \"full moon effect\" on women in a developmental center. Further, they suggest the discrepancies in findings on lunar effects can be accounted for by (i) a lack of equivalent operational definitions and (ii) a person selection factor. It is argued that the Hicks-Caskey and Potter findings are undermined by weekday, holiday, season, weather, particular staff-subject interactions, and expectancy effects. In addition, the proposed explanations for differing outcomes in lunar studies do not explain both the negative findings and conflicting positive findings."},{"id":967,"title":"Talk and silence sequences in informal conversations: III. Interspeaker influence","url":"https://www.researchgate.net/publication/229532465_Talk_and_silence_sequences_in_informal_conversations_III_Interspeaker_influence","abstraction":"Literature on the structure of two-person conversations has consistently found that partners become more similar in mean duration of pauses and switching pauses over the course of interaction. Evidence on influence in vocalization duration is primarily negative. No direct evidence of interspeaker influence on a moment-to-moment basis is available. In this study 12 dyadic conversations are analyzed for interspeaker influence, with the use of time series regression procedures. Two versions of the data are presented: a probability summary including the probability of breaking mutual silences, continuing simultaneous talk, and continuing talking alone and a turn summary including vocalization, pause, and switching pause duration. Results show that (1) moment-to-moment influences are present in both versions of the data, (2) these influences differ from dyad to dyad, (3) the influences are both positive (matching) and negative (compensating), and (4) the magnitude of interspeaker influence on a temporal basis is small but detectable. In addition, there are overall tendencies to match in switching pause, probability of continued simultaneous speech, and probability of breaking mutual silences. Dyads show both compensation and matching on vocalization duration and pause-related variables. Implications of these data for past and future explanation of social interaction are explored."},{"id":968,"title":"Virtual interpersonal touch: Expressing and recognizing emotions through haptic devices","url":"https://www.researchgate.net/publication/234802824_Virtual_interpersonal_touch_Expressing_and_recognizing_emotions_through_haptic_devices","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This article examines the phenomenon of Virtual Interpersonal Touch (VIT), people touching one another via force-feedback haptic devices. As collaborative virtual environments become utilized more effectively, it is only natural that interactants will have the ability to touch one another. In the work presented here, we used relatively basic devices to begin to explore the expression of emotion through VIT. In Experiment 1, participants utilized a 2 DOF force-feedback joystick to express seven emotions. We examined various dimensions of the forces generated and subjective ratings of the difficulty of expressing those emotions. In Experiment 2, a separate group of participants attempted to recognize the recordings of emotions generated in Experiment 1. In Experiment 3, pairs of participants attempted to communicate the seven emotions using physical handshakes. Results indicated that humans were above chance when recognizing emotions via VIT but not as accurate as people expressing emotions through nonmediated handshakes. We discuss a theoretical framework for understanding emotions expressed through touch as well as the implications of the current findings for the utilization of VIT in human-computer interaction.\n</div> \n<p></p>"},{"id":969,"title":"Negotiating over Time: Impediments to Integrative Solutions","url":"https://www.researchgate.net/publication/223473055_Negotiating_over_Time_Impediments_to_Integrative_Solutions","abstraction":"Most negotiation relationships consist of interactions that occur across time. This paper explores the influence of two variables on the outcomes of such negotiations: the mobility of negotiators and the frame of the short-term sacrifice required to reach long-term gain. Specifically, we focus on the integrativeness of agreements both within a static negotiating period as well as across several negotiations. Subjects participated in an experiment that consisted of 10 similar negotiations across time, with two issues being addressed in each negotiation. The experiment was created to allow for the possibility of a moderately advantageous integrative agreement within each static negotiation, as well as a superior integrative agreement across negotiations. A higher level of negotiator mobility was predicted to decrease the integrativeness of outcomes across negotiations. This prediction was supported. The frame of the sacrifice required of subjects was predicted to affect integrativeness both within as well as across negotiations, such that subjects would be less likely to sacrifice on an issue, or issues, if it meant accepting a perceived loss rather than a reduced gain. This prediction was not supported. The results are discussed in terms of their implications for managerial negotiation and decision making."},{"id":970,"title":"TARGET ARTICLE: Immersive Virtual Environment Technology as a Methodological Tool for Social Psychology","url":"https://www.researchgate.net/publication/247504321_TARGET_ARTICLE_Immersive_Virtual_Environment_Technology_as_a_Methodological_Tool_for_Social_Psychology","abstraction":"Historically, at least 3 methodological problems have dogged experimental social psychology: the experimental control-mundane realism trade-off, lack of replication, and unrepresentative sampling. We argue that immersive virtual environment technology (IVET) can help ameliorate, if not solve, these methodological problems and, thus, holds promise as a new social psychological research tool. In this article, we first present an overview of IVET and review IVET-based research within psychology and other fields. Next, we propose a general model of social influence within immersive virtual environments and present some preliminary findings regarding its utility for social psychology. Finally, we present a new paradigm for experimental social psychology that may enable researchers to unravel the very fabric of social interaction."},{"id":971,"title":"Mimicry for money: Behavioral consequences of imitation","url":"https://www.researchgate.net/publication/222674581_Mimicry_for_money_Behavioral_consequences_of_imitation","abstraction":"Two experiments investigated the idea that mimicry leads to pro-social behavior. It was hypothesized that mimicking the verbal behavior of customers would increase the size of tips. In Experiment 1, a waitress either mimicked half her customers by literally repeating their order or did not mimic her customers. It was found that she received significantly larger tips when she mimicked her customers than when she did not. In Experiment 2, in addition to a mimicry- and non-mimicry condition, a baseline condition was included in which the average tip was assessed prior to the experiment. The results indicated that, compared to the baseline, mimicry leads to larger tips. These results demonstrate that mimicry can be advantageous for the imitator because it can make people more generous."},{"id":972,"title":"Types of touch in cross-sex relationships between coworkers: Perceptions of relational and emotional messages, inappropriateness, and sexual harassment","url":"https://www.researchgate.net/publication/248925769_Types_of_touch_in_cross-sex_relationships_between_coworkers_Perceptions_of_relational_and_emotional_messages_inappropriateness_and_sexual_harassment","abstraction":"This study examined observers' perceptions of nine different types of touch (including a \"no touch\" control condition) used in cross-sex relationships between coworkers. Results showed that face touch sends particularly strong relational and emotional messages. A soft touch in the cheek area of the face was seen as signaling more affection, attraction, flirtation, and love than the other types of touch. Face touch was also rated as the most inappropriate and sexually harassing of the nine types of touch examined. Arm around the waist was also rated as showing relatively high levels of attraction and flirtation, as well as inappropriateness and harassment. No touch and handshaking conveyed the most formality. Observers rated women as more affectionate, trusting, happy, and composed than men across the touch conditions. Men, however, were judged to be more attracted to their cross-sex partners than were women. These and other findings are discussed to shed light on the multiple interpretations of touch within the context of relationships between cross-sex coworkers."},{"id":973,"title":"Relational message interpretations of touch, conversation distance, and posture","url":"https://www.researchgate.net/publication/226098600_Relational_message_interpretations_of_touch_conversation_distance_and_posture","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n According to a social meaning model of nonverbal communication, many nonverbal behaviors have consensually recognized meanings. Two field experiments examined this presumption by investigating the relational message interpretations assigned to differing levels and types of touch, proximity, and posture. Also examined were the possible moderating effects of the communicator characteristics of gender and attractiveness and relationship characteristics of gender composition and status differentials. Results showed that touching typically conveyed more composure, immediacy, receptivity/trust, affection, similarity/depth/equality, dominance, and informality than its absence. The form of touch also mattered, with handholding and face touching expressing the most intimacy, composure, and informality; handholding and the handshake expressing the least dominance, and the handshake conveying the most formality but also receptivity/trust. Postural openness/relaxation paralleled touch in conveying greater intimacy, composure, informality, and similarity but was also less dominant than a closed/tense posture. Close proximity was also more immediate and similar but dominant. Proximity and postural openness together produced differential interpretations of composure, similarity, and affection. Gender initiator attractiveness was more influential than status in moderating interpretations.\n</div> \n<p></p>"},{"id":974,"title":"How to Touch Humans: Guidelines for Social Agents and Robots That Can Touch","url":"https://www.researchgate.net/publication/257936516_How_to_Touch_Humans_Guidelines_for_Social_Agents_and_Robots_That_Can_Touch","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Touch is an essential channel in interpersonal and affective communication, yet most social agents currently lack the capability to touch the user. In this paper we show the credibility of three premises that make the case that providing touch capability to social robots will increase their effectiveness in communicating emotions, building trust and achieving behavioral changes. The first premise is that humans can communicate distinct emotions through touch only, the second is that this is also possible through mediated (virtual) touch, and the third is that social agents can use the same mediated touch technology as effectively as humans. Based on a literature review, we also formulate ten design rules as guidance for the development of social agents that can touch. These rules concern parameters that regulate the meaning of touch cues like context and familiarity, the implicit and explicit meanings of touch, user characteristics, and parameters that can be communicated through affective touch.\n</div> \n<p></p>"},{"id":975,"title":"Empathic Touch by Relational Agents","url":"https://www.researchgate.net/publication/220395383_Empathic_Touch_by_Relational_Agents","abstraction":"We describe a series of experiments with an agent designed to model human conversational touch-capable of physically touching users in synchrony with speech and other nonverbal communicative behavior-and its use in expressing empathy to users in distress. The agent is composed of an animated human face that is displayed on a monitor affixed to the top of a human mannequin, with touch conveyed by an air bladder that squeezes a user's hand. We demonstrate that when touch is used alone, hand squeeze pressure and number of squeezes are associated with user perceptions of affect arousal conveyed by an agent, while number of squeezes and squeeze duration are associated with affect valence. We also show that, when affect-relevant cues are present in facial display, speech prosody, and touch used simultaneously by the agent, facial display dominates user perceptions of affect valence, and facial display and prosody are associated with affect arousal, while touch had little effect. Finally, we show that when touch is used in the context of an empathic, comforting interaction (but without the manipulation of affect cues in other modalities), it can lead to better perceptions of relationship with the agent, but only for users who are comfortable being touched by other people."},{"id":976,"title":"Persuasive Robotics: The Influence of Robot Gender on Human Behavior","url":"https://www.researchgate.net/publication/224090579_Persuasive_Robotics_The_Influence_of_Robot_Gender_on_Human_Behavior","abstraction":"Persuasive Robotics is the study of persuasion as it applies to human-robot interaction (HRI). Persuasion can be generally defined as an attempt to change another's beliefs or behavior. The act of influencing others is fundamental to nearly every type of social interaction. Any agent desiring to seamlessly operate in a social manner will need to incorporate this type of core human behavior. As in human interaction, myriad aspects of a humanoid robot's appearance and behavior can significantly alter its persuasiveness - this work will focus on one particular factor: gender. In the current study, run at the Museum of Science in Boston, subjects interacted with a humanoid robot whose gender was varied. After a short interaction and persuasive appeal, subjects responded to a donation request made by the robot, and subsequently completed a post-study questionnaire. Findings showed that men were more likely to donate money to the female robot, while women showed little preference. Subjects also tended to rate the robot of the opposite sex as more credible, trustworthy, and engaging. In the case of trust and engagement the effect was much stronger between male subjects and the female robot. These results demonstrate the importance of considering robot and human gender in the design of HRI."},{"id":977,"title":"How, When, and Why to Use Digital Experimental Virtual Environments to Study Social Behavior","url":"https://www.researchgate.net/publication/227673964_How_When_and_Why_to_Use_Digital_Experimental_Virtual_Environments_to_Study_Social_Behavior","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Over the last decade, researchers have begun using immersive virtual environment technology (IVET; commonly known as virtual reality) to create digital experimental virtual environments (DEVEs) to investigate social psychological processes. Researchers increasingly recognize that IVET provides powerful and cost-effective ways to manipulate theoretical variables and to measure a host of outcome variables, while providing a remarkable level of experimental control and ecological realism. In this article, we discuss IVET, the nature of social influence within DEVEs, and the use of DEVEs to study social behavior.\n</div> \n<p></p>"},{"id":978,"title":"Facial Similarity Between Voters and Candidates Causes Influence","url":"https://www.researchgate.net/publication/31283469_Facial_Similarity_Between_Voters_and_Candidates_Causes_Influence","abstraction":"Social science research demonstrates that people are drawn to others perceived as similar. We extend this finding to political candidates by comparing the relative effects of candidate familiarity as well as partisan, issue, gender, and facial similarity on voters’ evaluations of candidates. In Experiment 1, during the week of the 2006 Florida gubernatorial race, a national representative sample of voters viewed images of two unfamiliar candidates (Crist and Davis) morphed with either themselves or other voters. Results demonstrated a strong preference for facially similar candidates, despite no conscious awareness of the similarity manipulation. In Experiment 2, one week before the 2004 presidential election, a national representative sample of voters evaluated familiar candidates (Bush and Kerry). Strong partisans were unmoved by the facial similarity manipulation, but weak partisans and independents preferred the candidate with whom their own face had been morphed over the candidate morphed with another voter. In Experiment 3, we compared the effects of policy similarity and facial similarity using a set of prospective 2008 presidential candidates. Even though the effects of party and policy similarity dominated, facial similarity proved a significant cue for unfamiliar candidates. Thus, the evidence across the three studies suggests that even in high-profile elections, voters prefer candidates high in facial similarity, but most strongly with unfamiliar candidates."},{"id":979,"title":"Social Signal Processing: Understanding social interactions through nonverbal behavior analysis (PDF)","url":"https://www.researchgate.net/publication/228930411_Social_Signal_Processing_Understanding_social_interactions_through_nonverbal_behavior_analysis_PDF","abstraction":"This paper introduces Social Signal Processing (SSP), the domain aimed at automatic understanding of social interactions through analysis of nonverbal behavior. The core idea of SSP is that nonverbal behavior is machine de-tectable evidence of social signals, the relational attitudes exchanged between interacting individuals. Social signals include (dis-)agreement, empathy, hostility, and any other attitude towards others that is expressed not only by words but by nonverbal behaviors such as facial expression and body posture as well. Thus, nonverbal behavior analysis is used as a key to automatic understanding of social interac-tions. This paper presents not only a survey of the related literature and the main concepts underlying SSP, but also an illustrative example of how such concepts are applied to the analysis of conflicts in competitive discussions."},{"id":980,"title":"Persuasive robotics : how robots change our minds","url":"https://www.researchgate.net/publication/38006829_Persuasive_robotics_how_robots_change_our_minds","abstraction":"Thesis (S.M.)--Massachusetts Institute of Technology, School of Architecture and Planning, Program in Media Arts and Sciences, 2009. Includes bibliographical references (p. 169-174). This thesis explores the extent to which socially capable humanoid robots have the potential to influence human belief, perception and behavior. Sophisticated computational systems coupled with human-like form and function render such robots as potentially powerful forms of persuasive technology. Currently, there is very little understanding of the persuasive potential of such machines. As personal robots become a reality in our immediate environment, a better understanding of the mechanisms behind, and the capabilities of, their ability to influence, is becoming increasingly important. This thesis proposes some guiding principles by which to qualify persuasion. A study was designed in which the MDS (Mobile Dexterous Social) robotic platform was used to solicit visitors for donations at the Museum of Science in Boston. The study tests some nonverbal behavioral variables known to change persuasiveness in humans, and measures their effect in human-robot interaction. The results of this study indicate that factors such as robot-gender, subject-gender, touch, interpersonal distance, and the perceived autonomy of the robot, have a huge impact on the interaction between human and robot, and must be taken into consideration when designing sociable robots. This thesis applies the term persuasive robotics to define and test the theoretical and practical implications for robot-triggered changes in human attitude and behavior. Its results provide for a vast array of speculations with regard to what practical applications may become available using this framework. by Michael Steven Siegel. S.M."},{"id":981,"title":"One Dimensional Turing-Like Handshake Test for Motor Intelligence","url":"https://www.researchgate.net/publication/49729028_One_Dimensional_Turing-Like_Handshake_Test_for_Motor_Intelligence","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In the Turing test, a computer model is deemed to \"think intelligently\" if it can generate answers that are not distinguishable from those of a human. However, this test is limited to the linguistic aspects of machine intelligence. A salient function of the brain is the control of movement, and the movement of the human hand is a sophisticated demonstration of this function. Therefore, we propose a Turing-like handshake test, for machine motor intelligence. We administer the test through a telerobotic system in which the interrogator is engaged in a task of holding a robotic stylus and interacting with another party (human or artificial). Instead of asking the interrogator whether the other party is a person or a computer program, we employ a two-alternative forced choice method and ask which of two systems is more human-like. We extract a quantitative grade for each model according to its resemblance to the human handshake motion and name it \"Model Human-Likeness Grade\" (MHLG). We present three methods to estimate the MHLG. (i) By calculating the proportion of subjects' answers that the model is more human-like than the human; (ii) By comparing two weighted sums of human and model handshakes we fit a psychometric curve and extract the point of subjective equality (PSE); (iii) By comparing a given model with a weighted sum of human and random signal, we fit a psychometric curve to the answers of the interrogator and extract the PSE for the weight of the human in the weighted sum. Altogether, we provide a protocol to test computational models of the human handshake. We believe that building a model is a necessary step in understanding any phenomenon and, in this case, in understanding the neural mechanisms responsible for the generation of the human handshake.\n</div> \n<p></p>"},{"id":982,"title":"Automatic summarization of broadcast news using structural features","url":"https://www.researchgate.net/publication/221482863_Automatic_summarization_of_broadcast_news_using_structural_features","abstraction":"We present a method for summarizing broadcast news that is not affected by word errors in an automatic speech recognition transcription, using information about the structure of the news program. We construct a directed graphical model to represent the probability distribution and dependencies among the struc- tural features which we train by finding the values of parameters of the conditional probability tables. We then rank segments of the test set and extract the highest ranked ones as a summary. We present the procedure and preliminary test results."},{"id":983,"title":"ANVIL - A Generic Annotation Tool for Multimodal Dialogue","url":"https://www.researchgate.net/publication/221487883_ANVIL_-_A_Generic_Annotation_Tool_for_Multimodal_Dialogue","abstraction":"Anvil is a tool for the annotation of audiovisual material con- taining multimodal dialogue. Annotation takes place on freely definable, multiple layers (tracks) by inserting time-anchored elements that hold a number of typed attribute-value pairs. Higher-level elements (suprasegmental) consist of a sequence of elements. Attributes contain symbols or cross-level links to arbitrary other elements. Anvil is highly generic (usable with different annotation schemes), platform-independent, XML- based and fitted with an intuitive graphical user interface. For project integration, Anvil offers the import of speech transcrip- tion and export of text and table data for further statistical pro- cessing."},{"id":984,"title":"New Developments In Automatic Meeting Transcription","url":"https://www.researchgate.net/publication/2434640_New_Developments_In_Automatic_Meeting_Transcription","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In this paper we report on new developments in the automatic meeting transcription task. Unlike other types of speech (such as those found in Broadcast News and Switchboard), meetings are unique in their richer dynamics of human-to-human interaction. An intuitive \"thumbnail\" plot is proposed to visualize such turntaking behavior. We will also show how recognition of short turns can be improved by building a language model tailored specifically for short turns. Out-Of-Vocabulary (OOV) words become a more salient problem in the meeting transcription task, as they are mostly topic words and proper names, lack of which not only causes Word Error Rate (WER) increase, but also limits further use of recognition hypotheses. We describe a prototype system which uses the Web as a source for vocabulary expansion, and present preliminary OOV retrieval results. 1. INTRODUCTION As speech recognition research progresses from read speech(Wall Street Journal), to prepared speech (a major part of Broa...\n</div> \n<p></p>"},{"id":985,"title":"Automatic Role Recognition in Multiparty Conversations: An Approach Based on Turn Organization, Prosody, and Conditional Random Fields","url":"https://www.researchgate.net/publication/254057318_Automatic_Role_Recognition_in_Multiparty_Conversations_An_Approach_Based_on_Turn_Organization_Prosody_and_Conditional_Random_Fields","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Roles are a key aspect of social interactions, as they contribute to the overall predictability of social behavior (a necessary requirement to deal effectively with the people around us), and they result in stable, possibly machine-detectable behavioral patterns (a key condition for the application of machine intelligence technologies). This paper proposes an approach for the automatic recognition of roles in conversational broadcast data, in particular, news and talk shows. The approach makes use of behavioral evidence extracted from speaker turns and applies conditional random fields to infer the roles played by different individuals. The experiments are performed over a large amount of broadcast material (around 50 h), and the results show an accuracy higher than 85%.\n</div> \n<p></p>"},{"id":986,"title":"Towards a Technology of Nonverbal Communication: Vocal Behavior in Social and Affective Phenomena","url":"https://www.researchgate.net/publication/47508039_Towards_a_Technology_of_Nonverbal_Communication_Vocal_Behavior_in_Social_and_Affective_Phenomena","abstraction":"Nonverbal communication is the main channel through which we experience inner life of others, including their emotions, feelings, moods, social attitudes, etc. This attracts the interest of the computing community because nonverbal communication is based on cues like facial expressions, vocalizations, gestures, postures, etc. that we can perceive with our senses and can be (and often are) detected, analyzed and synthesized with automatic approaches. In other words, nonverbal communication can be used as a viable interface between computers and some of the most important aspects of human psychology such as emotions and social attitudes. As a result, a new computing domain seems to emerge that we can define “technology of nonverbal communication”. This chapter outlines some of the most salient aspects of such a potentially new domain and outlines some of its most important perspectives for the future."},{"id":987,"title":"Language-Independent Socio-Emotional Role Recognition in the AMI Meetings Corpus.","url":"https://www.researchgate.net/publication/221479668_Language-Independent_Socio-Emotional_Role_Recognition_in_the_AMI_Meetings_Corpus","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Social roles are a coding scheme that characterizes the relationships between group members during a discussion and their roles \"oriented toward the functioning of the group as a group\". This work presents an investigation on language-independent automatic social role recognition in AMI meetings based on turns statistics and prosodic features. At first, turn-taking statistics and prosodic features are integrated into a single generative conversation model which achieves a role recognition accuracy of 59%. This model is then extended to explicitly account for dependencies (or influence) between speakers achieving an accuracy of 65%. The last contribution consists in investigating the statistical dependencies between the formal and the social role that participants have; integrating the information related to the formal role in the model, the recognition achieves an accuracy of 68%.\n</div> \n<p></p>"},{"id":988,"title":"Modeling Group Discussion Dynamics","url":"https://www.researchgate.net/publication/255589659_Modeling_Group_Discussion_Dynamics","abstraction":"In this paper, we present a formal model of group discussion dyanmics. An understanding of the face-to-face communications in a group discussion can provide new clues about how humans collaborate to accomplish complex tasks and how the collaboration protocols can be learned. It can also help us to evaluate and facilitate brainstorming sessions. We will discuss the following three findings about the dynamics: Meet- ings in dierent languages and on dierent topics could follow the same form of dynamics; The functional roles of the meeting participants could be better understood by inspecting not only their individual speaking and activity features but also their interactions with each other; The outcome of a meeting could be predicted by inspecting how its participants interact."},{"id":989,"title":"Understanding social signals in multi-party conversations: Automatic recognition of socio-emotional roles in the AMI meeting corpus","url":"https://www.researchgate.net/publication/220753458_Understanding_social_signals_in_multi-party_conversations_Automatic_recognition_of_socio-emotional_roles_in_the_AMI_meeting_corpus","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Any social interaction is characterized by roles, patterns of behavior recognized as such by the interacting participants and corresponding to shared expectations that people hold about their own behavior as well as the behavior of others. In this respect, social roles are a key aspect of social interaction because they are the basis for making reasonable guesses about human behavior. Recognizing roles is a crucial need towards understanding (possibly in an automatic way) any social exchange, whether this means to identify dominant individuals, detect conflict, assess engagement or spot conversation highlights. This work presents an investigation on language-independent automatic social role recognition in AMI meetings, spontaneous multi-party conversations, based solely on turn organization and prosodic features. At first turn-taking statistics and prosodic features are integrated into a single generative conversation model which achieves an accuracy of 59%. This model is then extended to explicitly account for dependencies (or influence) between speakers achieving an accuracy of 65%. The last contribution consists in investigating the statistical dependency between the formal and the social role that participants have; integrating the information related to the formal role in the recognition model achieves an accuracy of 68%. The paper is concluded highlighting some future directions.\n</div> \n<p></p>"},{"id":990,"title":"Speaker diarization of meetings based on speaker role n-gram models.","url":"https://www.researchgate.net/publication/220736559_Speaker_diarization_of_meetings_based_on_speaker_role_n-gram_models","abstraction":"Speaker diarization of meeting recordings is generally based on acoustic information ignoring that meetings are instances of conversations. Several recent works have shown that the sequence of speakers in a conversation and their roles are related and statistically predictable. This paper proposes the use of speaker roles n-gram model to capture the conversation patterns probability and investigates its use as prior information into a state-of-the-art diarization system. Experiments are run on the AMI corpus annotated in terms of roles. The proposed technique reduces the diarization speaker error by 19% when the roles are known and by 17% when they are estimated. Furthermore the paper investigates how the n-gram models generalize to different settings like those from the Rich Transcription campaigns. Experiments on 17 meetings reveal that the speaker error can be reduced by 12% also in this case thus the n-gram can generalize across corpora."},{"id":991,"title":"Improving Speech Processing trough Social Signals: Automatic Speaker Segmentation of Political Debates using Role based Turn-Taking Patterns.","url":"https://www.researchgate.net/publication/49461034_Improving_Speech_Processing_trough_Social_Signals_Automatic_Speaker_Segmentation_of_Political_Debates_using_Role_based_Turn-Taking_Patterns","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Several recent works on social signals have addressed the problem of statistical modeling of social interaction in multi-party discussions showing that characteristics like turn-taking patterns can be modeled and predicted according to the role that each participant has in the discussion. Reversely this work investigates the use of social signals to improve conventional speech processing methods. In details we propose the use of turn-taking patterns induced by roles for improving speaker diarization, the task of determining 'Who spoke when' in an audio file. In detail, this work studies how to include this information as statistical prior on the speaker interactions for segmenting and clustering speakers in multi-party political debates. Experiments reveal that the proposed approach reduces the speaker error over the baseline by 25% when both the number of speakers and their roles are known and by 13% relative when the pattern information is estimated from the data. Furthermore we never verify a performance degradation in any recording. Experiments are also carried out to investigate the contribution of the first-order Markov assumption i.e. that the role of the speaker n is conditionally dependent on the role of the speaker n-1.\n</div> \n<p></p>"},{"id":992,"title":"Behavior Modeling with Probabilistic Context Free Grammars","url":"https://www.researchgate.net/publication/224218772_Behavior_Modeling_with_Probabilistic_Context_Free_Grammars","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Identifying the behavioral patterns in a social network setting is beneficial to understand how people behave in certain application domains. Such patterns can also be utilized to characterize social signals such as social roles from interactions. In this work, we examine how probabilistic context free grammars (PCFGs) can be utilized to model interactions and role taking in a social network. We describe how to automatically build a PCFG given a set of interactions as the training data. Our experiments on the Mission Survival Corpus 1 (MSC-1) dataset show that PCFGs are a concise way of modeling social entity behaviors and are useful in understanding the probability distribution of interactions as well as the behavior types that are observed.\n</div> \n<p></p>"},{"id":993,"title":"Locating case discussion segments in recorded medical team meetings","url":"https://www.researchgate.net/publication/228345993_Locating_case_discussion_segments_in_recorded_medical_team_meetings","abstraction":"Although there has been great interest in the issue of index-ing and providing access to multimedia records of meetings, with substantial efforts directed towards collection and analysis of meeting corpora, most research in this area is based on data collected at research labs, under somewhat artificial conditions. In contrast, this paper focuses on data recorded in a real-world setting where a number of health professionals participate in weekly meetings held as part of the work routines in a major hospital. These meetings have been observed to be highly structured, a fact that is due undoubtedly to the time pressures, as well as communication and dependability constraints characteristic of the context in which the meetings happen. The hypoth-esis investigated in this paper is that the conversational structure of these meetings enable their segmentation into meaningful sub-units, namely individual patient case discussions, based only on data on the roles of the par-ticipants and the duration and sequence of vocalisations. We describe the task of segmenting audio-visual records of multidisciplinary medical team meetings as a topic segmen-tation task, present a method for automatic segmentation based on a \"content-free\" representation of conversational structure, and report the results of a series of patient case segmentation experiments. The approach presented here achieves levels of segmentation accuracy (measured in terms of the standard P k and WD metrics) comparable to those attained by state of the art topic segmentation algorithms based on richer and combined knowledge sources."},{"id":994,"title":"Modeling other talkers for improved dialog act recognition in meetings","url":"https://www.researchgate.net/publication/221487278_Modeling_other_talkers_for_improved_dialog_act_recognition_in_meetings","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Automatic dialog act (DA) modeling has been shown to benefit meeting understanding, but current approaches to DA recog- nition tend to suffer from a common problem: they under- represent behaviors found at turn edges, during which the \"floor\"isnegotiatedamongmeetingparticipants. Weproposea new approach that takes into account speech from other talkers, relying only on speech/non-speech information from all partic- ipants. We find (1) that modeling other participants improves DA detection, even in the absence of other information, (2) that only the single locally most talkative other participant mat- ters, and (3) that 10 seconds provides a sufficiently large local context. Results further show significant performance improve- ments over a lexical-only system — particularly for the DAs of interest. We conclude that interaction-based modeling at turn edges can be achieved by relatively simple features and should be incorporated for improved meeting understanding. Index Terms : vocal interaction, cross-speaker modeling, speech/non-speech, dialog acts, meetings\n</div> \n<p></p>"},{"id":995,"title":"The forced-choice paradigm and the perception of facial expressions of emotion","url":"https://www.researchgate.net/publication/12134968_The_forced-choice_paradigm_and_the_perception_of_facial_expressions_of_emotion","abstraction":"The view that certain facial expressions of emotion are universally agreed on has been challenged by studies showing that the forced-choice paradigm may have artificially forced agreement. This article addressed this methodological criticism by offering participants the opportunity to select a none of these terms are correct option from a list of emotion labels in a modified forced-choice paradigm. The results show that agreement on the emotion label for particular facial expressions is still greater than chance, that artifactual agreement on incorrect emotion labels is obviated, that participants select the none option when asked to judge a novel expression, and that adding 4 more emotion labels does not change the pattern of agreement reported in universality studies. Although the original forced-choice format may have been prone to artifactual agreement, the modified forced-choice format appears to remedy that problem."},{"id":996,"title":"Visible Acts of MeaningAn Integrated Message Model of Language in Face-to-Face Dialogue","url":"https://www.researchgate.net/publication/242545265_Visible_Acts_of_MeaningAn_Integrated_Message_Model_of_Language_in_Face-to-Face_Dialogue","abstraction":"The authors propose that dialogue in face-to-face interaction is both audible and visible; language use in this setting includes visible acts of meaning such as facial displays and hand gestures. Several criteria distinguish these from other nonverbal acts: (a) They are sensitive to a sender-receiver relationship in that they are less likely to occur when an addressee will not see them, (b) they are analogically encoded symbols, (c) their meaning can be explicated or demonstrated in context, and (d) they are fully integrated with the accompanying words, although they may be redundant or nonredundant with these words. For these particular acts, the authors eschew the term nonverbal communication because it is a negative definition based solely on physical source. Instead, they propose an integrated message model in which the moment-by-moment audible and visible communicative acts are treated as a unified whole."},{"id":997,"title":"Body movement and emphasis in speech","url":"https://www.researchgate.net/publication/243764410_Body_movement_and_emphasis_in_speech","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Videotapes were made of opposite-sex pairs of students in conversation with one another who had been asked to discuss items from an attitude questionnaire on which they had disagreed. At the end of the conversation, the subjects were asked to replay the videotape and to indicate which body movements of themselves and their partners they considered conveyed emphasis; these movements were then categorized using a detailed body movement system. In a second procedure, the occurrence of vocal stress was scored, together with the associated body movements. The results showed that whereas a wide diversity of primarily hand/arm movements were selected by the subjects as communicating emphasis, it was movements of all parts of the body which were related to vocal stress. It was concluded that a close relationship does exist between body movement and tonic stress, and that this can only be effectively appreciated through a body movement scoring system which enables a detailed description to be given of the visual and temporal relationship between body movement and phonemic clause structure.\n</div> \n<p></p>"},{"id":998,"title":"Dynamic properties influence the perception of facial expressions","url":"https://www.researchgate.net/publication/11829580_Dynamic_properties_influence_the_perception_of_facial_expressions","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Two experiments were conducted to investigate the role played by dynamic information in identifying facial expressions of emotion. Dynamic expression sequences were created by generating and displaying morph sequences which changed the face from neutral to a peak expression in different numbers of intervening intermediate stages, to create fast (6 frames), medium (26 frames), and slow (101 frames) sequences. In experiment 1, participants were asked to describe what the person shown in each sequence was feeling. Sadness was more accurately identified when slow sequences were shown. Happiness, and to some extent surprise, was better from faster sequences, while anger was most accurately detected from the sequences of medium pace. In experiment 2 we used an intensity-rating task and static images as well as dynamic ones to examine whether effects were due to total time of the displays or to the speed of sequence. Accuracies of expression judgments were derived from the rated intensities and the results were similar to those of experiment 1 for angry and sad expressions (surprised and happy were close to ceiling). Moreover, the effect of display time was found only for dynamic expressions and not for static ones, suggesting that it was speed, not time, which was responsible for these effects. These results suggest that representations of basic expressions of emotion encode information about dynamic as well as static properties.\n</div> \n<p></p>"},{"id":999,"title":"Listeners as co-narrators","url":"https://www.researchgate.net/publication/12189986_Listeners_as_co-narrators","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A collaborative theory of narrative story-telling was tested in two experiments that examined what listeners do and their effect on the narrator. In 63 unacquainted dyads (81 women and 45 men), a narrator told his or her own close-call story. The listeners made 2 different kinds of listener responses: Generic responses included nodding and vocalizations such as \"mhm.\" Specific responses, such as wincing or exclaiming, were tightly connected to (and served to illustrate) what the narrator was saying at the moment. In experimental conditions that distracted listeners from the narrative content, listeners made fewer responses, especially specific ones, and the narrators also told their stories significantly less well, particularly at what should have been the dramatic ending. Thus, listeners were co-narrators both through their own specific responses, which helped illustrate the story, and in their apparent effect on the narrator's performance. The results demonstrate the importance of moment-by-moment collaboration in face-to-face dialogue.\n</div> \n<p></p>"},{"id":1000,"title":"How believable are real faces? Towards a perceptual basis for conversational animation","url":"https://www.researchgate.net/publication/4015891_How_believable_are_real_faces_Towards_a_perceptual_basis_for_conversational_animation","abstraction":"Regardless of whether the humans involved are virtual or real, well-developed conversational skills are a necessity. The synthesis of interface agents that are not only understandable but also believable can be greatly aided by knowledge of which facial motions are perceptually necessary and sufficient for clear and believable conversational facial expressions. Here, we recorded several core conversational expressions (agreement, disagreement, happiness, sadness, thinking, and confusion) from several individuals, and then psychophysically determined the perceptual ambiguity and believability of the expressions. The results show that people can identify these expressions quite well, although there are some systematic patterns of confusion. People were also very confident of their identifications and found the expressions to be rather believable. The specific pattern of confusions and confidence ratings have strong implications for conversational animation. Finally, the present results provide the information necessary to begin a more fine-grained analysis of the core components of these expressions."}]}