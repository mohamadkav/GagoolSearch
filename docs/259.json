{"id":259,"title":"Factorized Asymptotic Bayesian Hidden Markov Models","url":"https://www.researchgate.net/publication/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper addresses the issue of model selection for hidden Markov models\n <br> (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has\n <br> been recently developed for model selection on independent hidden variables\n <br> (i.e., mixture models), for time-dependent hidden variables. As with FAB in\n <br> mixture models, FAB for HMMs is derived as an iterative lower bound\n <br> maximization algorithm of a factorized information criterion (FIC). It\n <br> inherits, from FAB for mixture models, several desirable properties for\n <br> learning HMMs, such as asymptotic consistency of FIC with marginal\n <br> log-likelihood, a shrinkage effect for hidden state selection, monotonic\n <br> increase of the lower FIC bound through the iterative optimization. Further, it\n <br> does not have a tunable hyper-parameter, and thus its model selection process\n <br> can be fully automated. Experimental results shows that FAB outperforms\n <br> states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in\n <br> terms of model selection accuracy and computational efficiency.\n</div> \n<p></p>"}