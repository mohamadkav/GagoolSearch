{"id":270,"title":"Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization","url":"https://www.researchgate.net/publication/288713780_Bridging_the_Gap_between_Stochastic_Gradient_MCMC_and_Stochastic_Optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian\n <br> analogs to popular stochastic optimization methods; however, this connection is\n <br> not well studied. We explore this relationship by applying simulated annealing\n <br> to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two\n <br> key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)\n <br> adaptive element-wise momentum weights. The zero-temperature limit gives a\n <br> novel stochastic optimization method with adaptive element-wise momentum\n <br> weights, while conventional optimization methods only have a shared, static\n <br> momentum weight. Under certain assumptions, our theoretical analysis suggests\n <br> the proposed simulated annealing approach converges close to the global optima.\n <br> Experiments on several deep neural network models show state-of-the-art results\n <br> compared to related stochastic optimization algorithms.\n</div> \n<p></p>"}