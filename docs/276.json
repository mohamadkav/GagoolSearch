{"id":276,"title":"Learning Deep Generative Models with Doubly Stochastic MCMC","url":"https://www.researchgate.net/publication/278413644_Learning_Deep_Generative_Models_with_Doubly_Stochastic_MCMC","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We present doubly stochastic gradient MCMC, a simple and generic method for\n <br> (approximate) Bayesian inference of deep generative models in the collapsed\n <br> continuous parameter space. At each MCMC sampling step, the algorithm randomly\n <br> draws a mini-batch of data samples to estimate the gradient of log-posterior\n <br> and further estimates the intractable expectation over latent variables via a\n <br> Gibbs sampler or a neural adaptive importance sampler. We demonstrate the\n <br> effectiveness on learning deep sigmoid belief networks (DSBNs). Compared to the\n <br> state-of-the-art methods using Gibbs sampling with data augmentation, our\n <br> algorithm is much more efficient and manages to learn DSBNs on large datasets.\n</div> \n<p></p>"}