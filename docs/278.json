{"id":278,"title":"Hamiltonian ABC","url":"https://www.researchgate.net/publication/273327856_Hamiltonian_ABC","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Approximate Bayesian computation (ABC) is a powerful and elegant framework\n <br> for performing inference in simulation-based models. However, due to the\n <br> difficulty in scaling likelihood estimates, ABC remains useful for relatively\n <br> low-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of\n <br> likelihood-free algorithms that apply recent advances in scaling Bayesian\n <br> learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find\n <br> that a small number forward simulations can effectively approximate the ABC\n <br> gradient, allowing Hamiltonian dynamics to efficiently traverse parameter\n <br> spaces. We also describe a new simple yet general approach of incorporating\n <br> random seeds into the state of the Markov chain, further reducing the random\n <br> walk behavior of HABC. We demonstrate HABC on several typical ABC problems, and\n <br> show that HABC samples comparably to regular Bayesian inference using true\n <br> gradients on a high-dimensional problem from machine learning.\n</div> \n<p></p>"}