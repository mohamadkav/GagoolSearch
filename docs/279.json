{"id":279,"title":"Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC","url":"https://www.researchgate.net/publication/273157486_Large-Scale_Distributed_Bayesian_Matrix_Factorization_using_Stochastic_Gradient_MCMC","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Despite having various attractive qualities such as high prediction accuracy\n <br> and the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix\n <br> Factorization has not been widely adopted because of the prohibitive cost of\n <br> inference. In this paper, we propose a scalable distributed Bayesian matrix\n <br> factorization algorithm using stochastic gradient MCMC. Our algorithm, based on\n <br> Distributed Stochastic Gradient Langevin Dynamics, can not only match the\n <br> prediction accuracy of standard MCMC methods like Gibbs sampling, but at the\n <br> same time is as fast and simple as stochastic gradient descent. In our\n <br> experiments, we show that our algorithm can achieve the same level of\n <br> prediction accuracy as Gibbs sampling an order of magnitude faster. We also\n <br> show that our method reduces the prediction error as fast as distributed\n <br> stochastic gradient descent, achieving a 4.1% improvement in RMSE for the\n <br> Netflix dataset and an 1.8% for the Yahoo music dataset.\n</div> \n<p></p>"}