{"id":318,"title":"Sublinear Approximate Inference for Probabilistic Programs","url":"https://www.researchgate.net/publication/267983152_Sublinear_Approximate_Inference_for_Probabilistic_Programs","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Probabilistic programming languages can simplify the development of machine\n <br> learning techniques, but only if inference is sufficiently scalable.\n <br> Unfortunately, Bayesian parameter estimation for highly coupled models such as\n <br> regressions and state-space models still scales badly. This paper describes a\n <br> sublinear-time algorithm for making Metropolis-Hastings updates to latent\n <br> variables in probabilistic programs. This approach generalizes recently\n <br> introduced approximate MH techniques: instead of subsampling data items assumed\n <br> to be independent, it subsamples edges in a dynamically constructed graphical\n <br> model. It thus applies to a broader class of problems and interoperates with\n <br> general-purpose inference techniques. Empirical results are presented for\n <br> Bayesian logistic regression, nonlinear classification via joint Dirichlet\n <br> process mixtures, and parameter estimation for stochastic volatility models\n <br> (with state estimation via particle MCMC). All three applications use the same\n <br> implementation, and each requires under 20 lines of probabilistic code.\n</div> \n<p></p>"}