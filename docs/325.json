{"id":325,"title":"Optimal aggregation of classifiers in statistical learning, Annals of Statististics, 32(1)","url":"https://www.researchgate.net/publication/38349018_Optimal_aggregation_of_classifiers_in_statistical_learning_Annals_of_Statististics_321","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Classification can be considered as nonparametric estimation of sets, where the risk is defined by means of a specific distance between sets associated with misclassification error. It is shown that the rates of convergence of classifiers depend on two parameters: the complexity of the class of candidate sets and the margin parameter. The dependence is explicitly given, indicating that optimal fast rates approaching $O(n^{-1})$ can be attained, where n is the sample size, and that the proposed classifiers have the property of robustness to the margin. The main result of the paper concerns optimal aggregation of classifiers: we suggest a classifier that automatically adapts both to the complexity and to the margin, and attains the optimal fast rates, up to a logarithmic factor.\n</div> \n<p></p>"}