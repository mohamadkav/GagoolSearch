{"id":335,"title":"Asynchronous Parallel Stochastic Gradient Descent - A Numeric Core for Scalable Distributed Machine Learning Algorithms","url":"https://www.researchgate.net/publication/277023572_Asynchronous_Parallel_Stochastic_Gradient_Descent_-_A_Numeric_Core_for_Scalable_Distributed_Machine_Learning_Algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The implementation of a vast majority of machine learning (ML) algorithms\n <br> boils down to solving a numerical optimization problem. In this context,\n <br> Stochastic Gradient Descent (SGD) methods have long proven to provide good\n <br> results, both in terms of convergence and accuracy. Recently, several\n <br> parallelization approaches have been proposed in order to scale SGD to solve\n <br> very large ML problems. At their core, most of these approaches are following a\n <br> map-reduce scheme. This paper presents a novel parallel updating algorithm for\n <br> SGD, which utilizes the asynchronous single-sided communication paradigm.\n <br> Compared to existing methods, ASGD provides faster (or at least equal)\n <br> convergence, close to linear scaling and stable accuracy.\n</div> \n<p></p>"}