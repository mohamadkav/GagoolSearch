{"id":340,"title":"Hierarchical Mixture Modeling With Normalized Inverse-Gaussian Priors","url":"https://www.researchgate.net/publication/4741797_Hierarchical_Mixture_Modeling_With_Normalized_Inverse-Gaussian_Priors","abstraction":"In recent years the Dirichlet process prior has experienced a great success in the context of Bayesian mixture modeling. The idea of overcoming discreteness of its realizations by exploiting it in hierarchical models, combined with the development of suitable sampling techniques, represent one of the reasons of its popularity. In this article we propose the normalized inverse-Gaussian (N듈G) process as an alternative to the Dirichlet process to be used in Bayesian hierarchical models. The N듈G prior is constructed via its finite-dimensional distributions. This prior, although sharing the discreteness property of the Dirichlet prior, is characterized by a more elaborate and sensible clustering which makes use of all the information contained in the data. Whereas in the Dirichlet case the mass assigned to each observation depends solely on the number of times that it occurred, for the N듈G prior the weight of a single observation depends heavily on the whole number of ties in the sample. Moreover, expressions corresponding to relevant statistical quantities, such as a priori moments and the predictive distributions, are as tractable as those arising from the Dirichlet process. This implies that well-established sampling schemes can be easily extended to cover hierarchical models based on the N듈G process. The mixture of N듈G process and the mixture of Dirichlet process are compared using two examples involving mixtures of normals."}