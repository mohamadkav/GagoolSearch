{"id":385,"title":"Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms","url":"https://www.researchgate.net/publication/51914398_Explicit_Learning_Curves_for_Transduction_and_Application_to_Clusteringand_Compression_Algorithms","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Inductive learning is based on inferring a general rule from a finite data\n <br> set and using it to label new data. In transduction one attempts to solve the\n <br> problem of using a labeled training set to label a set of unlabeled points,\n <br> which are given to the learner prior to learning. Although transduction seems\n <br> at the outset to be an easier task than induction, there have not been many\n <br> provably useful algorithms for transduction. Moreover, the precise relation\n <br> between induction and transduction has not yet been determined. The main\n <br> theoretical developments related to transduction were presented by Vapnik more\n <br> than twenty years ago. One of Vapnik's basic results is a rather tight error\n <br> bound for transductive classification based on an exact computation of the\n <br> hypergeometric tail. While tight, this bound is given implicitly via a\n <br> computational routine. Our first contribution is a somewhat looser but explicit\n <br> characterization of a slightly extended PAC-Bayesian version of Vapnik's\n <br> transductive bound. This characterization is obtained using concentration\n <br> inequalities for the tail of sums of random variables obtained by sampling\n <br> without replacement. We then derive error bounds for compression schemes such\n <br> as (transductive) support vector machines and for transduction algorithms based\n <br> on clustering. The main observation used for deriving these new error bounds\n <br> and algorithms is that the unlabeled test points, which in the transductive\n <br> setting are known in advance, can be used in order to construct useful data\n <br> dependent prior distributions over the hypothesis space.\n</div> \n<p></p>"}