{"id":390,"title":"Generating Sentences from a Continuous Space","url":"https://www.researchgate.net/publication/284219020_Generating_Sentences_from_a_Continuous_Space","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The standard unsupervised recurrent neural network language model (RNNLM)\n <br> generates sentences one word at a time and does not work from an explicit\n <br> global distributed sentence representation. In this work, we present an\n <br> RNN-based variational autoencoder language model that incorporates distributed\n <br> latent representations of entire sentences. This factorization allows it to\n <br> explicitly model holistic properties of sentences such as style, topic, and\n <br> high-level syntactic features. Samples from the prior over these sentence\n <br> representations remarkably produce diverse and well-formed sentences through\n <br> simple deterministic decoding. By examining paths through this latent space, we\n <br> are able to generate coherent novel sentences that interpolate between known\n <br> sentences. We present techniques for solving the difficult learning problem\n <br> presented by this model, demonstrate strong performance in the imputation of\n <br> missing tokens, and explore many interesting properties of the latent sentence\n <br> space.\n</div> \n<p></p>"}