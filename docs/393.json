{"id":393,"title":"MuProp: Unbiased Backpropagation for Stochastic Neural Networks","url":"https://www.researchgate.net/publication/284219954_MuProp_Unbiased_Backpropagation_for_Stochastic_Neural_Networks","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Deep neural networks are powerful parametric models that can be trained\n <br> efficiently using the backpropagation algorithm. Stochastic neural networks\n <br> combine the power of large parametric functions with that of graphical models,\n <br> which makes it possible to learn very complex distributions. However, as\n <br> backpropagation is not directly applicable to stochastic networks that include\n <br> discrete sampling operations within their computational graph, training such\n <br> networks remains difficult. We present MuProp, an unbiased gradient estimator\n <br> for stochastic networks, designed to make this task easier. MuProp improves on\n <br> the likelihood-ratio estimator by reducing its variance using a control variate\n <br> based on the first-order Taylor expansion of a mean-field network. Crucially,\n <br> unlike prior attempts at using backpropagation for training stochastic\n <br> networks, the resulting estimator is unbiased and well behaved. Our experiments\n <br> on structured output prediction and discrete latent variable modeling\n <br> demonstrate that MuProp yields consistently good performance across a range of\n <br> difficult tasks.\n</div> \n<p></p>"}