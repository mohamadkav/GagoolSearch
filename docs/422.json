{"id":422,"title":"Expectation Propagation for Approximate Bayesian Inference","url":"https://www.researchgate.net/publication/234108836_Expectation_Propagation_for_Approximate_Bayesian_Inference","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n This paper presents a new deterministic approximation technique in Bayesian\n <br> networks. This method, \"Expectation Propagation\", unifies two previous\n <br> techniques: assumed-density filtering, an extension of the Kalman filter, and\n <br> loopy belief propagation, an extension of belief propagation in Bayesian\n <br> networks. All three algorithms try to recover an approximate distribution which\n <br> is close in KL divergence to the true distribution. Loopy belief propagation,\n <br> because it propagates exact belief states, is useful for a limited class of\n <br> belief networks, such as those which are purely discrete. Expectation\n <br> Propagation approximates the belief states by only retaining certain\n <br> expectations, such as mean and variance, and iterates until these expectations\n <br> are consistent throughout the network. This makes it applicable to hybrid\n <br> networks with discrete and continuous nodes. Expectation Propagation also\n <br> extends belief propagation in the opposite direction - it can propagate richer\n <br> belief states that incorporate correlations between nodes. Experiments with\n <br> Gaussian mixture models show Expectation Propagation to be convincingly better\n <br> than methods with similar computational cost: Laplace's method, variational\n <br> Bayes, and Monte Carlo. Expectation Propagation also provides an efficient\n <br> algorithm for training Bayes point machine classifiers.\n</div> \n<p></p>"}