{"id":432,"title":"Stochastic variational inference for large-scale discrete choice models using adaptive batch sizes","url":"https://www.researchgate.net/publication/262569028_Stochastic_variational_inference_for_large-scale_discrete_choice_models_using_adaptive_batch_sizes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Discrete choice models describe the choices made by decision makers among\n <br> alternatives and play an important role in transportation planning, marketing\n <br> research and other applications. The mixed multinomial logit (MMNL) model is a\n <br> popular discrete choice model that captures heterogeneity in the preferences of\n <br> decision makers through random coefficients. While Markov chain Monte Carlo\n <br> methods provide the Bayesian analogue to classical procedures for estimating\n <br> MMNL models, computations can be prohibitively expensive for large datasets.\n <br> Approximate inference can be obtained using variational methods at a lower\n <br> computational cost with competitive accuracy. In this paper, we develop\n <br> variational methods for estimating MMNL models that allow random coefficients\n <br> to be correlated in the posterior and can be extended to large-scale datasets.\n <br> We explore three alternatives: (1) Laplace variational inference, (2)\n <br> nonconjugate variational message passing and (3) stochastic linear regression,\n <br> and compare their performances using real and simulated data. To accelerate\n <br> convergence for large datasets, we develop stochastic variational inference for\n <br> MMNL models using each of the above three alternatives. Stochastic variational\n <br> inference allows data to be processed in minibatches by optimizing global\n <br> variational parameters using stochastic gradient approximation. A novel\n <br> strategy for increasing minibatch sizes adaptively within stochastic\n <br> variational inference is proposed.\n</div> \n<p></p>"}