{"id":447,"title":"Asynchronous Distributed Learning of Topic Models.","url":"https://www.researchgate.net/publication/221619032_Asynchronous_Distributed_Learning_of_Topic_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning al- gorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors indepen- dently perform Gibbs sampling on their local data and communicate their infor- mation in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard L DA and HDP samplers, but with significant improvements in computation time and me mory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processor s. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced.\n</div> \n<p></p>"}