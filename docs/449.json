{"id":449,"title":"Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation","url":"https://www.researchgate.net/publication/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In the internet era there has been an explosion in the amount of digital text\n <br> information available, leading to difficulties of scale for traditional\n <br> inference algorithms for topic models. Recent advances in stochastic\n <br> variational inference algorithms for latent Dirichlet allocation (LDA) have\n <br> made it feasible to learn topic models on large-scale corpora, but these\n <br> methods do not currently take full advantage of the collapsed representation of\n <br> the model. We propose a stochastic algorithm for collapsed variational Bayesian\n <br> inference for LDA, which is simpler and more efficient than the state of the\n <br> art method. We show connections between collapsed variational Bayesian\n <br> inference and MAP estimation for LDA, and leverage these connections to prove\n <br> convergence properties of the proposed algorithm. In experiments on large-scale\n <br> text corpora, the algorithm was found to converge faster and often to a better\n <br> solution than the previous method. Human-subject experiments also demonstrated\n <br> that the method can learn coherent topics in seconds on small corpora,\n <br> facilitating the use of topic models in interactive document analysis software.\n</div> \n<p></p>"}