{"id":477,"title":"Strategies and Principles of Distributed Machine Learning on Big Data","url":"https://www.researchgate.net/publication/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The rise of Big Data has led to new demands for Machine Learning (ML) systems\n <br> to learn complex models with millions to billions of parameters, that promise\n <br> adequate capacity to digest massive datasets and offer powerful predictive\n <br> analytics thereupon. In order to run ML algorithms at such scales, on a\n <br> distributed cluster with 10s to 1000s of machines, it is often the case that\n <br> significant engineering efforts are required --- and one might fairly ask if\n <br> such engineering truly falls within the domain of ML research or not. Taking\n <br> the view that Big ML systems can benefit greatly from ML-rooted statistical and\n <br> algorithmic insights --- and that ML researchers should therefore not shy away\n <br> from such systems design --- we discuss a series of principles and strategies\n <br> distilled from our recent efforts on industrial-scale ML solutions. These\n <br> principles and strategies span a continuum from application, to engineering,\n <br> and to theoretical research and development of Big ML systems and\n <br> architectures, with the goal of understanding how to make them efficient,\n <br> generally-applicable, and supported with convergence and scaling guarantees.\n <br> They concern four key questions which traditionally receive little attention in\n <br> ML research: How to distribute an ML program over a cluster? How to bridge ML\n <br> computation with inter-machine communication? How to perform such\n <br> communication? What should be communicated between machines? By exposing\n <br> underlying statistical and algorithmic characteristics unique to ML programs\n <br> but not typically seen in traditional computer programs, and by dissecting\n <br> successful cases to reveal how we have harnessed these principles to design and\n <br> develop both high-performance distributed ML software as well as\n <br> general-purpose ML frameworks, we present opportunities for ML researchers and\n <br> practitioners to further shape and grow the area that lies between ML and\n <br> systems.\n</div> \n<p></p>"}