{"id":49,"title":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models","url":"https://www.researchgate.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The recently developed Bayesian Gaussian process latent variable model\n <br> (GPLVM) is a powerful generative model for discovering low dimensional\n <br> embeddings in linear time complexity. However, modern datasets are so large\n <br> that even linear-time models find them difficult to cope with. We introduce a\n <br> novel re-parametrisation of variational inference for the GPLVM and sparse GP\n <br> model that allows for an efficient distributed inference algorithm.\n <br> We present a unifying derivation for both models, analytically deriving the\n <br> optimal variational distribution over the inducing points. We then assess the\n <br> suggested inference on datasets of different sizes, showing that it scales well\n <br> with both data and computational resources. We furthermore demonstrate its\n <br> practicality in real-world settings using datasets with up to 100 thousand\n <br> points, comparing the inference to sequential implementations, assessing the\n <br> distribution of the load among the different nodes, and testing its robustness\n <br> to network failures.\n</div> \n<p></p>"}