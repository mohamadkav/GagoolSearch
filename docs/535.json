{"id":535,"title":"Approximation Errors of Online Sparsification Criteria","url":"https://www.researchgate.net/publication/265967012_Approximation_Errors_of_Online_Sparsification_Criteria","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Many machine learning frameworks, such as resource-allocating networks,\n <br> kernel-based methods, Gaussian processes, and radial-basis-function networks,\n <br> require a sparsification scheme in order to address the online learning\n <br> paradigm. For this purpose, several online sparsification criteria have been\n <br> proposed to restrict the model definition on a subset of samples. The most\n <br> known criterion is the (linear) approximation criterion, which discards any\n <br> sample that can be well represented by the already contributing samples, an\n <br> operation with excessive computational complexity. Several computationally\n <br> efficient sparsification criteria have been introduced in the literature, such\n <br> as the distance, the coherence and the Babel criteria. In this paper, we\n <br> provide a framework that connects these sparsification criteria to the issue of\n <br> approximating samples, by deriving theoretical bounds on the approximation\n <br> errors. Moreover, we investigate the error of approximating any feature, by\n <br> proposing upper-bounds on the approximation error for each of the\n <br> aforementioned sparsification criteria. Two classes of features are described\n <br> in detail, the empirical mean and the principal axes in the kernel principal\n <br> component analysis.\n</div> \n<p></p>"}