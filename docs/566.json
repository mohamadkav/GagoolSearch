{"id":566,"title":"Kernelized Bayesian Matrix Factorization","url":"https://www.researchgate.net/publication/232906044_Kernelized_Bayesian_Matrix_Factorization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We extend kernelized matrix factorization with a fully Bayesian treatment and\n <br> with an ability to work with multiple side information sources expressed as\n <br> different kernels. Kernel functions have been introduced to matrix\n <br> factorization to integrate side information about the rows and columns (e.g.,\n <br> objects and users in recommender systems), which is necessary for making\n <br> out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite\n <br> graph inference, where the output matrix is binary, but extensions to more\n <br> general matrices are straightforward. We extend the state of the art in two key\n <br> aspects: (i) A fully conjugate probabilistic formulation of the kernelized\n <br> matrix factorization problem enables an efficient variational approximation,\n <br> whereas fully Bayesian treatments are not computationally feasible in the\n <br> earlier approaches. (ii) Multiple side information sources are included,\n <br> treated as different kernels in multiple kernel learning that additionally\n <br> reveals which side information sources are informative. Our method outperforms\n <br> alternatives in predicting drug-protein interactions on two data sets. We then\n <br> show that our framework can also be used for solving multilabel learning\n <br> problems by considering samples and labels as the two domains where matrix\n <br> factorization operates on. Our algorithm obtains the lowest Hamming loss values\n <br> on 10 out of 14 multilabel classification data sets compared to five\n <br> state-of-the-art multilabel learning algorithms.\n</div> \n<p></p>"}