{"id":580,"title":"On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes","url":"https://www.researchgate.net/publication/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The variational framework for learning inducing variables Titsias (2009) has\n <br> had a large impact on the Gaussian process literature. The framework may be\n <br> interpreted as minimizing a rigorously defined Kullback-Leibler divergence\n <br> between the approximate and posterior processes. To our knowledge this\n <br> connection has thus far gone unremarked in the literature. Many of the\n <br> technical requirements for such a result were derived in the pioneering work of\n <br> Seeger (2003,2003b). In this work we give a relatively gentle and largely\n <br> self-contained explanation of the result. The result is important in\n <br> understanding the variational inducing framework and could lead to principled\n <br> novel generalizations.\n</div> \n<p></p>"}