{"id":629,"title":"Moller, M.F.: A Scaled Conjugate Gradient Algorithm For Fast Supervised Learning. Neural Networks 6, 525-533","url":"https://www.researchgate.net/publication/222482794_Moller_MF_A_Scaled_Conjugate_Gradient_Algorithm_For_Fast_Supervised_Learning_Neural_Networks_6_525-533","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n A supervised learning algorithm (Scaled Conjugate Gradient, SCG) is introduced. The performance of SCG is benchmarked against that of the standard back propagation algorithm (BP) (Rumelhart, Hinton, &amp; Williams, 1986), the conjugate gradient algorithm with line search (CGL) (Johansson, Dowla, &amp; Goodman, 1990) and the one-step Broyden-Fletcher-Goldfarb-Shanno memoriless quasi-Newton algorithm (BFGS) (Battiti, 1990). SCG is fully-automated, includes no critical user-dependent parameters, and avoids a time consuming line search, which CGL and BFGS use in each iteration in order to determine an appropriate step size. Experiments show that SCG is considerably faster than BP, CGL, and BFGS.\n</div> \n<p></p>"}