{"id":638,"title":"Dropout as a Bayesian Approximation: Appendix","url":"https://www.researchgate.net/publication/277959103_Dropout_as_a_Bayesian_Approximation_Appendix","abstraction":"We show that a multilayer perceptron (MLP) with arbitrary depth and nonlinearities, with dropout applied after every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation offers an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. This document is an appendix for the main paper \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" by Gal and Ghahramani, 2015."}