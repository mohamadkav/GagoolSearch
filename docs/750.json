{"id":750,"title":"A new integral loss function for Bayesian optimization","url":"https://www.researchgate.net/publication/264936662_A_new_integral_loss_function_for_Bayesian_optimization","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n We consider the problem of maximizing a real-valued continuous function $f$\n <br> using a Bayesian approach. Since the early work of Jonas Mockus and Antanas\n <br> \\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by\n <br> considering the loss function $\\max f - M_n$ (where $M_n$ denotes the best\n <br> function value observed after $n$ evaluations of $f$). This loss function puts\n <br> emphasis on the value of the maximum, at the expense of the location of the\n <br> maximizer. In the special case of a one-step Bayes-optimal strategy, it leads\n <br> to the classical Expected Improvement (EI) sampling criterion. This is a\n <br> special case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk\n <br> associated to a certain uncertainty measure (here, the expected loss) on the\n <br> quantity of interest is minimized at each step of the algorithm. In this\n <br> article, assuming that $f$ is defined over a measure space $(\\mathbb{X},\n <br> \\lambda)$, we propose to consider instead the integral loss function\n <br> $\\int_{\\mathbb{X}} (f - M_n)_{+}\\, d\\lambda$, and we show that this leads, in\n <br> the case of a Gaussian process prior, to a new numerically tractable sampling\n <br> criterion that we call $\\rm EI^2$ (for Expected Integrated Expected\n <br> Improvement). A numerical experiment illustrates that a SUR strategy based on\n <br> this new sampling criterion reduces the error on both the value and the\n <br> location of the maximizer faster than the EI-based strategy.\n</div> \n<p></p>"}