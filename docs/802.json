{"id":802,"title":"Asymptotic analysis of covariance parameter estimation for Gaussian processes in the misspecified case","url":"https://www.researchgate.net/publication/269280638_Asymptotic_analysis_of_covariance_parameter_estimation_for_Gaussian_processes_in_the_misspecified_case","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n In parametric estimation of covariance function of Gaussian processes, it is\n <br> often the case that the true covariance function does not belong to the\n <br> parametric set used for estimation. This situation is called the misspecified\n <br> case. In this case, it has been observed that, for irregular spatial sampling\n <br> of observation points, Cross Validation can yield smaller prediction errors\n <br> than Maximum Likelihood. Motivated by this comparison, we provide a general\n <br> asymptotic analysis of the misspecified case, for independent observation\n <br> points with uniform distribution. We prove that the Maximum Likelihood\n <br> estimator asymptotically minimizes a Kullback-Leibler divergence, within the\n <br> misspecified parametric set, while Cross Validation asymptotically minimizes\n <br> the integrated square prediction error. In a Monte Carlo simulation, we show\n <br> that the covariance parameters estimated by Maximum Likelihood and Cross\n <br> Validation, and the corresponding Kullback-Leibler divergences and integrated\n <br> square prediction errors, can be strongly contrasting. On a more technical\n <br> level, we provide new increasing-domain asymptotic results for the situation\n <br> where the eigenvalues of the covariance matrices involved are not upper\n <br> bounded.\n</div> \n<p></p>"}