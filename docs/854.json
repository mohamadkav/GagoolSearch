{"id":854,"title":"Ensemble Learning for Hidden Markov Models","url":"https://www.researchgate.net/publication/224881830_Ensemble_Learning_for_Hidden_Markov_Models","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n The standard method for training Hidden Markov Models optimizes a point estimate of the model parameters. This estimate, which can be viewed as the maximum of a posterior probability density over the model parameters, may be susceptible to overfitting, and contains no indication of parameter uncertainty. Also, this maximummay be unrepresentative of the posterior probability distribution. In this paper we study a method in which we optimize an ensemble which approximates the entire posterior probability distribution. The ensemble learning algorithm requires the same resources as the traditional Baum--Welch algorithm. The traditional training algorithm for hidden Markov models is an expectation-- maximization (EM) algorithm (Dempster et al. 1977) known as the Baum--Welch algorithm. It is a maximum likelihood method, or, with a simple modification, a penalized maximum likelihood method, which can be viewed as maximizing a posterior probability density over the model parameters. Recently, ...\n</div> \n<p></p>"}