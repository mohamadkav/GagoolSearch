{"id":9,"title":"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144","url":"https://www.researchgate.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144","abstraction":"<p itemprop=\"description\"> <strong>ABSTRACT</strong> </p>\n<div>\n Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian\n <br> statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP\n <br> mixtures has enabled the application of nonparametric Bayesian methods to a variety of\n <br> practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it\n <br> is important to explore alternatives. One class of alternatives is provided by variational\n <br> methods, a class of deterministic algorithms that convert inference problems into\n <br> optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far,\n <br> variational methods have mainly been explored in the parametric setting, in particular\n <br> within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001;\n <br> Blei et al. 2003). In this paper, we present a variational inference algorithm for DP\n <br> mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms\n <br> for DP mixtures of Gaussians and present an application to a large-scale image analysis \n <br> problem.\n</div> \n<p></p>"}